{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6ee6c7",
   "metadata": {},
   "source": [
    "## Project Members:\n",
    "    Crystal Zhu (cyz22)\n",
    "    Lyra D'Souza (lad279)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6396cf",
   "metadata": {},
   "source": [
    "## Brief Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1c647",
   "metadata": {},
   "source": [
    "Describe what we intended "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f7964",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3214c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import hfst_dev as hfst\n",
    "import graphviz\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import CFG\n",
    "from nltk import grammar, parse\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.parse.util import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c853a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream English \n",
    "\n",
    "istream = hfst.HfstInputStream('English')\n",
    "assert istream.is_good() == True\n",
    "English = istream.read()\n",
    "istream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457447eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up definitions from phoneclass.fst \n",
    "\n",
    "defs = {'English' : English}\n",
    "\n",
    "VowAA = hfst.regex('AA0 | AA1 | AA2', definitions=defs)\n",
    "defs['VowAA'] = VowAA\n",
    "VowAE = hfst.regex('AE0 | AE1 | AE2', definitions=defs)\n",
    "defs['VowAE'] = VowAE\n",
    "VowAH = hfst.regex('AH0 | AH1 | AH2', definitions=defs)\n",
    "defs['VowAH'] = VowAH\n",
    "VowAO = hfst.regex('AO0 | AO1 | AO2', definitions=defs)\n",
    "defs['VowAO'] = VowAO\n",
    "VowAW = hfst.regex('AW0 | AW1 | AW2', definitions=defs)\n",
    "defs['VowAW'] = VowAW\n",
    "VowAY = hfst.regex('AY0 | AY1 | AY2', definitions=defs)\n",
    "defs['VowAY'] = VowAY\n",
    "VowEH = hfst.regex('EH0 | EH1 | EH2', definitions=defs)\n",
    "defs['VowEH'] = VowEH\n",
    "VowER = hfst.regex('ER0 | ER1 | ER2', definitions=defs)\n",
    "defs['VowER'] = VowER\n",
    "VowEY = hfst.regex('EY0 | EY1 | EY2', definitions=defs)\n",
    "defs['VowEY'] = VowEY\n",
    "VowIH = hfst.regex('IH0 | IH1 | IH2', definitions=defs)\n",
    "defs['VowIH'] = VowIH\n",
    "VowIY = hfst.regex('IY0 | IY1 | IY2', definitions=defs)\n",
    "defs['VowIY'] = VowIY\n",
    "VowOW = hfst.regex('OW0 | OW1 | OW2', definitions=defs)\n",
    "defs['VowOW'] = VowOW\n",
    "VowOY = hfst.regex('OY0 | OY1 | OY2', definitions=defs)\n",
    "defs['VowOY'] = VowOY\n",
    "VowUH = hfst.regex('UH0 | UH1 | UH2', definitions=defs)\n",
    "defs['VowUH'] = VowUH\n",
    "VowUW = hfst.regex('UW0 | UW1 | UW2', definitions=defs)\n",
    "defs['VowUW'] = VowUW\n",
    "\n",
    "Vow0 = hfst.regex('AH0| IH0| ER0| IY0| OW0| AA0| EH0| UW0| AE0| AO0| AY0| EY0| AW0| UH0| OY0', definitions=defs)\n",
    "defs['Vow0'] = Vow0\n",
    "Vow1 = hfst.regex('EH1| AE1| AA1| IH1| IY1| EY1| OW1| AO1| AY1| AH1| UW1| ER1| AW1| UH1| OY1', definitions=defs)\n",
    "defs['Vow1'] = Vow1\n",
    "Vow2 = hfst.regex('EH2| EY2| AE2| AY2| AA2| IH2| OW2| IY2| AO2| UW2| AH2| AW2| ER2| UH2| OY2', definitions=defs)\n",
    "defs['Vow2'] = Vow2\n",
    "\n",
    "Vow = hfst.regex('Vow0 | Vow1 | Vow2', definitions=defs)\n",
    "defs['Vow'] = Vow\n",
    "\n",
    "Nas = hfst.regex('N | M | NG', definitions=defs)\n",
    "defs['Nas'] = Nas\n",
    "\n",
    "Phone = hfst.regex('AH0| N| S| L| T| R| K| D| IH0| M| Z| ER0| IY0| B| EH1| P| AE1| AA1| IH1| F| G| V| IY1| NG| HH| EY1| W| SH| OW1| OW0| AO1| AY1| AH1| UW1| JH| Y| CH| AA0| ER1| EH2| EY2| AE2| AY2| AA2| EH0| IH2| TH| AW1| OW2| UW0| IY2| AO2| AE0| UH1| AO0| AY0| UW2| AH2| EY0| OY1| AW2| DH| ZH| ER2| UH2| AW0| UH0| OY2| OY0', definitions = defs)\n",
    "defs['Phone'] = Phone\n",
    "\n",
    "Cons = hfst.regex('[Phone - Vow]', definitions = defs)\n",
    "defs['Cons'] = Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f8ce8",
   "metadata": {},
   "source": [
    "## Creating Classes of Words Based on Stress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a2a69",
   "metadata": {},
   "source": [
    "We use hfst to create classes of words that we can use to construct iambic pentameter based on the stress classes provided to us: primary stress (s1), secondary stress (s2), and unstressed (s0). We have only created machines for the stress cases that are relevnat to the unstressed-stressed pattern of iambic pentameter, so for example, something like s0s0 would not be helpful, therefore we have excluded it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678d9a7",
   "metadata": {},
   "source": [
    "### One Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0cacc",
   "metadata": {},
   "source": [
    "#### Stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5eb9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeeb7b8",
   "metadata": {},
   "source": [
    "#### Unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b3bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bba20a",
   "metadata": {},
   "source": [
    "### Two Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fdd15",
   "metadata": {},
   "source": [
    "#### Main stress first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af72513",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df8093",
   "metadata": {},
   "source": [
    "#### Main stress second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44b687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2679a",
   "metadata": {},
   "source": [
    "### Three Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098528c",
   "metadata": {},
   "source": [
    "#### Stressed, unstressed, stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716ce8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s1\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s2\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s1\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s2\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s1\"] = q\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow2 Cons*]].l].u'\n",
    "r = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s2\"] = r\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "s = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s2s1\"] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b92153",
   "metadata": {},
   "source": [
    "#### Unstressed, stressed, unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1fa4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s2\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow2 Cons* Vow0 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s2s0\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s0\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s2\"] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a7a65",
   "metadata": {},
   "source": [
    "In some cases when constructing our vocabulary and our grammar, it became necessary to verify the stress pattern of a word. Below is the code we used to view the lower side representation of a given word based on the streamed in English dictionary in hfst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca80c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SAE0KUH1RAA2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_input(x,n=1,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "\n",
    "# Example word: sakura\n",
    "expr = '[{sakura} .o. English].l'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "sample_input(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d39b80",
   "metadata": {},
   "source": [
    "## Generating Rhyme Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcafdf",
   "metadata": {},
   "source": [
    "Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fa78b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Phone* VowAA Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAA\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAE Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAE\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAO Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAO\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAW\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowEH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeEH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowER Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeER\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowEY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeEY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowIH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeIH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowIY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeIY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowOW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeOW\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowUH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeUH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowUW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeUW\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61d422",
   "metadata": {},
   "source": [
    "Based on the rhyme classes we define above through the use of hfst, we use sampling to generate a vocabulary for our end rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c01cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['degro|ot',\n",
       " 'brandnew',\n",
       " \"kanemaru'|s\",\n",
       " 'so|undview',\n",
       " 'mildew',\n",
       " 'carto|ons',\n",
       " 'livin|gro|oms',\n",
       " 'supe|rco|oled',\n",
       " 'dru',\n",
       " 'esc|he|ws']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_input(x,n=10,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "\n",
    "# Example vowel sound: VowUW\n",
    "expr = '[English .o. [[ Phone* VowUW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "sample_input(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "062b0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "AW = [\"bloodhound\", \"crowns\", \"blackout\", \"reroute\", \"loud\", \"hometown\", \"scowl\", \"countdown\", \"rouse\", \"mount\"]\n",
    "AY = [\"devise\", \"privatize\", \"bribe\", \"modernize\", \"coincide\", \"chimes\", \"deprived\", \"reunite\", \"apprise\", \"knifelike\"]\n",
    "EH = [\"doorsteps\", \"aspects\", \"flare\", \"sleepwear\", \"pens\", \"pastel\", \"bullpen\", \"pipette\"]\n",
    "ER = [\"rewired\", \"spindler\", \"harvesters\", \"thunders\", \"lowered\", \"gander\", \"prisoners\", \"trimmer\", \"scholar\", \"modern\"]\n",
    "EY = [\"prepaid\", \"gateways\", \"blockades\", \"replace\", \"cliched\", \"acclimate\", \"drain\", \"birthdays\", \"upscale\", \"sedate\"]\n",
    "IH = [\"hallways\", \"parades\", \"dislocate\", \"hurricane\", \"escape\", \"downplay\", \"shortchange\", \"lace\", \"days\"]\n",
    "IY = [\"coyote\", \"squeaky\", \"delete\", \"cheek\", \"cream\", \"blackberry\", \"publicly\", \"blatantly\"]\n",
    "OW = [\"yolks\", \"chrome\", \"intone\", \"pronto\", \"sorrow\", \"disowned\", \"potatoes\", \"mole\", \"notes\"]\n",
    "OY = [\"decoy\", \"convoy\", \"noise\", \"annoy\", \"purloin\", \"steroid\", \"datapoint\", \"boy\", \"tabloids\", \"soy\"]\n",
    "UH = [\"wolves\", \"underwood\", \"scrapbooks\", \"cooked\", \"schedules\", \"cookbooks\", \"endure\", \"understood\", \"rook\", \"woods\"]\n",
    "UW = [\"balloons\", \"typhoons\", \"duped\", \"croon\", \"loon\", \"resume\", \"ingenue\", \"remove\", \"lawsuit\", \"troops\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f228ebb",
   "metadata": {},
   "source": [
    "Based on the lists generated above, we created a dictionary of words in our vocabulary mapped to the rhyme class of their last syllable. As this is a rather large dictionary, we wrote it to to an external text file that we then import below to use moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2c41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve rhyme dictionary from external file \n",
    "from rhyme_dict import rhyme_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c967105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'snowfall': 'AA',\n",
       " 'sakura': 'AA',\n",
       " 'enlarge': 'AA',\n",
       " 'jumpstart': 'AA',\n",
       " 'unharmed': 'AA',\n",
       " 'remark': 'AA',\n",
       " 'wasp': 'AA',\n",
       " 'resolved': 'AA',\n",
       " 'jock': 'AA',\n",
       " 'charm': 'AA',\n",
       " 'bask': 'AE',\n",
       " 'replant': 'AE',\n",
       " 'chasse': 'AE',\n",
       " 'hunchback': 'AE',\n",
       " 'woodland': 'AE',\n",
       " 'thrash': 'AE',\n",
       " 'catch': 'AE',\n",
       " 'ads': 'AE',\n",
       " 'fad': 'AE',\n",
       " 'mailbag': 'AE',\n",
       " 'oration': 'AH',\n",
       " 'straightened': 'AH',\n",
       " 'walnut': 'AH',\n",
       " 'abhorrent': 'AH',\n",
       " 'credence': 'AH',\n",
       " 'megaton': 'AH',\n",
       " 'puma': 'AH',\n",
       " 'stuffs': 'AH',\n",
       " 'junction': 'AH',\n",
       " 'patients': 'AH',\n",
       " 'troughs': 'AO',\n",
       " 'frauds': 'AO',\n",
       " 'meatballs': 'AO',\n",
       " 'imports': 'AO',\n",
       " 'lords': 'AO',\n",
       " 'hoar': 'AO',\n",
       " 'malformed': 'AO',\n",
       " 'billboard': 'AO',\n",
       " 'shorn': 'AO',\n",
       " 'thorns': 'AO',\n",
       " 'bloodhound': 'AW',\n",
       " 'crowns': 'AW',\n",
       " 'blackout': 'AW',\n",
       " 'reroute': 'AW',\n",
       " 'loud': 'AW',\n",
       " 'hometown': 'AW',\n",
       " 'scowl': 'AW',\n",
       " 'countdown': 'AW',\n",
       " 'rouse': 'AW',\n",
       " 'mount': 'AW',\n",
       " 'devise': 'AY',\n",
       " 'privatize': 'AY',\n",
       " 'bribe': 'AY',\n",
       " 'modernize': 'AY',\n",
       " 'coincide': 'AY',\n",
       " 'chimes': 'AY',\n",
       " 'deprived': 'AY',\n",
       " 'reunite': 'AY',\n",
       " 'apprise': 'AY',\n",
       " 'knifelike': 'AY',\n",
       " 'doorsteps': 'EH',\n",
       " 'aspects': 'EH',\n",
       " 'flare': 'EH',\n",
       " 'sleepwear': 'EH',\n",
       " 'pens': 'EH',\n",
       " 'pastel': 'EH',\n",
       " 'bullpen': 'EH',\n",
       " 'pipette': 'EH',\n",
       " 'rewired': 'ER',\n",
       " 'spindler': 'ER',\n",
       " 'harvesters': 'ER',\n",
       " 'thunders': 'ER',\n",
       " 'lowered': 'ER',\n",
       " 'gander': 'ER',\n",
       " 'prisoners': 'ER',\n",
       " 'trimmer': 'ER',\n",
       " 'scholar': 'ER',\n",
       " 'modern': 'ER',\n",
       " 'prepaid': 'EY',\n",
       " 'gateways': 'EY',\n",
       " 'blockades': 'EY',\n",
       " 'replace': 'EY',\n",
       " 'cliched': 'EY',\n",
       " 'acclimate': 'EY',\n",
       " 'drain': 'EY',\n",
       " 'birthdays': 'EY',\n",
       " 'upscale': 'EY',\n",
       " 'sedate': 'EY',\n",
       " 'hallways': 'IH',\n",
       " 'parades': 'IH',\n",
       " 'dislocate': 'IH',\n",
       " 'hurricane': 'IH',\n",
       " 'escape': 'IH',\n",
       " 'downplay': 'IH',\n",
       " 'shortchange': 'IH',\n",
       " 'lace': 'IH',\n",
       " 'days': 'IH',\n",
       " 'coyote': 'IY',\n",
       " 'squeaky': 'IY',\n",
       " 'delete': 'IY',\n",
       " 'cheek': 'IY',\n",
       " 'cream': 'IY',\n",
       " 'blackberry': 'IY',\n",
       " 'publicly': 'IY',\n",
       " 'blatantly': 'IY',\n",
       " 'yolks': 'OW',\n",
       " 'chrome': 'OW',\n",
       " 'intone': 'OW',\n",
       " 'pronto': 'OW',\n",
       " 'sorrow': 'OW',\n",
       " 'disowned': 'OW',\n",
       " 'potatoes': 'OW',\n",
       " 'mole': 'OW',\n",
       " 'notes': 'OW',\n",
       " 'decoy': 'OY',\n",
       " 'convoy': 'OY',\n",
       " 'noise': 'OY',\n",
       " 'annoy': 'OY',\n",
       " 'purloin': 'OY',\n",
       " 'steroid': 'OY',\n",
       " 'datapoint': 'OY',\n",
       " 'boy': 'OY',\n",
       " 'tabloids': 'OY',\n",
       " 'soy': 'OY',\n",
       " 'wolves': 'UH',\n",
       " 'underwood': 'UH',\n",
       " 'scrapbooks': 'UH',\n",
       " 'cooked': 'UH',\n",
       " 'schedules': 'UH',\n",
       " 'cookbooks': 'UH',\n",
       " 'endure': 'UH',\n",
       " 'understood': 'UH',\n",
       " 'rook': 'UH',\n",
       " 'woods': 'UH',\n",
       " 'balloons': 'UW',\n",
       " 'typhoons': 'UW',\n",
       " 'duped': 'UW',\n",
       " 'croon': 'UW',\n",
       " 'loon': 'UW',\n",
       " 'resume': 'UW',\n",
       " 'ingenue': 'UW',\n",
       " 'remove': 'UW',\n",
       " 'lawsuit': 'UW',\n",
       " 'troops': 'UW',\n",
       " 'arched': 'AA',\n",
       " 'default': 'AA',\n",
       " 'sidetracked': 'AE',\n",
       " 'unpacked': 'AE',\n",
       " 'brazen': 'AH',\n",
       " 'shaven': 'AH',\n",
       " 'bored': 'AO',\n",
       " 'restored': 'AO',\n",
       " 'endowed': 'AW',\n",
       " 'proud': 'AW',\n",
       " 'wild': 'AY',\n",
       " 'surprised': 'AY',\n",
       " 'inept': 'EH',\n",
       " 'brunette': 'EH',\n",
       " 'inner': 'ER',\n",
       " 'rotten': 'ER',\n",
       " 'disgraced': 'EY',\n",
       " 'handmade': 'EY',\n",
       " 'irate': 'IH',\n",
       " 'late': 'IH',\n",
       " 'leaky': 'IY',\n",
       " 'biweekly': 'IY',\n",
       " 'uncontrolled': 'OW',\n",
       " 'afloat': 'OW',\n",
       " 'destroyed': 'OY',\n",
       " 'adroit': 'OY',\n",
       " 'good': 'UH',\n",
       " 'beechwood': 'UH',\n",
       " 'removed': 'UW',\n",
       " 'cartoon': 'UW',\n",
       " 'farms': 'AA',\n",
       " 'attacks': 'AE',\n",
       " 'awakens': 'AH',\n",
       " 'explores': 'AO',\n",
       " 'drowns': 'AW',\n",
       " 'slice': 'AY',\n",
       " 'upsets': 'EH',\n",
       " 'acquires': 'ER',\n",
       " 'explains': 'EY',\n",
       " 'buffets': 'IH',\n",
       " 'repeats': 'IY',\n",
       " 'pokes': 'OW',\n",
       " 'employs': 'OY',\n",
       " 'hooks': 'UH',\n",
       " 'spoons': 'UW',\n",
       " 'cars': 'AA',\n",
       " 'arm': 'AA',\n",
       " 'stalks': 'AA',\n",
       " 'cap': 'AE',\n",
       " 'snacks': 'AE',\n",
       " 'patience': 'AH',\n",
       " 'agents': 'AH',\n",
       " 'core': 'AO',\n",
       " 'smores': 'AO',\n",
       " 'crowd': 'AW',\n",
       " 'clouds': 'AW',\n",
       " 'eye': 'AY',\n",
       " 'spies': 'AY',\n",
       " 'effect': 'EH',\n",
       " 'checks': 'EH',\n",
       " 'singer': 'ER',\n",
       " 'swimmers': 'ER',\n",
       " 'face': 'EY',\n",
       " 'rays': 'EY',\n",
       " 'plate': 'IH',\n",
       " 'rollerskates': 'IH',\n",
       " 'geek': 'IY',\n",
       " 'leeks': 'IY',\n",
       " 'troll': 'OW',\n",
       " 'souls': 'OW',\n",
       " 'choice': 'OY',\n",
       " 'toys': 'OY',\n",
       " 'neighborhoods': 'UH',\n",
       " 'foot': 'UH',\n",
       " 'lagoon': 'UW',\n",
       " 'baboons': 'UW',\n",
       " 'time': 'AY',\n",
       " 'times': 'AY',\n",
       " 'limes': 'AY',\n",
       " 'dinnertime': 'AY',\n",
       " 'paradigm': 'AY',\n",
       " 'windchime': 'AY'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhyme_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2558a4",
   "metadata": {},
   "source": [
    "## Generate Iambic Pentameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6210764",
   "metadata": {},
   "source": [
    "Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac412fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input and output\n",
    "\n",
    "def sample_input(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return random.sample(set(x2.extract_paths(max_cycles=3).keys()),n)[0].replace('|', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0965b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(wordClasses : list, defs) -> (str, str):\n",
    "    # [wordClasses] a LIST of [word class, frequency] lists, with word classes defined in [defs]\n",
    "    # Frequencies should add up to 1\n",
    "    # \n",
    "    # Returns: (word class, sample)\n",
    "    r = random.random()\n",
    "    for wordClass in wordClasses:\n",
    "        r -= wordClass[1]\n",
    "        if r < 0:\n",
    "            return (wordClass[0], sample_input(hfst.regex(wordClass[0], definitions=defs), n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fef2625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesToList(lst, wordClasses : dict):\n",
    "    result = []\n",
    "    sumFreq = 0\n",
    "    for wc in lst:\n",
    "        result.append([wc, wordClasses[wc]])\n",
    "        sumFreq += wordClasses[wc]\n",
    "    for i in range(len(result)):\n",
    "        result[i][1] /= sumFreq\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9546ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iambs(wordClasses : dict, defs):\n",
    "    # [wordClasses] a DICTIONARY mapping word classes (defined in [defs]) to frequencies\n",
    "    # Frequencies should add up to 1\n",
    "    # Each element: s1 (primary), s2 (secondary), s0 (unstressed)\n",
    "    syllables = []\n",
    "    words_out = []\n",
    "    index = 0\n",
    "    while index < 10:\n",
    "        if index == 0:\n",
    "            preLst = [\"s0\", \"s0s1\", \"s2s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\", \"s2s1s0\", \"s2s1s2\"]\n",
    "        elif index == 8:\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\"])\n",
    "        elif index == 9:\n",
    "            preLst = [\"s1\"]\n",
    "        else:\n",
    "            # Unstressed\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\", \"s2s1s0\", \"s2s1s2\"])\n",
    "            # Stressed\n",
    "            else:\n",
    "                preLst = [\"s1\", \"s1s0\", \"s1s2\", \"s1s0s1\", \"s1s0s2\", \"s1s2s1\", \"s1s2s2\"]\n",
    "                if syllables[index - 1] == \"s0\":\n",
    "                    preLst.extend([\"s2s0s2\", \"s2s0s1\", \"s2s2s1\"])\n",
    "                    \n",
    "        lst = classesToList(preLst, wordClasses)      \n",
    "        wordClass, word = sample(lst, defs)\n",
    "        wordSyl = wordClass.split(\"s\")\n",
    "        for syl in wordSyl:\n",
    "            if syl != \"\":   \n",
    "                syllables.append(\"s\" + syl) \n",
    "                index += 1\n",
    "        words_out.append(word)\n",
    "        \n",
    "    return words_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a398128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "class wordIterable:\n",
    "    def __init__(self, file, n):\n",
    "        p1 = load_parser(file, trace=0, cache=False)\n",
    "        g1 = p1.grammar()\n",
    "        self.parser = p1\n",
    "        self.grammar = g1\n",
    "        self.n = n\n",
    "        self.sents = []\n",
    "        gen1 = generate(g1, depth=2*n)\n",
    "        s_temp = list(gen1)\n",
    "        for s in s_temp:\n",
    "            if len(s) == self.n and self.check_grammatical(s) and s not in self.sents:\n",
    "                self.sents.append(s)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if len(self.sents) > 0:\n",
    "            i = randint(0, len(self.sents) - 1)\n",
    "            sent = self.sents[i]\n",
    "            del self.sents[i]\n",
    "            return sent\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def check_grammatical(self, s):\n",
    "        try:\n",
    "            t = next(self.parser.parse(s))\n",
    "            return True\n",
    "        except StopIteration:\n",
    "            return False\n",
    "        except ValueError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe355cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently encoding lists as dictionary.keys() makes everything run faster\n",
    "wordClasses = {\"s0\": 1/16, \"s1\": 1/16, \"s0s1\": 1/16, \"s1s0\": 1/16, \"s2s1\": 1/16, \"s1s2\": 1/16, \"s0s1s0\": 1/16, \"s0s1s2\": 1/16, \"s0s2s0\": 1/16, \"s2s1s0\": 1/16, \"s2s1s2\": 1/16, \"s1s0s1\": 1/16, \"s1s0s2\": 1/16, \"s1s2s1\": 1/16, \"s2s0s2\": 1/16, \"s2s0s1\": 1/16}\n",
    "rhymePatterns = {\"AA\": 0, \"AE\": 0, \"AH\": 0, \"AO\": 0, \"AW\": 0, \"AY\": 0, \"EH\": 0, \"ER\": 0, \"EY\": 0, \"IH\": 0, \"IY\": 0, \"OW\": 0, \"OY\": 0, \"UH\": 0, \"UW\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1763b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail = 0\n",
    "with open('dummy.fcfg', 'r') as f:\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            tail += 1\n",
    "            lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bc1b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_left(left):\n",
    "    if '[' not in left and left == 'S' or left.split('[')[0] == 'S':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "399b9cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_stress(left, right, sp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# sp: stress patterns for symbols 1 and 2 [sp1, sp2]\n",
    "# return (left, right)\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_stress = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_stress = left[:ind] + '_' + ''.join(sp) + left[ind:]\n",
    "        else:\n",
    "            left_stress = left + '_' + ''.join(sp)\n",
    "        \n",
    "    right_stress = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_stress.append(right[i][:ind] + '_' + sp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_stress.append(right[i] + '_' + sp[i])\n",
    "            \n",
    "    return left_stress + ' -> ' + right_stress[0] + ' ' + (right_stress[1] if len(right) > 1 else '')\n",
    "    #left_stress, right_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0a57a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_meter_add(sp1, sp2):\n",
    "    sp1end = 's' + sp1.split('s')[-1]\n",
    "    sp2begin = 's' + sp2.split('s')[1]\n",
    "    if sp1end == 's0':\n",
    "        if sp2begin == 's1':\n",
    "            return True\n",
    "        if sp2begin == 's0':\n",
    "            return False\n",
    "    if sp1end == 's1':\n",
    "        if sp2begin == 's0':\n",
    "            return True\n",
    "        if sp2begin == 's1':\n",
    "            return False\n",
    "    if sp1end == 's2':\n",
    "        sp1pu = 's' + sp1.split('s')[-2]\n",
    "        if sp1pu == 's0':\n",
    "            if sp2begin == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp1pu == 's1':\n",
    "            if sp2begin == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp1\")\n",
    "    if sp2begin == 's2':\n",
    "        sp2sec = 's' + sp2.split('s')[2]\n",
    "        if sp2sec == 's0':\n",
    "            if sp1end == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp2sec == 's1':\n",
    "            if sp1end == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b144a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iambic_pent(left, sp1, sp2):\n",
    "    if check_sentence_left(left):\n",
    "        if not check_meter_add(sp1, sp2):\n",
    "            return False\n",
    "        if len(sp1 + sp2) != 20:\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's1':\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's2' and len(sp1.split('s')[1:]) > 1 and 's' + sp1.split('s')[2] == 's0':\n",
    "            return False\n",
    "        return True\n",
    "    else:\n",
    "        return check_meter_add(sp1, sp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09517151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(wordClasses):\n",
    "    result = []\n",
    "    for sp in wordClasses:\n",
    "        result.append(sp)\n",
    "    \n",
    "    for sp1 in wordClasses:\n",
    "        for sp2 in wordClasses:\n",
    "            if check_meter_add(sp1, sp2):\n",
    "                result.append(sp1 + sp2)\n",
    "    \n",
    "    limitHit = False\n",
    "    for sp in result:\n",
    "        if len(sp) >= 10:\n",
    "            limitHit = True\n",
    "            break\n",
    "            \n",
    "    if not limitHit:\n",
    "        result = concat(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1d94311",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = wordClasses.keys()\n",
    "wc = concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25d594a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_grammar(wordClasses):\n",
    "    grammar_section = False\n",
    "    lexical_section = False\n",
    "    index = 0;\n",
    "\n",
    "    while grammar_section == False:\n",
    "        if lines[index] == '# Grammar Rules':\n",
    "            grammar_section = True\n",
    "        index += 1\n",
    "\n",
    "    new_grammar = set()\n",
    "    while lexical_section == False:\n",
    "        if lines[index] == '# Lexical Rules':\n",
    "            lexical_section = True\n",
    "        if ' -> ' in lines[index]:\n",
    "            left = lines[index].split(' -> ')[0]\n",
    "            right = lines[index].split(' -> ')[1].split(' ')\n",
    "            if len(right) == 1:\n",
    "                for sp in wordClasses:\n",
    "                    new_grammar.add(rule_add_stress(left, right, [sp]))\n",
    "            else:\n",
    "                for sp1 in wordClasses:\n",
    "                    for sp2 in wordClasses:\n",
    "                        if check_iambic_pent(left, sp1, sp2):\n",
    "                            new_grammar.add(rule_add_stress(left, right, [sp1, sp2]))\n",
    "                            \n",
    "        index += 1\n",
    "\n",
    "    lexical_rules = []\n",
    "    while index < tail:\n",
    "        lexical_rules.append(lines[index])\n",
    "        index += 1\n",
    "\n",
    "    \n",
    "    with open('dummynew.fcfg', 'w') as f:\n",
    "        f.write(\"% start S\\n\")\n",
    "        for rule in new_grammar:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')\n",
    "        for rule in lexical_rules:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d66bcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_grammar(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13168abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = load_parser(\"dummynew.fcfg\", trace=0, cache=False)\n",
    "gd = pd.grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcdcc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grammatical(p, s):\n",
    "    try:\n",
    "        t = next(p.parse(s))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "gen = generate(gd, depth=20)\n",
    "\n",
    "s = next(gen)\n",
    "generated_sents = []\n",
    "for i in range(100):\n",
    "    while not check_grammatical(pd, s) or s in generated_sents:\n",
    "        s = next(gen)\n",
    "    generated_sents.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb23f7",
   "metadata": {},
   "source": [
    "As expected, it takes a very long time to run the generations, so we have stored the results of our 100,000 sentence generation run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For stashing the results of generate in a file ##\n",
    "\n",
    "# with open(r'sentences.txt', 'w') as fp:\n",
    "#     for s in generated_sents:\n",
    "#         # write each item on a new line\n",
    "#         fp.write(\"%s\\n\" % s)\n",
    "#     print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250c736",
   "metadata": {},
   "source": [
    "As the generation function built into nltk has a very specific order to how it generates sentences, we shuffle the indices of teh generations to create as much variety as possible in the lines that go into our sonnets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle indices \n",
    "random.shuffle(generated_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6ea78",
   "metadata": {},
   "source": [
    "### Functions for Constructing Stanzas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf74b8",
   "metadata": {},
   "source": [
    "Below we tie together the many parts of our program into a handful of functions that generate the pieces that are combiend to create the structure of a sonnet. We have one function to create a quatrain-style stanza following the a/b/a/b rhyming pattern and anotehr function to produce a rhyming couplet, accompanied by assorted helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7a12ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers \n",
    "\n",
    "def check_line(line, poem_so_far):\n",
    "    \"\"\"\n",
    "    Verifies that a given word has only appeared in the existing body of \n",
    "    the sonnet no more than once before.\n",
    "    \"\"\"\n",
    "    for word in line:\n",
    "        count = 0\n",
    "        for line in poem_so_far:\n",
    "            if word in line:\n",
    "                count += 1\n",
    "            if count > 1 and word != 'the':\n",
    "                return False\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d28218a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random(g, start='S'):\n",
    "    if start[0] == \"'\" and start[-1] == \"'\":\n",
    "        return start\n",
    "    \n",
    "    randlst = []\n",
    "    for rule in g:\n",
    "        if rule.split(' -> ')[0] == start:\n",
    "            randlst.append(rule)\n",
    "            \n",
    "    #print(randlst)\n",
    "            \n",
    "    if len(randlst) == 0:\n",
    "        return None\n",
    "\n",
    "    rg = None\n",
    "    while rg == None:\n",
    "        s = ''\n",
    "        ri = random.randint(0, len(randlst) - 1)\n",
    "        r = randlst[ri]\n",
    "        right = r.split(' -> ')[1].split(' ')\n",
    "        #print(r)\n",
    "        for item in right:\n",
    "            rg = generate_random(g, start=item)\n",
    "            if rg == None:\n",
    "                break\n",
    "            if rg[0] == \"'\" and rg[-1] == \"'\":\n",
    "                rg = rg[1:-1]\n",
    "            s += rg + ' '\n",
    "        if len(randlst) == 1:\n",
    "            return None\n",
    "        else:\n",
    "            randlst = randlst[0:ri] + randlst[ri+1:len(randlst)]\n",
    "\n",
    "    return s[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d8a89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random(g, rhyme_dict, start='S', rhyme_with=None, avoid_word=None, syllables=0, prev_node=None):\n",
    "    if start[0] == \"'\" and start[-1] == \"'\":\n",
    "        word = start[1:-1]\n",
    "        if rhyme_with != None and 10 - syllables == len(prev_node.split('_')[1].split('s')[1:]) and rhyme_dict[word] != rhyme_with:\n",
    "            return None\n",
    "        else:\n",
    "            return [word]\n",
    "    \n",
    "    randlst = []\n",
    "    for rule in g:\n",
    "        if rule.split(' -> ')[0] == start:\n",
    "            randlst.append(rule)\n",
    "            \n",
    "    #print(randlst)\n",
    "            \n",
    "    if len(randlst) == 0:\n",
    "        return None\n",
    "\n",
    "    rg = None\n",
    "    while rg == None:\n",
    "        sentence = []\n",
    "        ri = random.randint(0, len(randlst) - 1)\n",
    "        r = randlst[ri]\n",
    "        left = r.split(' -> ')[0]\n",
    "        right = r.split(' -> ')[1].split(' ')\n",
    "        #print(r)\n",
    "        eliminate_options = True\n",
    "        #print(left, right)\n",
    "        for i, item in enumerate(right):\n",
    "            #print(right)\n",
    "            \"\"\"\n",
    "            if len(right) == 1 and right[0][0] == \"'\" and right[0][-1] == \"'\":\n",
    "                newSyllablesLeft = 0\n",
    "            elif i == len(right) - 1:\n",
    "                newSyllablesLeft = len(right[0].split('_')[1].split('s')[1:])\n",
    "            else:\n",
    "                newSyllablesLeft = len(right[1].split('_')[1].split('s')[1:])\n",
    "            \"\"\"\n",
    "            if i == 0:\n",
    "                syllablesNext = syllables\n",
    "            else:\n",
    "                syllablesNext = syllables + len(right[0].split('_')[1].split('s')[1:])\n",
    "            #print(left, right)\n",
    "            #print(syllablesNext)\n",
    "            rg = generate_random(g, rhyme_dict, start=item, rhyme_with=rhyme_with, avoid_word=avoid_word, syllables=syllablesNext, prev_node=left)\n",
    "            #print(rg)\n",
    "            if rg == None:\n",
    "                break\n",
    "            if avoid_word != None and rg[0] == avoid_word: # in avoid_words:\n",
    "                rg = None\n",
    "                #eliminate_options = False\n",
    "                break\n",
    "            for word in rg:\n",
    "                sentence.append(word)\n",
    "                \n",
    "        if eliminate_options == True:\n",
    "            if len(randlst) == 1:\n",
    "                return None\n",
    "            else:\n",
    "                randlst = randlst[0:ri] + randlst[ri+1:len(randlst)]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a4d899d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = set(['a', 'b'])\n",
    "random.sample(d, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0592fbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/s38h_sh56ws6tyb8shhjxqnw0000gn/T/ipykernel_42200/911304922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhyme_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhyme_with\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'AA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "print(generate_random(g, rhyme_dict, rhyme_with='AA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f7b0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abab_stanza(grammar_list, rhyme_dict, poem_so_far=[]):\n",
    "    first_sent = generate_random(grammar_list, rhyme_dict)\n",
    "    sents_list = poem_so_far\n",
    "    sents_list.append(first_sent)\n",
    "    a_word = first_sent[-1]\n",
    "    a = rhyme_dict[a_word]\n",
    "    \n",
    "    second_sent = generate_random(grammar_list, rhyme_dict, avoid_word=a_word) #, avoid_words=[a_word]\n",
    "    sents_list.append(second_sent)\n",
    "    b_word = second_sent[-1]\n",
    "    b = rhyme_dict[b_word]\n",
    "    \n",
    "    third_sent = generate_random(grammar_list, rhyme_dict, rhyme_with=a, avoid_word=a_word) #, avoid_words=[a_word, b_word]\n",
    "    sents_list.append(third_sent)\n",
    "    c_word = third_sent[-1]\n",
    "    \n",
    "    fourth_sent = generate_random(grammar_list, rhyme_dict, rhyme_with=b, avoid_word=b_word) #, avoid_words=[a_word, b_word, c_word]\n",
    "    sents_list.append(fourth_sent)\n",
    "    \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d50ec12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gg_couplet(grammar_list, rhyme_dict, poem_so_far):\n",
    "    first_sent = generate_random(grammar_list, rhyme_dict)\n",
    "    sents_list = poem_so_far\n",
    "    sents_list.append(first_sent)\n",
    "    a_word = first_sent[-1]\n",
    "    a = rhyme_dict[a_word]\n",
    "    \n",
    "    second_sent = generate_random(grammar_list, rhyme_dict, rhyme_with=a, avoid_word=a_word) #, avoid_words=[a_word]\n",
    "    sents_list.append(second_sent)\n",
    "    \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abab_stanza(grammar_list, rhyme_dict, poem_so_far=[]):\n",
    "    \"\"\"\n",
    "    Generates a quatrain with an a/b/a/b rhyme scheme based on the following parameters\n",
    "    \n",
    "    ARGUMENTS\n",
    "    =========\n",
    "    \n",
    "    generated_sents : list\n",
    "        generations produced from the grammar \n",
    "    \n",
    "    rhyme_dict : dict\n",
    "        dictionary of the rhyme associated with each word in the grammar \n",
    "    \n",
    "    poem_so_far : list of lists\n",
    "        the existing lines of the poem that have been accumulated so far\n",
    "    \"\"\"\n",
    "    \n",
    "    #r1 = random.randint(0, len(generated_sents) - 1)\n",
    "    first_sent = generate_random(grammar_list).split(' ')\n",
    "    sents_list = poem_so_far\n",
    "    print(first_sent)\n",
    "    \n",
    "    #while not check_line(first_sent, sents_list):\n",
    "        #r1 = random.randint(0, len(generated_sents) - 1)\n",
    "        #first_sent = generated_sents[r1]\n",
    "    \n",
    "    sents_list.append(first_sent)\n",
    "    \n",
    "    a_word = first_sent[-1]\n",
    "    a = rhyme_dict[a_word]\n",
    "    \n",
    "    #r2 = random.randint(0, len(generated_sents) - 1)\n",
    "    second_sent = generate_random(grammar_list).split(' ')\n",
    "    print(second_sent)\n",
    "    \n",
    "    #while not check_line(second_sent, sents_list) or second_sent[-1] == a_word:\n",
    "        #r2 = random.randint(0, len(generated_sents) - 1)\n",
    "        #second_sent = generated_sents[r2]\n",
    "    \n",
    "    sents_list.append(second_sent)\n",
    "    \n",
    "    b_word = second_sent[-1]\n",
    "    b = rhyme_dict[b_word]\n",
    "   \n",
    "    \n",
    "    # shuffle indicies \n",
    "    #inds1 = list(range(0, len(generated_sents)))\n",
    "    #random.shuffle(inds1)\n",
    "    \n",
    "    i = 0\n",
    "    while True: #i < len(inds1):\n",
    "        sent = generate_random(grammar_list).split(' ') #generated_sents[inds1[i]]\n",
    "        if rhyme_dict[sent[-1]] == a and sent not in sents_list and sent[-1] != a_word: #and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break;\n",
    "        i += 1\n",
    "        print(sent)\n",
    "        #if i == len(inds1):\n",
    "            #print(\"Sorry! Could not find a rhyme.\")\n",
    "\n",
    "    #inds2 = list(range(0, len(generated_sents)))\n",
    "    #random.shuffle(inds2)\n",
    "    \n",
    "    j = 0\n",
    "    while True: #j < len(inds2):\n",
    "        sent = generate_random(grammar_list).split(' ') #generated_sents[inds2[j]]                    \n",
    "        if rhyme_dict[sent[-1]] == b and sent not in sents_list and sent[-1] != b_word: #and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break; \n",
    "        j += 1\n",
    "        print(sent)\n",
    "        #if j == len(inds2):\n",
    "            #print(\"Sorry! Could not find a rhyme.\")\n",
    "    \n",
    "    return sents_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gg_couplet(grammar_list, rhyme_dict, poem_so_far):\n",
    "    \"\"\"\n",
    "    Generates a rhyming couplet based on the following parameters\n",
    "    \n",
    "    ARGUMENTS\n",
    "    =========\n",
    "    \n",
    "    generated_sents : list\n",
    "        generations produced from the grammar \n",
    "    rhyme_dict : dict\n",
    "        dictionary of the rhyme associated with each word in the grammar \n",
    "    poem_so_far : list of lists\n",
    "        the existing lines of the poem that have been accumulated so far\n",
    "    \"\"\"\n",
    "    \n",
    "    #r = random.randint(0, len(generated_sents) - 1)\n",
    "    sent = generate_random(grammar_list).split(' ') #generated_sents[r]\n",
    "    sents_list = poem_so_far\n",
    "    \n",
    "    #while not check_line(sent, sents_list):\n",
    "     #   r = random.randint(0, len(generated_sents) - 1)\n",
    "     #   sent = generated_sents[r]\n",
    "    \n",
    "    sents_list.append(sent)\n",
    "          \n",
    "    g_word = sent[-1]\n",
    "    g = rhyme_dict[g_word]\n",
    "    \n",
    "    #inds = list(range(0, len(generated_sents)))\n",
    "    #random.shuffle(inds)\n",
    "    \n",
    "    i = 0\n",
    "    while True: #i < len(inds):\n",
    "        sent = generate_random(grammar_list).split(' ') #generated_sents[inds[i]]          \n",
    "        if rhyme_dict[sent[-1]] == g and sent not in sents_list and sent[-1] != g_word: #and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break;\n",
    "        i +=1\n",
    "        #if i == len(inds):\n",
    "            #print(\"Sorry! Could not find a rhyme.\")\n",
    "        \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f6a1b",
   "metadata": {},
   "source": [
    "## Building the Sonnet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0e0b0",
   "metadata": {},
   "source": [
    "Now we get to put all of the pieces together to generate a sonnet! It is possible that running the below cell will result in the program saying it could not find a rhyme. This is due to the smaller size of our vocabulary and the constrainst placed around stress and rhyming. If that happens, simply run the the cell again, and a sonnet should be generated with new stress and rhyme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac30bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = []\n",
    "with open('dummynew.fcfg', 'r') as f:\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            g.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd708459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['its', 'beechwood', 'trimmer', 'farms', 'his', 'modern', 'days']\n",
      "['your', 'inner', 'tabloids', 'reunite', 'typhoons']\n",
      "['rewired', 'imports', 'bask', 'cartoon', 'parades']\n",
      "['their', 'beechwood', 'hunchback', 'pokes', 'the', 'rotten', 'troops']\n",
      "[['its', 'beechwood', 'trimmer', 'farms', 'his', 'modern', 'days'], ['your', 'inner', 'tabloids', 'reunite', 'typhoons'], ['rewired', 'imports', 'bask', 'cartoon', 'parades'], ['their', 'beechwood', 'hunchback', 'pokes', 'the', 'rotten', 'troops']]\n",
      "['my', 'squeaky', 'cap', 'attacks', 'inept', 'blockades']\n",
      "['the', 'shaven', 'swimmers', 'reunite', 'balloons']\n",
      "['unpacked', 'balloons', 'replace', 'our', 'straightened', 'face']\n",
      "['unpacked', 'unharmed', 'parades', 'replant', 'typhoons']\n",
      "[['its', 'beechwood', 'trimmer', 'farms', 'his', 'modern', 'days'], ['your', 'inner', 'tabloids', 'reunite', 'typhoons'], ['rewired', 'imports', 'bask', 'cartoon', 'parades'], ['their', 'beechwood', 'hunchback', 'pokes', 'the', 'rotten', 'troops'], ['my', 'squeaky', 'cap', 'attacks', 'inept', 'blockades'], ['the', 'shaven', 'swimmers', 'reunite', 'balloons'], ['unpacked', 'balloons', 'replace', 'our', 'straightened', 'face'], ['unpacked', 'unharmed', 'parades', 'replant', 'typhoons']]\n",
      "['her', 'handmade', 'flare', 'attacks', 'the', 'underwood']\n",
      "['rewired', 'souls', 'intone', 'abhorrent', 'times']\n",
      "['surprised', 'blockades', 'replant', 'our', 'leaky', 'wolves']\n",
      "['your', 'chrome', 'brunette', 'parades', 'intone', 'his', 'chimes']\n",
      "[['its', 'beechwood', 'trimmer', 'farms', 'his', 'modern', 'days'], ['your', 'inner', 'tabloids', 'reunite', 'typhoons'], ['rewired', 'imports', 'bask', 'cartoon', 'parades'], ['their', 'beechwood', 'hunchback', 'pokes', 'the', 'rotten', 'troops'], ['my', 'squeaky', 'cap', 'attacks', 'inept', 'blockades'], ['the', 'shaven', 'swimmers', 'reunite', 'balloons'], ['unpacked', 'balloons', 'replace', 'our', 'straightened', 'face'], ['unpacked', 'unharmed', 'parades', 'replant', 'typhoons'], ['her', 'handmade', 'flare', 'attacks', 'the', 'underwood'], ['rewired', 'souls', 'intone', 'abhorrent', 'times'], ['surprised', 'blockades', 'replant', 'our', 'leaky', 'wolves'], ['your', 'chrome', 'brunette', 'parades', 'intone', 'his', 'chimes']]\n",
      "['its', 'rotten', 'patients', 'slice', 'sedate', 'typhoons']\n",
      "[['its', 'beechwood', 'trimmer', 'farms', 'his', 'modern', 'days'], ['your', 'inner', 'tabloids', 'reunite', 'typhoons'], ['rewired', 'imports', 'bask', 'cartoon', 'parades'], ['their', 'beechwood', 'hunchback', 'pokes', 'the', 'rotten', 'troops'], ['my', 'squeaky', 'cap', 'attacks', 'inept', 'blockades'], ['the', 'shaven', 'swimmers', 'reunite', 'balloons'], ['unpacked', 'balloons', 'replace', 'our', 'straightened', 'face'], ['unpacked', 'unharmed', 'parades', 'replant', 'typhoons'], ['her', 'handmade', 'flare', 'attacks', 'the', 'underwood'], ['rewired', 'souls', 'intone', 'abhorrent', 'times'], ['surprised', 'blockades', 'replant', 'our', 'leaky', 'wolves'], ['your', 'chrome', 'brunette', 'parades', 'intone', 'his', 'chimes'], ['its', 'rotten', 'patients', 'slice', 'sedate', 'typhoons'], ['our', 'good', 'oration', 'pokes', 'adroit', 'balloons']]\n"
     ]
    }
   ],
   "source": [
    "first_stage = generate_abab_stanza(g, rhyme_dict, [])\n",
    "print(first_stage)\n",
    "second_stage = generate_abab_stanza(g, rhyme_dict, first_stage)\n",
    "print(second_stage)\n",
    "third_stage = generate_abab_stanza(g, rhyme_dict, second_stage)\n",
    "print(third_stage)\n",
    "final_sonnet = generate_gg_couplet(g, rhyme_dict, third_stage)\n",
    "print(final_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb39bc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================SONNET================\n",
      "\n",
      "its beechwood trimmer farms his modern days\n",
      "your inner tabloids reunite typhoons\n",
      "rewired imports bask cartoon parades\n",
      "their beechwood hunchback pokes the rotten troops\n",
      "my squeaky cap attacks inept blockades\n",
      "the shaven swimmers reunite balloons\n",
      "unpacked balloons replace our straightened face\n",
      "unpacked unharmed parades replant typhoons\n",
      "her handmade flare attacks the underwood\n",
      "rewired souls intone abhorrent times\n",
      "surprised blockades replant our leaky wolves\n",
      "your chrome brunette parades intone his chimes\n",
      "its rotten patients slice sedate typhoons\n",
      "our good oration pokes adroit balloons\n"
     ]
    }
   ],
   "source": [
    "print(\"=================SONNET================\" + \"\\n\")\n",
    "for line in final_sonnet:\n",
    "    print(*line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed27f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
