{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4a134d",
   "metadata": {},
   "source": [
    "## Project Members:\n",
    "    Crystal Zhu (cyz22)\n",
    "    Lyra D'Souza (lad279)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8eae3e",
   "metadata": {},
   "source": [
    "## Brief Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e92c3",
   "metadata": {},
   "source": [
    "Describe what we intended "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f7964",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3214c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import hfst_dev as hfst\n",
    "import graphviz\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import CFG\n",
    "from nltk import grammar, parse\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.parse.util import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c853a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream English \n",
    "\n",
    "istream = hfst.HfstInputStream('English')\n",
    "assert istream.is_good() == True\n",
    "English = istream.read()\n",
    "istream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457447eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up definitions from phoneclass.fst \n",
    "\n",
    "defs = {'English' : English}\n",
    "\n",
    "VowAA = hfst.regex('AA0 | AA1 | AA2', definitions=defs)\n",
    "defs['VowAA'] = VowAA\n",
    "VowAE = hfst.regex('AE0 | AE1 | AE2', definitions=defs)\n",
    "defs['VowAE'] = VowAE\n",
    "VowAH = hfst.regex('AH0 | AH1 | AH2', definitions=defs)\n",
    "defs['VowAH'] = VowAH\n",
    "VowAO = hfst.regex('AO0 | AO1 | AO2', definitions=defs)\n",
    "defs['VowAO'] = VowAO\n",
    "VowAW = hfst.regex('AW0 | AW1 | AW2', definitions=defs)\n",
    "defs['VowAW'] = VowAW\n",
    "VowAY = hfst.regex('AY0 | AY1 | AY2', definitions=defs)\n",
    "defs['VowAY'] = VowAY\n",
    "VowEH = hfst.regex('EH0 | EH1 | EH2', definitions=defs)\n",
    "defs['VowEH'] = VowEH\n",
    "VowER = hfst.regex('ER0 | ER1 | ER2', definitions=defs)\n",
    "defs['VowER'] = VowER\n",
    "VowEY = hfst.regex('EY0 | EY1 | EY2', definitions=defs)\n",
    "defs['VowEY'] = VowEY\n",
    "VowIH = hfst.regex('IH0 | IH1 | IH2', definitions=defs)\n",
    "defs['VowIH'] = VowIH\n",
    "VowIY = hfst.regex('IY0 | IY1 | IY2', definitions=defs)\n",
    "defs['VowIY'] = VowIY\n",
    "VowOW = hfst.regex('OW0 | OW1 | OW2', definitions=defs)\n",
    "defs['VowOW'] = VowOW\n",
    "VowOY = hfst.regex('OY0 | OY1 | OY2', definitions=defs)\n",
    "defs['VowOY'] = VowOY\n",
    "VowUH = hfst.regex('UH0 | UH1 | UH2', definitions=defs)\n",
    "defs['VowUH'] = VowUH\n",
    "VowUW = hfst.regex('UW0 | UW1 | UW2', definitions=defs)\n",
    "defs['VowUW'] = VowUW\n",
    "\n",
    "Vow0 = hfst.regex('AH0| IH0| ER0| IY0| OW0| AA0| EH0| UW0| AE0| AO0| AY0| EY0| AW0| UH0| OY0', definitions=defs)\n",
    "defs['Vow0'] = Vow0\n",
    "Vow1 = hfst.regex('EH1| AE1| AA1| IH1| IY1| EY1| OW1| AO1| AY1| AH1| UW1| ER1| AW1| UH1| OY1', definitions=defs)\n",
    "defs['Vow1'] = Vow1\n",
    "Vow2 = hfst.regex('EH2| EY2| AE2| AY2| AA2| IH2| OW2| IY2| AO2| UW2| AH2| AW2| ER2| UH2| OY2', definitions=defs)\n",
    "defs['Vow2'] = Vow2\n",
    "\n",
    "Vow = hfst.regex('Vow0 | Vow1 | Vow2', definitions=defs)\n",
    "defs['Vow'] = Vow\n",
    "\n",
    "Nas = hfst.regex('N | M | NG', definitions=defs)\n",
    "defs['Nas'] = Nas\n",
    "\n",
    "Phone = hfst.regex('AH0| N| S| L| T| R| K| D| IH0| M| Z| ER0| IY0| B| EH1| P| AE1| AA1| IH1| F| G| V| IY1| NG| HH| EY1| W| SH| OW1| OW0| AO1| AY1| AH1| UW1| JH| Y| CH| AA0| ER1| EH2| EY2| AE2| AY2| AA2| EH0| IH2| TH| AW1| OW2| UW0| IY2| AO2| AE0| UH1| AO0| AY0| UW2| AH2| EY0| OY1| AW2| DH| ZH| ER2| UH2| AW0| UH0| OY2| OY0', definitions = defs)\n",
    "defs['Phone'] = Phone\n",
    "\n",
    "Cons = hfst.regex('[Phone - Vow]', definitions = defs)\n",
    "defs['Cons'] = Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f8ce8",
   "metadata": {},
   "source": [
    "## Creating Classes of Words Based on Stress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a2a69",
   "metadata": {},
   "source": [
    "Only created classes we could use: for example s0s0 wouldnt be helpful to us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678d9a7",
   "metadata": {},
   "source": [
    "### One Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0cacc",
   "metadata": {},
   "source": [
    "#### Stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5eb9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeeb7b8",
   "metadata": {},
   "source": [
    "#### Unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b3bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bba20a",
   "metadata": {},
   "source": [
    "### Two Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fdd15",
   "metadata": {},
   "source": [
    "#### Main stress first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af72513",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df8093",
   "metadata": {},
   "source": [
    "#### Main stress second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c44b687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2679a",
   "metadata": {},
   "source": [
    "### Three Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098528c",
   "metadata": {},
   "source": [
    "#### Stressed, unstressed, stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716ce8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s1\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s2\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s1\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s2\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s1\"] = q\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow2 Cons*]].l].u'\n",
    "r = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s2\"] = r\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "s = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s2s1\"] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b92153",
   "metadata": {},
   "source": [
    "#### Unstressed, stressed, unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1fa4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s2\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow2 Cons* Vow0 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s2s0\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s0\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s2\"] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2558a4",
   "metadata": {},
   "source": [
    "## Generate Iambic Pentameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac412fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input and output\n",
    "\n",
    "def sample_input(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        word = None\n",
    "        dictionary = ['upright', 'abundant', 'annoying', 'adversely', 'quickly', 'very', 'and', 'or', 'but', 'every', 'some', 'no', 'a', 'that', 'who', 'whom', 'risotto', 'aqueduct', 'aqueducts', 'wok', 'woks', 'guitar', 'guitars', 'trustee', 'trustees', 'accredits', 'accredit']\n",
    "        while word == None or word not in dictionary:\n",
    "            word = random.sample(set(x2.extract_paths(max_cycles=3).keys()),n)[0].replace('|', '')\n",
    "        return word\n",
    "def sample_output(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.output_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0965b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(wordClasses : list, defs) -> (str, str):\n",
    "    # [wordClasses] a LIST of [word class, frequency] lists, with word classes defined in [defs]\n",
    "    # Frequencies should add up to 1\n",
    "    # \n",
    "    # Returns: (word class, sample)\n",
    "    r = random.random()\n",
    "    for wordClass in wordClasses:\n",
    "        r -= wordClass[1]\n",
    "        if r < 0:\n",
    "            return (wordClass[0], sample_input(hfst.regex(wordClass[0], definitions=defs), n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef2625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesToList(lst, wordClasses : dict):\n",
    "    result = []\n",
    "    sumFreq = 0\n",
    "    for wc in lst:\n",
    "        result.append([wc, wordClasses[wc]])\n",
    "        sumFreq += wordClasses[wc]\n",
    "    for i in range(len(result)):\n",
    "        result[i][1] /= sumFreq\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9546ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iambs(wordClasses : dict, defs):\n",
    "    # [wordClasses] a DICTIONARY mapping word classes (defined in [defs]) to frequencies\n",
    "    # Frequencies should add up to 1\n",
    "    # Each element: s1 (primary), s2 (secondary), s0 (unstressed)\n",
    "    syllables = []\n",
    "    words_out = []\n",
    "    index = 0\n",
    "    while index < 10:\n",
    "        if index == 0:\n",
    "            preLst = [\"s0\", \"s0s1\", \"s2s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\", \"s2s1s0\", \"s2s1s2\"]\n",
    "        elif index == 8:\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\"])\n",
    "        elif index == 9:\n",
    "            preLst = [\"s1\"]\n",
    "        else:\n",
    "            # Unstressed\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\", \"s2s1s0\", \"s2s1s2\"])\n",
    "            # Stressed\n",
    "            else:\n",
    "                preLst = [\"s1\", \"s1s0\", \"s1s2\", \"s1s0s1\", \"s1s0s2\", \"s1s2s1\", \"s1s2s2\"]\n",
    "                if syllables[index - 1] == \"s0\":\n",
    "                    preLst.extend([\"s2s0s2\", \"s2s0s1\", \"s2s2s1\"])\n",
    "                    \n",
    "        lst = classesToList(preLst, wordClasses)      \n",
    "        wordClass, word = sample(lst, defs)\n",
    "        wordSyl = wordClass.split(\"s\")\n",
    "        for syl in wordSyl:\n",
    "            if syl != \"\":   \n",
    "                syllables.append(\"s\" + syl) \n",
    "                index += 1\n",
    "        words_out.append(word)\n",
    "        \n",
    "    return words_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe8c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordClasses = {\"s0\": 1/16, \"s1\": 1/16, \"s0s1\": 1/16, \"s1s0\": 1/16, \"s2s1\": 1/16, \"s1s2\": 1/16, \"s0s1s0\": 1/16, \"s0s1s2\": 1/16, \"s0s2s0\": 1/16, \"s2s1s0\": 1/16, \"s2s1s2\": 1/16, \"s1s0s1\": 1/16, \"s1s0s2\": 1/16, \"s1s2s1\": 1/16, \"s2s0s2\": 1/16, \"s2s0s1\": 1/16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc3bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grammatical(s):\n",
    "    try:\n",
    "        t = next(p.parse(s))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a9194f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = load_parser(\"grammar.fcfg\", trace=0, cache=False)\n",
    "g = p.grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1044bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49508f50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/3705285304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_grammatical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_iambs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordClasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/2947860502.py\u001b[0m in \u001b[0;36mgenerate_iambs\u001b[0;34m(wordClasses, defs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassesToList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreLst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mwordClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mwordSyl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msyl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordSyl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/1584782538.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(wordClasses, defs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mwordClass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwordClass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhfst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordClass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefinitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/1737598299.py\u001b[0m in \u001b[0;36msample_input\u001b[0;34m(x, n, cycles)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'upright'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'abundant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annoying'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adversely'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quickly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'very'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'and'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'or'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'but'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'every'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'some'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'no'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'that'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'who'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'whom'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'risotto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aqueduct'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aqueducts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wok'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'woks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'guitar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'guitars'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trustee'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trustees'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accredits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accredit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_cycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4744/p3env/lib/python3.7/site-packages/libhfst_dev.py\u001b[0m in \u001b[0;36mextract_paths\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   4547\u001b[0m               \u001b[0mretval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_random_paths_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4549\u001b[0;31m               \u001b[0mretval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_paths_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cycles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4550\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4744/p3env/lib/python3.7/site-packages/libhfst_dev.py\u001b[0m in \u001b[0;36m_extract_paths_fd\u001b[0;34m(self, max_num, cycles, filter_fd)\u001b[0m\n\u001b[1;32m   4106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_paths_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_libhfst_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHfstTransducer__extract_paths_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_fd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_random_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sent = None\n",
    "while sent == None or not check_grammatical(sent):\n",
    "    sent = generate_iambs(wordClasses, defs)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'some actor whom a black cat that chases a shiny rat quickly chases chases small dogs'.split()\n",
    "t = next(p.parse(s))\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a398128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "class wordIterable:\n",
    "    def __init__(self, file, n):\n",
    "        p1 = load_parser(file, trace=0, cache=False)\n",
    "        g1 = p1.grammar()\n",
    "        self.parser = p1\n",
    "        self.grammar = g1\n",
    "        self.n = n\n",
    "        self.sents = []\n",
    "        gen1 = generate(g1, depth=2*n)\n",
    "        s_temp = list(gen1)\n",
    "        for s in s_temp:\n",
    "            if len(s) == self.n and self.check_grammatical(s) and s not in self.sents:\n",
    "                self.sents.append(s)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if len(self.sents) > 0:\n",
    "            i = randint(0, len(self.sents) - 1)\n",
    "            sent = self.sents[i]\n",
    "            del self.sents[i]\n",
    "            return sent\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def check_grammatical(self, s):\n",
    "        try:\n",
    "            t = next(self.parser.parse(s))\n",
    "            return True\n",
    "        except StopIteration:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a703e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'risotto', 'is', 'upright']\n",
      "['every', 'risotto', 'is', 'abundant']\n",
      "['every', 'risotto', 'is', 'annoying']\n",
      "['every', 'risotto', 'was', 'upright']\n",
      "['every', 'risotto', 'was', 'abundant']\n",
      "['every', 'risotto', 'was', 'annoying']\n",
      "['every', 'aqueduct', 'is', 'upright']\n",
      "['every', 'aqueduct', 'is', 'abundant']\n",
      "['every', 'aqueduct', 'is', 'annoying']\n",
      "['every', 'aqueduct', 'was', 'upright']\n",
      "['every', 'aqueduct', 'was', 'abundant']\n",
      "['every', 'aqueduct', 'was', 'annoying']\n",
      "['every', 'wok', 'is', 'upright']\n",
      "['every', 'wok', 'is', 'abundant']\n",
      "['every', 'wok', 'is', 'annoying']\n",
      "['every', 'wok', 'was', 'upright']\n",
      "['every', 'wok', 'was', 'abundant']\n",
      "['every', 'wok', 'was', 'annoying']\n",
      "['every', 'guitar', 'is', 'upright']\n",
      "['every', 'guitar', 'is', 'abundant']\n",
      "['every', 'guitar', 'is', 'annoying']\n",
      "['every', 'guitar', 'was', 'upright']\n",
      "['every', 'guitar', 'was', 'abundant']\n",
      "['every', 'guitar', 'was', 'annoying']\n",
      "['every', 'trustee', 'is', 'upright']\n",
      "['every', 'trustee', 'is', 'abundant']\n",
      "['every', 'trustee', 'is', 'annoying']\n",
      "['every', 'trustee', 'was', 'upright']\n",
      "['every', 'trustee', 'was', 'abundant']\n",
      "['every', 'trustee', 'was', 'annoying']\n",
      "['some', 'risotto', 'is', 'upright']\n",
      "['some', 'risotto', 'is', 'abundant']\n",
      "['some', 'risotto', 'is', 'annoying']\n",
      "['some', 'risotto', 'was', 'upright']\n",
      "['some', 'risotto', 'was', 'abundant']\n",
      "['some', 'risotto', 'was', 'annoying']\n",
      "['some', 'aqueduct', 'is', 'upright']\n",
      "['some', 'aqueduct', 'is', 'abundant']\n",
      "['some', 'aqueduct', 'is', 'annoying']\n",
      "['some', 'aqueduct', 'was', 'upright']\n",
      "['some', 'aqueduct', 'was', 'abundant']\n",
      "['some', 'aqueduct', 'was', 'annoying']\n",
      "['some', 'aqueducts', 'are', 'upright']\n",
      "['some', 'aqueducts', 'are', 'abundant']\n",
      "['some', 'aqueducts', 'are', 'annoying']\n",
      "['some', 'aqueducts', 'were', 'upright']\n",
      "['some', 'aqueducts', 'were', 'abundant']\n",
      "['some', 'aqueducts', 'were', 'annoying']\n",
      "['some', 'wok', 'is', 'upright']\n",
      "['some', 'wok', 'is', 'abundant']\n"
     ]
    }
   ],
   "source": [
    "gen = generate(g, depth=4)\n",
    "\n",
    "generated_sents = []\n",
    "for i in range(50):\n",
    "    s = next(gen)\n",
    "    while not check_grammatical(s) or s in generated_sents:\n",
    "        s = next(gen)\n",
    "    generated_sents.append(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fe355cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently encoding lists as dictionary.keys() makes everything run faster\n",
    "wordClasses = {\"s0\": 1/16, \"s1\": 1/16, \"s0s1\": 1/16, \"s1s0\": 1/16, \"s2s1\": 1/16, \"s1s2\": 1/16, \"s0s1s0\": 1/16, \"s0s1s2\": 1/16, \"s0s2s0\": 1/16, \"s2s1s0\": 1/16, \"s2s1s2\": 1/16, \"s1s0s1\": 1/16, \"s1s0s2\": 1/16, \"s1s2s1\": 1/16, \"s2s0s2\": 1/16, \"s2s0s1\": 1/16}\n",
    "rhymePatterns = {\"AA\": 0, \"AE\": 0, \"AH\": 0, \"AO\": 0, \"AW\": 0, \"AY\": 0, \"EH\": 0, \"ER\": 0, \"EY\": 0, \"IH\": 0, \"IY\": 0, \"OW\": 0, \"OY\": 0, \"UH\": 0, \"UW\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1763b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail = 0\n",
    "with open('dummy.fcfg', 'r') as f:\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            tail += 1\n",
    "            lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1bc1b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_left(left):\n",
    "    if '[' not in left and left == 'S' or left.split('[')[0] == 'S':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "399b9cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_stress(left, right, sp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# sp: stress patterns for symbols 1 and 2 [sp1, sp2]\n",
    "# return (left, right)\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_stress = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_stress = left[:ind] + '_' + ''.join(sp) + left[ind:]\n",
    "        else:\n",
    "            left_stress = left + '_' + ''.join(sp)\n",
    "        \n",
    "    right_stress = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_stress.append(right[i][:ind] + '_' + sp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_stress.append(right[i] + '_' + sp[i])\n",
    "            \n",
    "    return left_stress + ' -> ' + right_stress[0] + ' ' + (right_stress[1] if len(right) > 1 else '')\n",
    "    #left_stress, right_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1cdf8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_rhyme(left, right, rp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# rp: rhyme patterns for symbols 1 and 2 [rp1, rp2]\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_rhyme = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_rhyme = left[:ind] + '_' + rp[-1] + left[ind:]\n",
    "        else:\n",
    "            left_rhyme = left + '_' + rp[-1]\n",
    "    \n",
    "    right_rhyme = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_rhyme.append(right[i][:ind] + '_' + rp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_rhyme.append(right[i] + '_' + rp[i])\n",
    "    \n",
    "    return left_rhyme + ' -> ' + right_rhyme[0] + ' ' + (right_rhyme[1] if len(right) > 1 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a0a57a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_meter_add(sp1, sp2):\n",
    "    sp1end = 's' + sp1.split('s')[-1]\n",
    "    sp2begin = 's' + sp2.split('s')[1]\n",
    "    if sp1end == 's0':\n",
    "        if sp2begin == 's1':\n",
    "            return True\n",
    "        if sp2begin == 's0':\n",
    "            return False\n",
    "    if sp1end == 's1':\n",
    "        if sp2begin == 's0':\n",
    "            return True\n",
    "        if sp2begin == 's1':\n",
    "            return False\n",
    "    if sp1end == 's2':\n",
    "        sp1pu = 's' + sp1.split('s')[-2]\n",
    "        if sp1pu == 's0':\n",
    "            if sp2begin == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp1pu == 's1':\n",
    "            if sp2begin == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp1\")\n",
    "    if sp2begin == 's2':\n",
    "        sp2sec = 's' + sp2.split('s')[2]\n",
    "        if sp2sec == 's0':\n",
    "            if sp1end == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp2sec == 's1':\n",
    "            if sp1end == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3b144a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iambic_pent(left, sp1, sp2):\n",
    "    if check_sentence_left(left):\n",
    "        if not check_meter_add(sp1, sp2):\n",
    "            return False\n",
    "        if len(sp1 + sp2) != 20:\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's1':\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's2' and len(sp1.split('s')[1:]) > 1 and 's' + sp1.split('s')[2] == 's0':\n",
    "            return False\n",
    "        return True\n",
    "    else:\n",
    "        return check_meter_add(sp1, sp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09517151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(wordClasses):\n",
    "    result = []\n",
    "    for sp in wordClasses:\n",
    "        result.append(sp)\n",
    "    \n",
    "    for sp1 in wordClasses:\n",
    "        for sp2 in wordClasses:\n",
    "            if check_meter_add(sp1, sp2):\n",
    "                result.append(sp1 + sp2)\n",
    "    \n",
    "    limitHit = False\n",
    "    for sp in result:\n",
    "        if len(sp) >= 10:\n",
    "            limitHit = True\n",
    "            break\n",
    "            \n",
    "    if not limitHit:\n",
    "        result = concat(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c1d94311",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = wordClasses.keys()\n",
    "wc = concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "25d594a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_grammar(wordClasses):\n",
    "    grammar_section = False\n",
    "    lexical_section = False\n",
    "    index = 0;\n",
    "\n",
    "    while grammar_section == False:\n",
    "        if lines[index] == '# Grammar Rules':\n",
    "            grammar_section = True\n",
    "        index += 1\n",
    "\n",
    "    new_grammar = set()\n",
    "    while lexical_section == False:\n",
    "        if lines[index] == '# Lexical Rules':\n",
    "            lexical_section = True\n",
    "        if ' -> ' in lines[index]:\n",
    "            left = lines[index].split(' -> ')[0]\n",
    "            right = lines[index].split(' -> ')[1].split(' ')\n",
    "            if len(right) == 1:\n",
    "                for sp in wordClasses:\n",
    "                    new_grammar.add(rule_add_stress(left, right, [sp]))\n",
    "            else:\n",
    "                for sp1 in wordClasses:\n",
    "                    for sp2 in wordClasses:\n",
    "                        if check_iambic_pent(left, sp1, sp2):\n",
    "                            new_grammar.add(rule_add_stress(left, right, [sp1, sp2]))\n",
    "                            #for rp1 in rhymePatterns:\n",
    "                                #for rp2 in rhymePatterns:\n",
    "                                    #sleft, sright = rule_add_stress(left, right, [sp1, sp2])\n",
    "                                    #rule_add_rhyme(sleft, sright, [rp1, rp2])\n",
    "                            \n",
    "        index += 1\n",
    "\n",
    "    lexical_rules = []\n",
    "    while index < tail:\n",
    "        lexical_rules.append(lines[index])\n",
    "        index += 1\n",
    "\n",
    "    \n",
    "    with open('dummynew.fcfg', 'w') as f:\n",
    "        f.write(\"% start S\\n\")\n",
    "        for rule in new_grammar:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')\n",
    "        for rule in lexical_rules:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d66bcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_grammar(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b443e378",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_grammar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/3748289772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_grammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_grammar' is not defined"
     ]
    }
   ],
   "source": [
    "# len(new_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13168abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = load_parser(\"dummynew.fcfg\", trace=0, cache=False)\n",
    "gd = pd.grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a9c28e67",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/556051789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'potatoes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'escape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ads'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#td = next(pd.parse(s))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#td = next(pd.parse(s))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#td = next(pd.parse(s))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = ['potatoes', 'escape', 'the', 'ads']\n",
    "td = next(pd.parse(s))\n",
    "#td = next(pd.parse(s))\n",
    "#td = next(pd.parse(s))\n",
    "#td = next(pd.parse(s))\n",
    "td.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fcdcc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grammatical(p, s):\n",
    "    try:\n",
    "        t = next(p.parse(s))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "gen = generate(gd, depth=20)\n",
    "\n",
    "s = next(gen)\n",
    "generated_sents = []\n",
    "for i in range(100000):\n",
    "    while not check_grammatical(pd, s) or s in generated_sents:\n",
    "        s = next(gen)\n",
    "    # print(s)\n",
    "    generated_sents.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c52909d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(generated_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6ebb32a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "## For stashing the results of generate in a file ##\n",
    "\n",
    "# with open(r'sentences.txt', 'w') as fp:\n",
    "#     for s in generated_sents:\n",
    "#         # write each item on a new line\n",
    "#         fp.write(\"%s\\n\" % s)\n",
    "#     print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d93be196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve information about rhyme \n",
    "from rhyme_dict import rhyme_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "90f3525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line(line, poem_so_far):\n",
    "    for word in line:\n",
    "        count = 0\n",
    "        for line in poem_so_far:\n",
    "            if word in line:\n",
    "                count += 1\n",
    "            if count > 1 and word != 'the':\n",
    "                return False\n",
    "    return True \n",
    "            \n",
    "\n",
    "def generate_abab_stanza(generated_sents, rhyme_dict, poem_so_far=[]):\n",
    "    \n",
    "    r1 = random.randint(0, len(generated_sents) - 1)\n",
    "    first_sent = generated_sents[r1]\n",
    "    sents_list = poem_so_far\n",
    "    \n",
    "    while not check_line(first_sent, sents_list):\n",
    "        r1 = random.randint(0, len(generated_sents) - 1)\n",
    "        first_sent = generated_sents[r1]\n",
    "    \n",
    "\n",
    "    sents_list.append(first_sent)\n",
    "    \n",
    "    a_word = first_sent[-1]\n",
    "    a = rhyme_dict[a_word]\n",
    "    \n",
    "    r2 = random.randint(0, len(generated_sents) - 1)\n",
    "    second_sent = generated_sents[r2]\n",
    "    \n",
    "    while not check_line(second_sent, sents_list) or second_sent[-1] == a_word:\n",
    "        r2 = random.randint(0, len(generated_sents) - 1)\n",
    "        second_sent = generated_sents[r2]\n",
    "    \n",
    "    sents_list.append(second_sent)\n",
    "    \n",
    "    b_word = second_sent[-1]\n",
    "    b = rhyme_dict[b_word]\n",
    "   \n",
    "    \n",
    "    # shuffle indicies \n",
    "    inds1 = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds1)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(inds1):\n",
    "        sent = generated_sents[inds1[i]]\n",
    "        if rhyme_dict[sent[-1]] == a and sent not in sents_list and sent[-1] != a_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break;\n",
    "        i += 1\n",
    "        if i == len(inds1):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "\n",
    "    inds2 = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds2)\n",
    "    \n",
    "    j = 0\n",
    "    while j < len(inds2):\n",
    "        sent = generated_sents[inds2[j]]                    \n",
    "        if rhyme_dict[sent[-1]] == b and sent not in sents_list and sent[-1] != b_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break; \n",
    "        j += 1\n",
    "        if j == len(inds2):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "    \n",
    "    return sents_list \n",
    "\n",
    "def generate_gg_couplet(generated_sents, rhyme_dict, poem_so_far):\n",
    "    \n",
    "    r = random.randint(0, len(generated_sents) - 1)\n",
    "    sent = generated_sents[r]\n",
    "    sents_list = poem_so_far\n",
    "    \n",
    "    while not check_line(sent, sents_list):\n",
    "        r = random.randint(0, len(generated_sents) - 1)\n",
    "        sent = generated_sents[r]\n",
    "    \n",
    "    sents_list.append(sent)\n",
    "          \n",
    "    g_word = sent[-1]\n",
    "    g = rhyme_dict[g_word]\n",
    "    \n",
    "    inds = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(inds):\n",
    "        sent = generated_sents[inds[i]]          \n",
    "        if rhyme_dict[sent[-1]] == g and sent not in sents_list and sent[-1] != g_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break;\n",
    "        i +=1\n",
    "        if i == len(inds):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "        \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c31340",
   "metadata": {},
   "source": [
    "## Build the Sonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6da16",
   "metadata": {},
   "source": [
    "Now we get to put all of the pieces together to generate a sonnet! It is possible that running the below cell will result in the program saying it could not find a rhyme. This is due to the smaller size of our vocabulary and the constrainst placed around stress and rhyming. If that happens, simply run the the cell again, and a sonnet should be generated with new stress and rhyme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "705b735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stage = generate_abab_stanza(generated_sents, rhyme_dict, [])\n",
    "second_stage = generate_abab_stanza(generated_sents, rhyme_dict, first_stage)\n",
    "third_stage = generate_abab_stanza(generated_sents, rhyme_dict, second_stage)\n",
    "final_result = generate_gg_couplet(generated_sents, rhyme_dict, third_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3bad8acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed typhoons delete the lowered loon\n",
      "abhorrent prepaid ads escape the notes\n",
      "sedate abhorrent pens intone the troops\n",
      "the upscale aspects reroute knifelike yolks\n",
      "sedate sedate typhoons resume the mole\n",
      "malformed balloons annoy the squeaky crowns\n",
      "the upscale scrapbooks downplay prepaid notes\n",
      "the understood blockades intone the scowl\n",
      "the straightened wolves replace the hurricane\n",
      "the squeaky bloodhound thunders modern lords\n",
      "cliched cliched parades escape the days\n",
      "resolved resolved parades dislocate troughs\n",
      "the knifelike lawsuit basks the straightened soy\n",
      "the modern wolves remove the datapoint\n"
     ]
    }
   ],
   "source": [
    "# print the final result \n",
    "for line in final_result:\n",
    "    print(*line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a2814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3env",
   "language": "python",
   "name": "p3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
