{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7f7964",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3214c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import hfst_dev as hfst\n",
    "import graphviz\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import CFG\n",
    "from nltk import grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c853a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream English \n",
    "\n",
    "istream = hfst.HfstInputStream('English')\n",
    "assert istream.is_good() == True\n",
    "English = istream.read()\n",
    "istream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd88eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied over from english.ipynb to sample input and output\n",
    "\n",
    "def sample_input(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "def sample_output(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.output_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "457447eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up definitions from phoneclass.fst \n",
    "\n",
    "defs = {'English' : English}\n",
    "\n",
    "VowAA = hfst.regex('AA0 | AA1 | AA2', definitions=defs)\n",
    "defs['VowAA'] = VowAA\n",
    "VowAE = hfst.regex('AE0 | AE1 | AE2', definitions=defs)\n",
    "defs['VowAE'] = VowAE\n",
    "VowAH = hfst.regex('AH0 | AH1 | AH2', definitions=defs)\n",
    "defs['VowAH'] = VowAH\n",
    "VowAO = hfst.regex('AO0 | AO1 | AO2', definitions=defs)\n",
    "defs['VowAO'] = VowAO\n",
    "VowAW = hfst.regex('AW0 | AW1 | AW2', definitions=defs)\n",
    "defs['VowAW'] = VowAW\n",
    "VowAY = hfst.regex('AY0 | AY1 | AY2', definitions=defs)\n",
    "defs['VowAY'] = VowAY\n",
    "VowEH = hfst.regex('EH0 | EH1 | EH2', definitions=defs)\n",
    "defs['VowEH'] = VowEH\n",
    "VowER = hfst.regex('ER0 | ER1 | ER2', definitions=defs)\n",
    "defs['VowER'] = VowER\n",
    "VowEY = hfst.regex('EY0 | EY1 | EY2', definitions=defs)\n",
    "defs['VowEY'] = VowEY\n",
    "VowIH = hfst.regex('IH0 | IH1 | IH2', definitions=defs)\n",
    "defs['VowIH'] = VowIH\n",
    "VowIY = hfst.regex('IY0 | IY1 | IY2', definitions=defs)\n",
    "defs['VowIY'] = VowIY\n",
    "VowOW = hfst.regex('OW0 | OW1 | OW2', definitions=defs)\n",
    "defs['VowOW'] = VowOW\n",
    "VowOY = hfst.regex('OY0 | OY1 | OY2', definitions=defs)\n",
    "defs['VowOY'] = VowOY\n",
    "VowUH = hfst.regex('UH0 | UH1 | UH2', definitions=defs)\n",
    "defs['VowUH'] = VowUH\n",
    "VowUW = hfst.regex('UW0 | UW1 | UW2', definitions=defs)\n",
    "defs['VowUW'] = VowUW\n",
    "\n",
    "Vow0 = hfst.regex('AH0| IH0| ER0| IY0| OW0| AA0| EH0| UW0| AE0| AO0| AY0| EY0| AW0| UH0| OY0', definitions=defs)\n",
    "defs['Vow0'] = Vow0\n",
    "Vow1 = hfst.regex('EH1| AE1| AA1| IH1| IY1| EY1| OW1| AO1| AY1| AH1| UW1| ER1| AW1| UH1| OY1', definitions=defs)\n",
    "defs['Vow1'] = Vow1\n",
    "Vow2 = hfst.regex('EH2| EY2| AE2| AY2| AA2| IH2| OW2| IY2| AO2| UW2| AH2| AW2| ER2| UH2| OY2', definitions=defs)\n",
    "defs['Vow2'] = Vow2\n",
    "\n",
    "Vow = hfst.regex('Vow0 | Vow1 | Vow2', definitions=defs)\n",
    "defs['Vow'] = Vow\n",
    "\n",
    "Nas = hfst.regex('N | M | NG', definitions=defs)\n",
    "defs['Nas'] = Nas\n",
    "\n",
    "Phone = hfst.regex('AH0| N| S| L| T| R| K| D| IH0| M| Z| ER0| IY0| B| EH1| P| AE1| AA1| IH1| F| G| V| IY1| NG| HH| EY1| W| SH| OW1| OW0| AO1| AY1| AH1| UW1| JH| Y| CH| AA0| ER1| EH2| EY2| AE2| AY2| AA2| EH0| IH2| TH| AW1| OW2| UW0| IY2| AO2| AE0| UH1| AO0| AY0| UW2| AH2| EY0| OY1| AW2| DH| ZH| ER2| UH2| AW0| UH0| OY2| OY0', definitions = defs)\n",
    "defs['Phone'] = Phone\n",
    "\n",
    "Cons = hfst.regex('[Phone - Vow]', definitions = defs)\n",
    "defs['Cons'] = Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f8ce8",
   "metadata": {},
   "source": [
    "## Creating Classes of Words Based on Stress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11130ff6",
   "metadata": {},
   "source": [
    "### One Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0cacc",
   "metadata": {},
   "source": [
    "#### Stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c5eb9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3b85f",
   "metadata": {},
   "source": [
    "#### Unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f45c80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203eac6",
   "metadata": {},
   "source": [
    "### Two Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fdd15",
   "metadata": {},
   "source": [
    "#### Main stress first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8af72513",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df8093",
   "metadata": {},
   "source": [
    "#### Main stress second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c44b687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98e42b",
   "metadata": {},
   "source": [
    "#### Both syllables stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "866d594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow1 Cons* ]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s2\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de1333",
   "metadata": {},
   "source": [
    "#### Both syllables unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "276a652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow0 Cons* ]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s0\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e896f5",
   "metadata": {},
   "source": [
    "### Three Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47781c40",
   "metadata": {},
   "source": [
    "#### Stressed, unstressed, stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9300318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s1\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s2\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s1\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s2\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s1\"] = q\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow2 Cons*]].l].u'\n",
    "r = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s2\"] = r\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "s = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s2s1\"] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301364a",
   "metadata": {},
   "source": [
    "#### Unstressed, stressed, unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "40c92ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s2\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow2 Cons* Vow0 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s2s0\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s0\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s2\"] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bc1ec",
   "metadata": {},
   "source": [
    "## Generate Iambic Pentameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "31bdd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(wordClasses : list, defs) -> (str, str):\n",
    "    # [wordClasses] a LIST of (word class, frequency) tuples, with word classes defined in [defs]\n",
    "    # Frequencies should add up to 1\n",
    "    # \n",
    "    # Returns: (word class, sample)\n",
    "    r = random.random()\n",
    "    for wordClass in wordClasses:\n",
    "        r -= wordClass[1]\n",
    "        if r < 0:\n",
    "            return (wordClass[0], sample_input(hfst.regex(wordClass[0], definitions=defs), n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "045c6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesToList(lst, wordClasses : dict):\n",
    "    result = []\n",
    "    sumFreq = 0\n",
    "    for wc in lst:\n",
    "        result.append((wc, wordClasses[wc]))\n",
    "        sumFreq += wordClasses[wc]\n",
    "    for i in range(len(result)):\n",
    "        result[i][1] /= sumFreq\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7f7b2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iambs(wordClasses : dict, defs):\n",
    "    # [wordClasses] a DICTIONARY mapping word classes (defined in [defs]) to frequencies\n",
    "    # Frequencies should add up to 1\n",
    "    # Each element: s1 (primary), s2 (secondary), s0 (unstressed)\n",
    "    syllables = []\n",
    "    words_out = []\n",
    "    index = 0\n",
    "    while index < 10:\n",
    "        if index == 0:\n",
    "            preLst = [\"s0\", \"s0s1\", \"s2s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\", \"s2s1s0\", \"s2s1s2\"]\n",
    "        else:\n",
    "            # Unstressed\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst += [\"s2s1\", \"s2s1s0\", \"s2s1s2\"]\n",
    "            # Stressed\n",
    "            else:\n",
    "                preLst = [\"s1\", \"s1s0\", \"s1s2\", \"s1s0s1\", \"s1s0s2\", \"s1s2s1\", \"s1s2s2\"]\n",
    "                if syllables[index - 1] == \"s0\":\n",
    "                    preLst += [\"s2s0s2\", \"s2s0s1\", \"s2s2s1\"]\n",
    "                    \n",
    "        lst = classesToList(preLst, wordClasses)      \n",
    "        wordClass, word = sample(lst)\n",
    "        syllables.append(wordClass)\n",
    "        words_out.append(word)\n",
    "        index += 1\n",
    "        \n",
    "    return words_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd5b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3env",
   "language": "python",
   "name": "p3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
