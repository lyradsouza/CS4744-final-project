{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7f7964",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3214c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import hfst_dev as hfst\n",
    "import graphviz\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import CFG\n",
    "from nltk import grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c853a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream English \n",
    "\n",
    "istream = hfst.HfstInputStream('English')\n",
    "assert istream.is_good() == True\n",
    "English = istream.read()\n",
    "istream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "674251ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "libhfst_dev.HfstTransducer"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd88eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to sample input and output\n",
    "\n",
    "def sample_input(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "def sample_output(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.output_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457447eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up definitions from phoneclass.fst \n",
    "\n",
    "defs = {'English' : English}\n",
    "\n",
    "VowAA = hfst.regex('AA0 | AA1 | AA2', definitions=defs)\n",
    "defs['VowAA'] = VowAA\n",
    "VowAE = hfst.regex('AE0 | AE1 | AE2', definitions=defs)\n",
    "defs['VowAE'] = VowAE\n",
    "VowAH = hfst.regex('AH0 | AH1 | AH2', definitions=defs)\n",
    "defs['VowAH'] = VowAH\n",
    "VowAO = hfst.regex('AO0 | AO1 | AO2', definitions=defs)\n",
    "defs['VowAO'] = VowAO\n",
    "VowAW = hfst.regex('AW0 | AW1 | AW2', definitions=defs)\n",
    "defs['VowAW'] = VowAW\n",
    "VowAY = hfst.regex('AY0 | AY1 | AY2', definitions=defs)\n",
    "defs['VowAY'] = VowAY\n",
    "VowEH = hfst.regex('EH0 | EH1 | EH2', definitions=defs)\n",
    "defs['VowEH'] = VowEH\n",
    "VowER = hfst.regex('ER0 | ER1 | ER2', definitions=defs)\n",
    "defs['VowER'] = VowER\n",
    "VowEY = hfst.regex('EY0 | EY1 | EY2', definitions=defs)\n",
    "defs['VowEY'] = VowEY\n",
    "VowIH = hfst.regex('IH0 | IH1 | IH2', definitions=defs)\n",
    "defs['VowIH'] = VowIH\n",
    "VowIY = hfst.regex('IY0 | IY1 | IY2', definitions=defs)\n",
    "defs['VowIY'] = VowIY\n",
    "VowOW = hfst.regex('OW0 | OW1 | OW2', definitions=defs)\n",
    "defs['VowOW'] = VowOW\n",
    "VowOY = hfst.regex('OY0 | OY1 | OY2', definitions=defs)\n",
    "defs['VowOY'] = VowOY\n",
    "VowUH = hfst.regex('UH0 | UH1 | UH2', definitions=defs)\n",
    "defs['VowUH'] = VowUH\n",
    "VowUW = hfst.regex('UW0 | UW1 | UW2', definitions=defs)\n",
    "defs['VowUW'] = VowUW\n",
    "\n",
    "Vow0 = hfst.regex('AH0| IH0| ER0| IY0| OW0| AA0| EH0| UW0| AE0| AO0| AY0| EY0| AW0| UH0| OY0', definitions=defs)\n",
    "defs['Vow0'] = Vow0\n",
    "Vow1 = hfst.regex('EH1| AE1| AA1| IH1| IY1| EY1| OW1| AO1| AY1| AH1| UW1| ER1| AW1| UH1| OY1', definitions=defs)\n",
    "defs['Vow1'] = Vow1\n",
    "Vow2 = hfst.regex('EH2| EY2| AE2| AY2| AA2| IH2| OW2| IY2| AO2| UW2| AH2| AW2| ER2| UH2| OY2', definitions=defs)\n",
    "defs['Vow2'] = Vow2\n",
    "\n",
    "Vow = hfst.regex('Vow0 | Vow1 | Vow2', definitions=defs)\n",
    "defs['Vow'] = Vow\n",
    "\n",
    "Nas = hfst.regex('N | M | NG', definitions=defs)\n",
    "defs['Nas'] = Nas\n",
    "\n",
    "Phone = hfst.regex('AH0| N| S| L| T| R| K| D| IH0| M| Z| ER0| IY0| B| EH1| P| AE1| AA1| IH1| F| G| V| IY1| NG| HH| EY1| W| SH| OW1| OW0| AO1| AY1| AH1| UW1| JH| Y| CH| AA0| ER1| EH2| EY2| AE2| AY2| AA2| EH0| IH2| TH| AW1| OW2| UW0| IY2| AO2| AE0| UH1| AO0| AY0| UW2| AH2| EY0| OY1| AW2| DH| ZH| ER2| UH2| AW0| UH0| OY2| OY0', definitions = defs)\n",
    "defs['Phone'] = Phone\n",
    "\n",
    "Cons = hfst.regex('[Phone - Vow]', definitions = defs)\n",
    "defs['Cons'] = Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f8ce8",
   "metadata": {},
   "source": [
    "## Generating Stress Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0d149",
   "metadata": {},
   "source": [
    "Only created classes we could use (unstressed stressed pattern): for example s0s0 wouldnt be helpful to us, so we didn't define it "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff452d",
   "metadata": {},
   "source": [
    "### One Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0cacc",
   "metadata": {},
   "source": [
    "#### Stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5eb9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21e37c",
   "metadata": {},
   "source": [
    "#### Unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b99af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc922fc6",
   "metadata": {},
   "source": [
    "### Two Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fdd15",
   "metadata": {},
   "source": [
    "#### Main stress first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af72513",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df8093",
   "metadata": {},
   "source": [
    "#### Main stress second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c44b687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd051caf",
   "metadata": {},
   "source": [
    "### Three Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5e7ca",
   "metadata": {},
   "source": [
    "#### Stressed, unstressed, stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df8871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s1\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s2\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s1\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s2\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s1\"] = q\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow2 Cons*]].l].u'\n",
    "r = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s2\"] = r\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "s = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s2s1\"] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c676d",
   "metadata": {},
   "source": [
    "#### Unstressed, stressed, unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10313a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s2\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow2 Cons* Vow0 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s2s0\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s0\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s2\"] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486810fe",
   "metadata": {},
   "source": [
    "## Generating Rhyme Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "350092f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Phone* VowAA Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAA\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAE Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAE\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAO Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAO\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAW\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowEH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeEH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowER Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeER\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowEY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeEY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowIH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeIH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowIY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeIY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowOW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeOW\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowUH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeUH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowUW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeUW\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120b500",
   "metadata": {},
   "source": [
    "## Generating Iambic Pentameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad10976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(wordClasses : list, defs) -> (str, str):\n",
    "    # [wordClasses] a LIST of [word class, frequency] lists, with word classes defined in [defs]\n",
    "    # Frequencies should add up to 1\n",
    "    # \n",
    "    # Returns: (word class, sample)\n",
    "    r = random.random()\n",
    "    for wordClass in wordClasses:\n",
    "        r -= wordClass[1]\n",
    "        if r < 0:\n",
    "            return (wordClass[0], sample_input(hfst.regex(wordClass[0], definitions=defs), n=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80eb12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesToList(lst, wordClasses : dict):\n",
    "    result = []\n",
    "    sumFreq = 0\n",
    "    for wc in lst:\n",
    "        result.append([wc, wordClasses[wc]])\n",
    "        sumFreq += wordClasses[wc]\n",
    "    for i in range(len(result)):\n",
    "        result[i][1] /= sumFreq\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd94d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iambs(wordClasses : dict, defs):\n",
    "    # [wordClasses] a DICTIONARY mapping word classes (defined in [defs]) to frequencies\n",
    "    # Frequencies should add up to 1\n",
    "    # Each element: s1 (primary), s2 (secondary), s0 (unstressed)\n",
    "    syllables = []\n",
    "    words_out = []\n",
    "    index = 0\n",
    "    while index < 10:\n",
    "        if index == 0:\n",
    "            preLst = [\"s0\", \"s0s1\", \"s2s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\", \"s2s1s0\", \"s2s1s2\"]\n",
    "        elif index == 8:\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\"])\n",
    "        elif index == 9:\n",
    "            preLst = [\"s1\"]\n",
    "        else:\n",
    "            # Unstressed\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\", \"s2s1s0\", \"s2s1s2\"])\n",
    "            # Stressed\n",
    "            else:\n",
    "                preLst = [\"s1\", \"s1s0\", \"s1s2\", \"s1s0s1\", \"s1s0s2\", \"s1s2s1\", \"s1s2s2\"]\n",
    "                if syllables[index - 1] == \"s0\":\n",
    "                    preLst.extend([\"s2s0s2\", \"s2s0s1\", \"s2s2s1\"])\n",
    "                    \n",
    "        lst = classesToList(preLst, wordClasses)      \n",
    "        wordClass, word = sample(lst, defs)\n",
    "        wordSyl = wordClass.split(\"s\")\n",
    "        for syl in wordSyl:\n",
    "            if syl != \"\":   \n",
    "                syllables.append(\"s\" + syl) \n",
    "                index += 1\n",
    "        words_out.append(word)\n",
    "        \n",
    "    return words_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f561d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordClasses = {\"s0\": 1/18, \"s1\": 1/18, \"s0s1\": 1/18, \"s1s0\": 1/18, \"s2s1\": 1/18, \"s1s2\": 1/18, \"s0s1s0\": 1/18, \"s0s1s2\": 1/18, \"s0s2s0\": 1/18, \"s2s1s0\": 1/18, \"s2s1s2\": 1/18, \"s1s0s1\": 1/18, \"s1s0s2\": 1/18, \"s1s2s1\": 1/18, \"s1s2s2\": 1/18, \"s2s0s2\": 1/18, \"s2s0s1\": 1/18, \"s2s2s1\": 1/18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbfe6c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ac|credit', 'denouement', 'ac|credit', 'co|os']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_iambs(wordClasses, defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafd901",
   "metadata": {},
   "source": [
    "... put line through parser => grammatically sound even if not semnatically \n",
    "\n",
    "Define a limited set of terminals and a grammar that checks GRAMMATICAL VALIDITY without semantic evaluation; can create a CFG that does this \n",
    "\n",
    "\n",
    "Could have each line be it's own sentence, or could have a way to check line breaks (The big dogs// were on logs); have a line break before\n",
    "\n",
    "\n",
    "Viable approach: creating a grammar that generates a sonnet nums=? meter=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "9837b115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRAA1NTOW0']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# keep this to show how we used hfst to generate rhymes and find words \n",
    "\n",
    "def sample_input(x,n=1,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "\n",
    "expr = '[{pronto} .o. English].l'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "sample_input(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "70ca14b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t|he|ro|ux',\n",
       " 'la|wsuit',\n",
       " 'lerew',\n",
       " 'revie|wed',\n",
       " 'bal|lo|u',\n",
       " 'rilwanu',\n",
       " 'verisimilitude',\n",
       " 'marco|u',\n",
       " \"tro|op's\",\n",
       " 'yazo|o']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_input(x,n=10,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "    \n",
    "expr = '[English .o. [[ Phone* VowUW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "sample_input(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a470e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AA = [\"snowfall\", \"sakura\", \"enlarge\", \"jumpstart\", \"unharmed\", \"remark\", \"wasp\", \"resolved\", \"jock\", \"charm\"]\n",
    "AE = [\"bask\", \"replant\", \"chasse\", \"hunchback\", \"woodland\", \"thrash\", \"catch\", \"ads\", \"fad\", \"mailbag\"]\n",
    "AH = [\"oration\", \"straightened\", \"walnut\", \"abhorrent\", \"credence\", \"megaton\", \"puma\", \"stuffs\", \"junction\", \"patients\"]\n",
    "AO = [\"troughs\", \"frauds\", \"meatballs\", \"imports\", \"lords\", \"hoar\", \"malformed\", \"billboard\", \"shorn\", \"thorns\"]\n",
    "AW = [\"bloodhound\", \"crowns\", \"blackout\", \"reroute\", \"loud\", \"hometown\", \"scowl\", \"countdown\", \"rouse\", \"mount\"]\n",
    "AY = [\"devise\", \"privatize\", \"bribe\", \"modernize\", \"coincide\", \"chimes\", \"deprived\", \"reunite\", \"apprise\", \"knifelike\"]\n",
    "EH = [\"doorsteps\", \"aspects\", \"flare\", \"sleepwear\", \"pens\", \"pastel\", \"bullpen\", \"pipette\"]\n",
    "ER = [\"rewired\", \"spindler\", \"harvesters\", \"thunders\", \"lowered\", \"gander\", \"prisoners\", \"trimmer\", \"scholar\", \"modern\"]\n",
    "EY = [\"prepaid\", \"gateways\", \"blockades\", \"replace\", \"cliched\", \"acclimate\", \"drain\", \"birthdays\", \"upscale\", \"sedate\"]\n",
    "IH = [\"hallways\", \"parades\", \"dislocate\", \"hurricane\", \"escape\", \"downplay\", \"shortchange\", \"lace\", \"days\"]\n",
    "IY = [\"coyote\", \"squeaky\", \"delete\", \"cheek\", \"cream\", \"blackberry\", \"publicly\", \"blatantly\"]\n",
    "OW = [\"yolks\", \"chrome\", \"intone\", \"pronto\", \"sorrow\", \"disowned\", \"potatoes\", \"mole\", \"notes\"]\n",
    "OY = [\"decoy\", \"convoy\", \"noise\", \"annoy\", \"purloin\", \"steroid\", \"datapoint\", \"boy\", \"tabloids\", \"soy\"]\n",
    "UH = [\"wolves\", \"underwood\", \"scrapbooks\", \"cooked\", \"schedules\", \"cookbooks\", \"endure\", \"understood\", \"rook\", \"woods\"]\n",
    "UW = [\"balloons\", \"typhoons\", \"duped\", \"croon\", \"loon\", \"resume\", \"ingenue\", \"remove\", \"lawsuit\", \"troops\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3env",
   "language": "python",
   "name": "p3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
