{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7f7964",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3214c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import hfst_dev as hfst\n",
    "import graphviz\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import CFG\n",
    "from nltk import grammar, parse\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.parse.util import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c853a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream English \n",
    "\n",
    "istream = hfst.HfstInputStream('English')\n",
    "assert istream.is_good() == True\n",
    "English = istream.read()\n",
    "istream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457447eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up definitions from phoneclass.fst \n",
    "\n",
    "defs = {'English' : English}\n",
    "\n",
    "VowAA = hfst.regex('AA0 | AA1 | AA2', definitions=defs)\n",
    "defs['VowAA'] = VowAA\n",
    "VowAE = hfst.regex('AE0 | AE1 | AE2', definitions=defs)\n",
    "defs['VowAE'] = VowAE\n",
    "VowAH = hfst.regex('AH0 | AH1 | AH2', definitions=defs)\n",
    "defs['VowAH'] = VowAH\n",
    "VowAO = hfst.regex('AO0 | AO1 | AO2', definitions=defs)\n",
    "defs['VowAO'] = VowAO\n",
    "VowAW = hfst.regex('AW0 | AW1 | AW2', definitions=defs)\n",
    "defs['VowAW'] = VowAW\n",
    "VowAY = hfst.regex('AY0 | AY1 | AY2', definitions=defs)\n",
    "defs['VowAY'] = VowAY\n",
    "VowEH = hfst.regex('EH0 | EH1 | EH2', definitions=defs)\n",
    "defs['VowEH'] = VowEH\n",
    "VowER = hfst.regex('ER0 | ER1 | ER2', definitions=defs)\n",
    "defs['VowER'] = VowER\n",
    "VowEY = hfst.regex('EY0 | EY1 | EY2', definitions=defs)\n",
    "defs['VowEY'] = VowEY\n",
    "VowIH = hfst.regex('IH0 | IH1 | IH2', definitions=defs)\n",
    "defs['VowIH'] = VowIH\n",
    "VowIY = hfst.regex('IY0 | IY1 | IY2', definitions=defs)\n",
    "defs['VowIY'] = VowIY\n",
    "VowOW = hfst.regex('OW0 | OW1 | OW2', definitions=defs)\n",
    "defs['VowOW'] = VowOW\n",
    "VowOY = hfst.regex('OY0 | OY1 | OY2', definitions=defs)\n",
    "defs['VowOY'] = VowOY\n",
    "VowUH = hfst.regex('UH0 | UH1 | UH2', definitions=defs)\n",
    "defs['VowUH'] = VowUH\n",
    "VowUW = hfst.regex('UW0 | UW1 | UW2', definitions=defs)\n",
    "defs['VowUW'] = VowUW\n",
    "\n",
    "Vow0 = hfst.regex('AH0| IH0| ER0| IY0| OW0| AA0| EH0| UW0| AE0| AO0| AY0| EY0| AW0| UH0| OY0', definitions=defs)\n",
    "defs['Vow0'] = Vow0\n",
    "Vow1 = hfst.regex('EH1| AE1| AA1| IH1| IY1| EY1| OW1| AO1| AY1| AH1| UW1| ER1| AW1| UH1| OY1', definitions=defs)\n",
    "defs['Vow1'] = Vow1\n",
    "Vow2 = hfst.regex('EH2| EY2| AE2| AY2| AA2| IH2| OW2| IY2| AO2| UW2| AH2| AW2| ER2| UH2| OY2', definitions=defs)\n",
    "defs['Vow2'] = Vow2\n",
    "\n",
    "Vow = hfst.regex('Vow0 | Vow1 | Vow2', definitions=defs)\n",
    "defs['Vow'] = Vow\n",
    "\n",
    "Nas = hfst.regex('N | M | NG', definitions=defs)\n",
    "defs['Nas'] = Nas\n",
    "\n",
    "Phone = hfst.regex('AH0| N| S| L| T| R| K| D| IH0| M| Z| ER0| IY0| B| EH1| P| AE1| AA1| IH1| F| G| V| IY1| NG| HH| EY1| W| SH| OW1| OW0| AO1| AY1| AH1| UW1| JH| Y| CH| AA0| ER1| EH2| EY2| AE2| AY2| AA2| EH0| IH2| TH| AW1| OW2| UW0| IY2| AO2| AE0| UH1| AO0| AY0| UW2| AH2| EY0| OY1| AW2| DH| ZH| ER2| UH2| AW0| UH0| OY2| OY0', definitions = defs)\n",
    "defs['Phone'] = Phone\n",
    "\n",
    "Cons = hfst.regex('[Phone - Vow]', definitions = defs)\n",
    "defs['Cons'] = Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f8ce8",
   "metadata": {},
   "source": [
    "## Creating Classes of Words Based on Stress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a2a69",
   "metadata": {},
   "source": [
    "Only created classes we could use: for example s0s0 wouldnt be helpful to us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678d9a7",
   "metadata": {},
   "source": [
    "### One Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0cacc",
   "metadata": {},
   "source": [
    "#### Stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5eb9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeeb7b8",
   "metadata": {},
   "source": [
    "#### Unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b3bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bba20a",
   "metadata": {},
   "source": [
    "### Two Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fdd15",
   "metadata": {},
   "source": [
    "#### Main stress first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af72513",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df8093",
   "metadata": {},
   "source": [
    "#### Main stress second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c44b687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2679a",
   "metadata": {},
   "source": [
    "### Three Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098528c",
   "metadata": {},
   "source": [
    "#### Stressed, unstressed, stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716ce8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s1\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s2\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s1\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s2\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s1\"] = q\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow2 Cons*]].l].u'\n",
    "r = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s2\"] = r\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "s = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s2s1\"] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b92153",
   "metadata": {},
   "source": [
    "#### Unstressed, stressed, unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1fa4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s2\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow2 Cons* Vow0 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s2s0\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s0\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s2\"] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2558a4",
   "metadata": {},
   "source": [
    "## Generate Iambic Pentameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac412fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input and output\n",
    "\n",
    "def sample_input(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        word = None\n",
    "        dictionary = ['upright', 'abundant', 'annoying', 'adversely', 'quickly', 'very', 'and', 'or', 'but', 'every', 'some', 'no', 'a', 'that', 'who', 'whom', 'risotto', 'aqueduct', 'aqueducts', 'wok', 'woks', 'guitar', 'guitars', 'trustee', 'trustees', 'accredits', 'accredit']\n",
    "        while word == None or word not in dictionary:\n",
    "            word = random.sample(set(x2.extract_paths(max_cycles=3).keys()),n)[0].replace('|', '')\n",
    "        return word\n",
    "def sample_output(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.output_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0965b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(wordClasses : list, defs) -> (str, str):\n",
    "    # [wordClasses] a LIST of [word class, frequency] lists, with word classes defined in [defs]\n",
    "    # Frequencies should add up to 1\n",
    "    # \n",
    "    # Returns: (word class, sample)\n",
    "    r = random.random()\n",
    "    for wordClass in wordClasses:\n",
    "        r -= wordClass[1]\n",
    "        if r < 0:\n",
    "            return (wordClass[0], sample_input(hfst.regex(wordClass[0], definitions=defs), n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef2625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesToList(lst, wordClasses : dict):\n",
    "    result = []\n",
    "    sumFreq = 0\n",
    "    for wc in lst:\n",
    "        result.append([wc, wordClasses[wc]])\n",
    "        sumFreq += wordClasses[wc]\n",
    "    for i in range(len(result)):\n",
    "        result[i][1] /= sumFreq\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9546ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iambs(wordClasses : dict, defs):\n",
    "    # [wordClasses] a DICTIONARY mapping word classes (defined in [defs]) to frequencies\n",
    "    # Frequencies should add up to 1\n",
    "    # Each element: s1 (primary), s2 (secondary), s0 (unstressed)\n",
    "    syllables = []\n",
    "    words_out = []\n",
    "    index = 0\n",
    "    while index < 10:\n",
    "        if index == 0:\n",
    "            preLst = [\"s0\", \"s0s1\", \"s2s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\", \"s2s1s0\", \"s2s1s2\"]\n",
    "        elif index == 8:\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\"])\n",
    "        elif index == 9:\n",
    "            preLst = [\"s1\"]\n",
    "        else:\n",
    "            # Unstressed\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\", \"s2s1s0\", \"s2s1s2\"])\n",
    "            # Stressed\n",
    "            else:\n",
    "                preLst = [\"s1\", \"s1s0\", \"s1s2\", \"s1s0s1\", \"s1s0s2\", \"s1s2s1\", \"s1s2s2\"]\n",
    "                if syllables[index - 1] == \"s0\":\n",
    "                    preLst.extend([\"s2s0s2\", \"s2s0s1\", \"s2s2s1\"])\n",
    "                    \n",
    "        lst = classesToList(preLst, wordClasses)      \n",
    "        wordClass, word = sample(lst, defs)\n",
    "        wordSyl = wordClass.split(\"s\")\n",
    "        for syl in wordSyl:\n",
    "            if syl != \"\":   \n",
    "                syllables.append(\"s\" + syl) \n",
    "                index += 1\n",
    "        words_out.append(word)\n",
    "        \n",
    "    return words_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe8c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordClasses = {\"s0\": 1/16, \"s1\": 1/16, \"s0s1\": 1/16, \"s1s0\": 1/16, \"s2s1\": 1/16, \"s1s2\": 1/16, \"s0s1s0\": 1/16, \"s0s1s2\": 1/16, \"s0s2s0\": 1/16, \"s2s1s0\": 1/16, \"s2s1s2\": 1/16, \"s1s0s1\": 1/16, \"s1s0s2\": 1/16, \"s1s2s1\": 1/16, \"s2s0s2\": 1/16, \"s2s0s1\": 1/16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc3bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grammatical(s):\n",
    "    try:\n",
    "        t = next(p.parse(s))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a9194f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = load_parser(\"grammar.fcfg\", trace=0, cache=False)\n",
    "g = p.grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1044bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49508f50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'s1s2s2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/3705285304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_grammatical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_iambs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordClasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/2947860502.py\u001b[0m in \u001b[0;36mgenerate_iambs\u001b[0;34m(wordClasses, defs)\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mpreLst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s2s0s2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s2s0s1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s2s2s1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassesToList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreLst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mwordClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mwordSyl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/1486919199.py\u001b[0m in \u001b[0;36mclassesToList\u001b[0;34m(lst, wordClasses)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msumFreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordClasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msumFreq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwordClasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 's1s2s2'"
     ]
    }
   ],
   "source": [
    "sent = None\n",
    "while sent == None or not check_grammatical(sent):\n",
    "    sent = generate_iambs(wordClasses, defs)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd9f6b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'actor', 'black', 'cat', 'chases', 'shiny', 'rat', 'chases', 'chases', 'small', 'dogs'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/674221512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'some actor whom a black cat that chases a shiny rat quickly chases chases small dogs'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4744/p3env/lib/python3.7/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4744/p3env/lib/python3.7/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4744/p3env/lib/python3.7/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{w!r}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             raise ValueError(\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[0;34m\"input words: %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m             )\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'actor', 'black', 'cat', 'chases', 'shiny', 'rat', 'chases', 'chases', 'small', 'dogs'\"."
     ]
    }
   ],
   "source": [
    "s = 'some actor whom a black cat that chases a shiny rat quickly chases chases small dogs'.split()\n",
    "t = next(p.parse(s))\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a398128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "class wordIterable:\n",
    "    def __init__(self, file, n):\n",
    "        p1 = load_parser(file, trace=0, cache=False)\n",
    "        g1 = p1.grammar()\n",
    "        self.parser = p1\n",
    "        self.grammar = g1\n",
    "        self.n = n\n",
    "        self.sents = []\n",
    "        gen1 = generate(g1, depth=2*n)\n",
    "        s_temp = list(gen1)\n",
    "        for s in s_temp:\n",
    "            if len(s) == self.n and self.check_grammatical(s) and s not in self.sents:\n",
    "                self.sents.append(s)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if len(self.sents) > 0:\n",
    "            i = randint(0, len(self.sents) - 1)\n",
    "            sent = self.sents[i]\n",
    "            del self.sents[i]\n",
    "            return sent\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def check_grammatical(self, s):\n",
    "        try:\n",
    "            t = next(self.parser.parse(s))\n",
    "            return True\n",
    "        except StopIteration:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a703e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'risotto', 'is', 'upright']\n",
      "['every', 'risotto', 'is', 'abundant']\n",
      "['every', 'risotto', 'is', 'annoying']\n",
      "['every', 'risotto', 'was', 'upright']\n",
      "['every', 'risotto', 'was', 'abundant']\n",
      "['every', 'risotto', 'was', 'annoying']\n",
      "['every', 'aqueduct', 'is', 'upright']\n",
      "['every', 'aqueduct', 'is', 'abundant']\n",
      "['every', 'aqueduct', 'is', 'annoying']\n",
      "['every', 'aqueduct', 'was', 'upright']\n",
      "['every', 'aqueduct', 'was', 'abundant']\n",
      "['every', 'aqueduct', 'was', 'annoying']\n",
      "['every', 'wok', 'is', 'upright']\n",
      "['every', 'wok', 'is', 'abundant']\n",
      "['every', 'wok', 'is', 'annoying']\n",
      "['every', 'wok', 'was', 'upright']\n",
      "['every', 'wok', 'was', 'abundant']\n",
      "['every', 'wok', 'was', 'annoying']\n",
      "['every', 'guitar', 'is', 'upright']\n",
      "['every', 'guitar', 'is', 'abundant']\n",
      "['every', 'guitar', 'is', 'annoying']\n",
      "['every', 'guitar', 'was', 'upright']\n",
      "['every', 'guitar', 'was', 'abundant']\n",
      "['every', 'guitar', 'was', 'annoying']\n",
      "['every', 'trustee', 'is', 'upright']\n",
      "['every', 'trustee', 'is', 'abundant']\n",
      "['every', 'trustee', 'is', 'annoying']\n",
      "['every', 'trustee', 'was', 'upright']\n",
      "['every', 'trustee', 'was', 'abundant']\n",
      "['every', 'trustee', 'was', 'annoying']\n",
      "['some', 'risotto', 'is', 'upright']\n",
      "['some', 'risotto', 'is', 'abundant']\n",
      "['some', 'risotto', 'is', 'annoying']\n",
      "['some', 'risotto', 'was', 'upright']\n",
      "['some', 'risotto', 'was', 'abundant']\n",
      "['some', 'risotto', 'was', 'annoying']\n",
      "['some', 'aqueduct', 'is', 'upright']\n",
      "['some', 'aqueduct', 'is', 'abundant']\n",
      "['some', 'aqueduct', 'is', 'annoying']\n",
      "['some', 'aqueduct', 'was', 'upright']\n",
      "['some', 'aqueduct', 'was', 'abundant']\n",
      "['some', 'aqueduct', 'was', 'annoying']\n",
      "['some', 'aqueducts', 'are', 'upright']\n",
      "['some', 'aqueducts', 'are', 'abundant']\n",
      "['some', 'aqueducts', 'are', 'annoying']\n",
      "['some', 'aqueducts', 'were', 'upright']\n",
      "['some', 'aqueducts', 'were', 'abundant']\n",
      "['some', 'aqueducts', 'were', 'annoying']\n",
      "['some', 'wok', 'is', 'upright']\n",
      "['some', 'wok', 'is', 'abundant']\n"
     ]
    }
   ],
   "source": [
    "gen = generate(g, depth=4)\n",
    "\n",
    "generated_sents = []\n",
    "for i in range(50):\n",
    "    s = next(gen)\n",
    "    while not check_grammatical(s) or s in generated_sents:\n",
    "        s = next(gen)\n",
    "    generated_sents.append(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8962ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently encoding lists as dictionary.keys() makes everything run faster\n",
    "wordClasses = {\"s0\": 1/16, \"s1\": 1/16, \"s0s1\": 1/16, \"s1s0\": 1/16, \"s2s1\": 1/16, \"s1s2\": 1/16, \"s0s1s0\": 1/16, \"s0s1s2\": 1/16, \"s0s2s0\": 1/16, \"s2s1s0\": 1/16, \"s2s1s2\": 1/16, \"s1s0s1\": 1/16, \"s1s0s2\": 1/16, \"s1s2s1\": 1/16, \"s2s0s2\": 1/16, \"s2s0s1\": 1/16}\n",
    "rhymePatterns = {\"AA\": 0, \"AE\": 0, \"AH\": 0, \"AO\": 0, \"AW\": 0, \"AY\": 0, \"EH\": 0, \"ER\": 0, \"EY\": 0, \"IH\": 0, \"IY\": 0, \"OW\": 0, \"OY\": 0, \"UH\": 0, \"UW\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb2964f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dummy.fcfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/501021149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dummy.fcfg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dummy.fcfg'"
     ]
    }
   ],
   "source": [
    "tail = 0\n",
    "with open('dummy.fcfg', 'r') as f:\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            tail += 1\n",
    "            lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_left(left):\n",
    "    if '[' not in left and left == 'S' or left.split('[')[0] == 'S':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_stress(left, right, sp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# sp: stress patterns for symbols 1 and 2 [sp1, sp2]\n",
    "# return (left, right)\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_stress = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_stress = left[:ind] + '_' + sp[0] + sp[1] + left[ind:]\n",
    "        else:\n",
    "            left_stress = left + '_' + sp[0] + sp[1]\n",
    "        \n",
    "    right_stress = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_stress.append(right[i][:ind] + '_' + sp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_stress.append(right[i] + '_' + sp[i])\n",
    "            \n",
    "    return left_stress + ' -> ' + right_stress[0] + ' ' + (right_stress[1] if len(right) > 1 else '')\n",
    "    #left_stress, right_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_rhyme(left, right, rp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# rp: rhyme patterns for symbols 1 and 2 [rp1, rp2]\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_rhyme = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_rhyme = left[:ind] + '_' + rp[-1] + left[ind:]\n",
    "        else:\n",
    "            left_rhyme = left + '_' + rp[-1]\n",
    "    \n",
    "    right_rhyme = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_rhyme.append(right[i][:ind] + '_' + rp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_rhyme.append(right[i] + '_' + rp[i])\n",
    "    \n",
    "    return left_rhyme + ' -> ' + right_rhyme[0] + ' ' + (right_rhyme[1] if len(right) > 1 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_meter_add(sp1, sp2):\n",
    "    sp1end = 's' + sp1.split('s')[-1]\n",
    "    sp2begin = 's' + sp2.split('s')[1]\n",
    "    if sp1end == 's0':\n",
    "        if sp2begin == 's1':\n",
    "            return True\n",
    "        if sp2begin == 's0':\n",
    "            return False\n",
    "    if sp1end == 's1':\n",
    "        if sp2begin == 's0':\n",
    "            return True\n",
    "        if sp2begin == 's1':\n",
    "            return False\n",
    "    if sp1end == 's2':\n",
    "        sp1pu = 's' + sp1.split('s')[-2]\n",
    "        if sp1pu == 's0':\n",
    "            if sp2begin == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp1pu == 's1':\n",
    "            if sp2begin == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp1\")\n",
    "    if sp2begin == 's2':\n",
    "        sp2sec = 's' + sp2.split('s')[2]\n",
    "        if sp2sec == 's0':\n",
    "            if sp1end == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp2sec == 's1':\n",
    "            if sp1end == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7924f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iambic_pent(left, sp1, sp2):\n",
    "    if check_sentence_left(left):\n",
    "        if not check_meter_add(sp1, sp2):\n",
    "            return False\n",
    "        if len(sp1 + sp2) != 20:\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's1':\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's2' and len(sp1.split('s')[1:]) > 1 and 's' + sp1.split('s')[2] == 's0':\n",
    "            return False\n",
    "        return True\n",
    "    else:\n",
    "        return check_meter_add(sp1, sp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(wordClasses):\n",
    "    result = []\n",
    "    for sp in wordClasses:\n",
    "        result.append(sp)\n",
    "    \n",
    "    for sp1 in wordClasses:\n",
    "        for sp2 in wordClasses:\n",
    "            if check_meter_add(sp1, sp2):\n",
    "                result.append(sp1 + sp2)\n",
    "    \n",
    "    limitHit = False\n",
    "    for sp in result:\n",
    "        if len(sp) >= 10:\n",
    "            limitHit = True\n",
    "            break\n",
    "            \n",
    "    if not limitHit:\n",
    "        result = concat(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = wordClasses.keys()\n",
    "wc = concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_grammar(wordClasses):\n",
    "    grammar_section = False\n",
    "    lexical_section = False\n",
    "    index = 0;\n",
    "\n",
    "    while grammar_section == False:\n",
    "        if lines[index] == '# Grammar Rules':\n",
    "            grammar_section = True\n",
    "        index += 1\n",
    "\n",
    "    new_grammar = set()\n",
    "    while lexical_section == False:\n",
    "        if lines[index] == '# Lexical Rules':\n",
    "            lexical_section = True\n",
    "        if ' -> ' in lines[index]:\n",
    "            left = lines[index].split(' -> ')[0]\n",
    "            right = lines[index].split(' -> ')[1].split(' ')\n",
    "            for sp1 in wordClasses:\n",
    "                for sp2 in wordClasses:\n",
    "                    if check_iambic_pent(left, sp1, sp2):\n",
    "                        #for rp1 in rhymePatterns:\n",
    "                            #for rp2 in rhymePatterns:\n",
    "                                #sleft, sright = rule_add_stress(left, right, [sp1, sp2])\n",
    "                                #rule_add_rhyme(sleft, sright, [rp1, rp2])\n",
    "                        new_grammar.add(rule_add_stress(left, right, [sp1, sp2]))\n",
    "        index += 1\n",
    "\n",
    "    lexical_rules = []\n",
    "    while index < tail:\n",
    "        lexical_rules.append(lines[index])\n",
    "        index += 1\n",
    "\n",
    "    \n",
    "    with open('dummynew.fcfg', 'w') as f:\n",
    "        f.write(\"% start S\\n\")\n",
    "        for rule in new_grammar:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')\n",
    "        for rule in lexical_rules:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f87862",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_grammar(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bb4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = load_parser(\"dummynew.fcfg\", trace=0, cache=False)\n",
    "gd = pd.grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ffde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['the', 'purple', 'dog', 'inflates', 'the', 'purple', 'dog']\n",
    "td = next(pd.parse(s))\n",
    "td = next(pd.parse(s))\n",
    "#td = next(pd.parse(s))\n",
    "#td = next(pd.parse(s))\n",
    "td.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eafedbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/2559052446.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gd' is not defined"
     ]
    }
   ],
   "source": [
    "def check_grammatical(p, s):\n",
    "    try:\n",
    "        t = next(p.parse(s))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "gen = generate(gd, depth=20)\n",
    "\n",
    "s = next(gen)\n",
    "generated_sents = []\n",
    "for i in range(5):\n",
    "    while not check_grammatical(pd, s) or s in generated_sents:\n",
    "        s = next(gen)\n",
    "    print(s)\n",
    "    generated_sents.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84e6405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sents = []\n",
    "generated_set = set(generated_sents )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "896e1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhyme_dict = {\n",
    "    \"snowfall\" : 'AA', \n",
    "    \"sakura\" : 'AA', \n",
    "    \"enlarge\": 'AA',\n",
    "    \"jumpstart\" : 'AA',\n",
    "    \"unharmed\" : 'AA',\n",
    "    \"remark\" : 'AA', \n",
    "    \"wasp\" : 'AA',\n",
    "    \"resolved\" :'AA',\n",
    "    \"jock\" : 'AA',\n",
    "    \"charm\" : 'AA',\n",
    "    \"bask\" : 'AE',\n",
    "    \"replant\" : 'AE',\n",
    "    \"chasse\" : 'AE',\n",
    "    \"hunchback\" : 'AE',\n",
    "    \"woodland\" : 'AE',\n",
    "    \"thrash\" : 'AE',\n",
    "    \"catch\" : 'AE',\n",
    "    \"ads\" : 'AE',\n",
    "    \"fad\" : 'AE',\n",
    "    \"mailbag\" : 'AE',\n",
    "    \"oration\" : 'AH',\n",
    "    \"straightened\" : 'AH',\n",
    "    \"walnut\" : 'AH',\n",
    "    \"abhorrent\" : 'AH',\n",
    "    \"credence\" : 'AH',\n",
    "    \"megaton\" : 'AH',\n",
    "    \"puma\" : 'AH',\n",
    "    \"stuffs\" : 'AH',\n",
    "    \"junction\" : 'AH',\n",
    "    \"patients\" : 'AH',\n",
    "    \"troughs\" : 'AO',\n",
    "    \"frauds\" : 'AO',\n",
    "    \"meatballs\" : 'AO',\n",
    "    \"imports\" : 'AO',\n",
    "    \"lords\" : 'AO',\n",
    "    \"hoar\" : 'AO',\n",
    "    \"malformed\" : 'AO',\n",
    "    \"billboard\" : 'AO',\n",
    "    \"shorn\" : 'AO',\n",
    "    \"thorns\" : 'AO',\n",
    "    \"bloodhound\" : 'AW',\n",
    "    \"crowns\" : 'AW',\n",
    "    \"blackout\" : 'AW',\n",
    "    \"reroute\" : 'AW', \n",
    "    \"loud\" : 'AW',\n",
    "    \"hometown\" : 'AW',\n",
    "    \"scowl\" : 'AW',\n",
    "    \"countdown\" : 'AW',\n",
    "    \"rouse\" : 'AW',\n",
    "    \"mount\" : 'AW',\n",
    "    \"bloodhound\" : ''\n",
    "    , \"crowns\", \"blackout\", \"reroute\", \"loud\", \"hometown\", \"scowl\", \"countdown\", \"rouse\", \"mount\"\n",
    "}\n",
    "    \n",
    "\n",
    "    \n",
    "AW = [\"bloodhound\", \"crowns\", \"blackout\", \"reroute\", \"loud\", \"hometown\", \"scowl\", \"countdown\", \"rouse\", \"mount\"]\n",
    "AY = [\"devise\", \"privatize\", \"bribe\", \"modernize\", \"coincide\", \"chimes\", \"deprived\", \"reunite\", \"apprise\", \"knifelike\"]\n",
    "EH = [\"doorsteps\", \"aspects\", \"flare\", \"sleepwear\", \"pens\", \"pastel\", \"bullpen\", \"pipette\"]\n",
    "ER = [\"rewired\", \"spindler\", \"harvesters\", \"thunders\", \"lowered\", \"gander\", \"prisoners\", \"trimmer\", \"scholar\", \"modern\"]\n",
    "EY = [\"prepaid\", \"gateways\", \"blockades\", \"replace\", \"cliched\", \"acclimate\", \"drain\", \"birthdays\", \"upscale\", \"sedate\"]\n",
    "IH = [\"hallways\", \"parades\", \"dislocate\", \"hurricane\", \"escape\", \"downplay\", \"shortchange\", \"lace\", \"days\"]\n",
    "IY = [\"coyote\", \"squeaky\", \"delete\", \"cheek\", \"cream\", \"blackberry\", \"publicly\", \"blatantly\"]\n",
    "OW = [\"yolks\", \"chrome\", \"intone\", \"pronto\", \"sorrow\", \"disowned\", \"potatoes\", \"mole\", \"notes\"]\n",
    "OY = [\"decoy\", \"convoy\", \"noise\", \"annoy\", \"purloin\", \"steroid\", \"datapoint\", \"boy\", \"tabloids\", \"soy\"]\n",
    "UH = [\"wolves\", \"underwood\", \"scrapbooks\", \"cooked\", \"schedules\", \"cookbooks\", \"endure\", \"understood\", \"rook\", \"woods\"]\n",
    "UW = [\"balloons\", \"typhoons\", \"duped\", \"croon\", \"loon\", \"resume\", \"ingenue\", \"remove\", \"lawsuit\", \"troops\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dd04659",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/2456915832.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrhyme_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/2456915832.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrhyme_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dfa63bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sonnet\n",
    "import random \n",
    "r = random.randint(0, 34)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edfd48e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1309789209.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96285/1309789209.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# generate sonnet\n",
    "import random \n",
    "\n",
    "\n",
    "\n",
    "def generate_abab_stanza(generated_sents, rhyme_dict, poem_so_far=[]):\n",
    "    \n",
    "    r = random.randint(0, len(generated_sents))\n",
    "\n",
    "    sents_list = poem_so_far\n",
    "    sents_list.append([generated_sents[r]])\n",
    "    a = rhyme_dict[generated_sents[r][-1]]\n",
    "\n",
    "    r = random.randint(0, len(generated_sents))\n",
    "\n",
    "    sents_list.append(generated_sents[r])\n",
    "    b = rhyme_dict[generated_sents[r][-1]]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(generated_sents):\n",
    "        if rhyme_dict[generated_sents[i][-1]] == a and generated_sents[i] not in sents_list:\n",
    "            sents_list.append(generated_sents[i])\n",
    "            break;\n",
    "        i += 1\n",
    "        if i == len(generated_sents):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "\n",
    "    j = 0\n",
    "    while i < len(generated_sents):\n",
    "        if rhyme_dict[generated_sents[j][-1]] == b and generated_sents[j] not in sents_list:\n",
    "            sents_list.append(generated_sents[j])\n",
    "            break; \n",
    "        j += 1\n",
    "        if j == len(generated_sents):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "    \n",
    "    return sents_list \n",
    "\n",
    "def generate_gg_couplet(generated_sents, rhyme_dict, poem_so_far):\n",
    "    \n",
    "    r = random.randint(0, len(generated_sents))\n",
    "    sents_list = poem_so_far\n",
    "    sents_list.append([generated_sents[r]])\n",
    "    g = rhyme_dict[generated_sents[r][-1]]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(generated_sents):\n",
    "        if rhyme_dict[generated_sents[i][-1]] == g and generated_sents[i] not in sents_list:\n",
    "            sents_list.append(generated_sents[i])\n",
    "            break;\n",
    "        i +=1\n",
    "        if i == len(generated_sents):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "        \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4402f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3env",
   "language": "python",
   "name": "p3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
