{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d27bc40",
   "metadata": {},
   "source": [
    "## Project Members:\n",
    "    Crystal Zhu (cyz22)\n",
    "    Lyra D'Souza (lad279)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90170f06",
   "metadata": {},
   "source": [
    "## Brief Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de713365",
   "metadata": {},
   "source": [
    "The purpose of our project was to leverage what we have learned about stress and rhyme through our work with HFST and what we have learned about sentence structure and building a grammar through work with CFGs and FCFGs to create a Shakespearean sonnet generator. Connecting these two topics was a challenging but interesting task, as there is no large-scale English grammar, we could use to verify the validity of the output we generated. Thus, we used HFST to define the rhyme classes and stress classes we use, and incorporated the vocabulary we were able to extract from HFST into a context-free grammar that guides how our generator composes its sonnets, but we do not have functionality for verifying if a sentence makes sense in English. \n",
    "\n",
    "Given more resources, it might have been interesting to experiment with ways to create semantically viable sentences, but based on our research, it appears that most of the options for doing that involve NLP concepts beyond the scope of this class and not really related to the concepts we have learned. There is (understandably) no comprehensive semantic parser for the English language made publicly available, nor is there a viable grammar to base a generator on, thus we were limited in what we could achieve by a grammar and a parser designed and implemented by us. As a result, our generator is not as sophisticated as we had hoped it could be, but we worked very hard to give it the capacity to generate viable sonnets, though the poems produced do leave much of their meaning to the imagination and discretion of the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f7964",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3214c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import hfst_dev as hfst\n",
    "import graphviz\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import CFG\n",
    "from nltk import grammar, parse\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.parse.util import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c853a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream English \n",
    "\n",
    "istream = hfst.HfstInputStream('English')\n",
    "assert istream.is_good() == True\n",
    "English = istream.read()\n",
    "istream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457447eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up definitions from phoneclass.fst \n",
    "\n",
    "defs = {'English' : English}\n",
    "\n",
    "VowAA = hfst.regex('AA0 | AA1 | AA2', definitions=defs)\n",
    "defs['VowAA'] = VowAA\n",
    "VowAE = hfst.regex('AE0 | AE1 | AE2', definitions=defs)\n",
    "defs['VowAE'] = VowAE\n",
    "VowAH = hfst.regex('AH0 | AH1 | AH2', definitions=defs)\n",
    "defs['VowAH'] = VowAH\n",
    "VowAO = hfst.regex('AO0 | AO1 | AO2', definitions=defs)\n",
    "defs['VowAO'] = VowAO\n",
    "VowAW = hfst.regex('AW0 | AW1 | AW2', definitions=defs)\n",
    "defs['VowAW'] = VowAW\n",
    "VowAY = hfst.regex('AY0 | AY1 | AY2', definitions=defs)\n",
    "defs['VowAY'] = VowAY\n",
    "VowEH = hfst.regex('EH0 | EH1 | EH2', definitions=defs)\n",
    "defs['VowEH'] = VowEH\n",
    "VowER = hfst.regex('ER0 | ER1 | ER2', definitions=defs)\n",
    "defs['VowER'] = VowER\n",
    "VowEY = hfst.regex('EY0 | EY1 | EY2', definitions=defs)\n",
    "defs['VowEY'] = VowEY\n",
    "VowIH = hfst.regex('IH0 | IH1 | IH2', definitions=defs)\n",
    "defs['VowIH'] = VowIH\n",
    "VowIY = hfst.regex('IY0 | IY1 | IY2', definitions=defs)\n",
    "defs['VowIY'] = VowIY\n",
    "VowOW = hfst.regex('OW0 | OW1 | OW2', definitions=defs)\n",
    "defs['VowOW'] = VowOW\n",
    "VowOY = hfst.regex('OY0 | OY1 | OY2', definitions=defs)\n",
    "defs['VowOY'] = VowOY\n",
    "VowUH = hfst.regex('UH0 | UH1 | UH2', definitions=defs)\n",
    "defs['VowUH'] = VowUH\n",
    "VowUW = hfst.regex('UW0 | UW1 | UW2', definitions=defs)\n",
    "defs['VowUW'] = VowUW\n",
    "\n",
    "Vow0 = hfst.regex('AH0| IH0| ER0| IY0| OW0| AA0| EH0| UW0| AE0| AO0| AY0| EY0| AW0| UH0| OY0', definitions=defs)\n",
    "defs['Vow0'] = Vow0\n",
    "Vow1 = hfst.regex('EH1| AE1| AA1| IH1| IY1| EY1| OW1| AO1| AY1| AH1| UW1| ER1| AW1| UH1| OY1', definitions=defs)\n",
    "defs['Vow1'] = Vow1\n",
    "Vow2 = hfst.regex('EH2| EY2| AE2| AY2| AA2| IH2| OW2| IY2| AO2| UW2| AH2| AW2| ER2| UH2| OY2', definitions=defs)\n",
    "defs['Vow2'] = Vow2\n",
    "\n",
    "Vow = hfst.regex('Vow0 | Vow1 | Vow2', definitions=defs)\n",
    "defs['Vow'] = Vow\n",
    "\n",
    "Nas = hfst.regex('N | M | NG', definitions=defs)\n",
    "defs['Nas'] = Nas\n",
    "\n",
    "Phone = hfst.regex('AH0| N| S| L| T| R| K| D| IH0| M| Z| ER0| IY0| B| EH1| P| AE1| AA1| IH1| F| G| V| IY1| NG| HH| EY1| W| SH| OW1| OW0| AO1| AY1| AH1| UW1| JH| Y| CH| AA0| ER1| EH2| EY2| AE2| AY2| AA2| EH0| IH2| TH| AW1| OW2| UW0| IY2| AO2| AE0| UH1| AO0| AY0| UW2| AH2| EY0| OY1| AW2| DH| ZH| ER2| UH2| AW0| UH0| OY2| OY0', definitions = defs)\n",
    "defs['Phone'] = Phone\n",
    "\n",
    "Cons = hfst.regex('[Phone - Vow]', definitions = defs)\n",
    "defs['Cons'] = Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f8ce8",
   "metadata": {},
   "source": [
    "## Creating Classes of Words Based on Stress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a2a69",
   "metadata": {},
   "source": [
    "We use hfst to create classes of words that we can use to construct iambic pentameter based on the stress classes provided to us: primary stress (s1), secondary stress (s2), and unstressed (s0). We have only created machines for the stress cases that are relevnat to the unstressed-stressed pattern of iambic pentameter, so for example, something like s0s0 would not be helpful, therefore we have excluded it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678d9a7",
   "metadata": {},
   "source": [
    "### One Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0cacc",
   "metadata": {},
   "source": [
    "#### Stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5eb9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeeb7b8",
   "metadata": {},
   "source": [
    "#### Unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b3bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bba20a",
   "metadata": {},
   "source": [
    "### Two Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fdd15",
   "metadata": {},
   "source": [
    "#### Main stress first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af72513",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df8093",
   "metadata": {},
   "source": [
    "#### Main stress second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44b687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2679a",
   "metadata": {},
   "source": [
    "### Three Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098528c",
   "metadata": {},
   "source": [
    "#### Stressed, unstressed, stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716ce8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s1\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s2\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s1\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s2\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s1\"] = q\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow2 Cons*]].l].u'\n",
    "r = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s2\"] = r\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "s = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s2s1\"] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b92153",
   "metadata": {},
   "source": [
    "#### Unstressed, stressed, unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1fa4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s2\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow2 Cons* Vow0 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s2s0\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s0\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s2\"] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b151e",
   "metadata": {},
   "source": [
    "In some cases when constructing our vocabulary and our grammar, it became necessary to verify the stress pattern of a word. Below is the code we used to view the lower side representation of a given word based on the streamed in English dictionary in hfst. It is worth noting that hfst is able to generate many words that don't show up when explciitly called for in the format below (e.g. when streamed in, English.hfst contains the word \"balloon\" and will generate it for a regex match of words conatining the vowel sound 'VowUW,' but when checking for matches with the orthography \"balloon\" in English, a ValueError is thrown as no matches are found, so the sample population is 0). This is a distinct drawback of the hfst functionality, but we were able to work around it by googling such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe7d2f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PLEY1N']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_input(x,n=1,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "\n",
    "# Example word: sakura\n",
    "expr = '[{plane} .o. English].l'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "sample_input(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f49955",
   "metadata": {},
   "source": [
    "## Generating Rhyme Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85517be8",
   "metadata": {},
   "source": [
    "We defined a rhyme between two words as them containing the same final vowel sound. Thus, we consider all words that share a final vowel sound to be rhymes of each other, and have created classes for each of the vowel sounds defined in HFST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb47d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Phone* VowAA Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAA\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAE Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAE\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAO Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAO\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAW\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowEH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeEH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowER Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeER\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowEY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeEY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowIH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeIH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowIY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeIY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowOW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeOW\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowUH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeUH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowUW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeUW\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2be9c",
   "metadata": {},
   "source": [
    "Based on the rhyme classes we define above through the use of hfst, we use sampling to generate a vocabulary for our end rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e086203d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cre|w',\n",
       " 'broadview',\n",
       " 'loew',\n",
       " 'to|olro|om',\n",
       " 'harpo|ons',\n",
       " 'kluge',\n",
       " 'tabo|o',\n",
       " 'rhe|w',\n",
       " 's|hutes',\n",
       " 'mahmo|od']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_input(x,n=10,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "\n",
    "# Example vowel sound: VowUW\n",
    "expr = '[English .o. [[ Phone* VowUW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "sample_input(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420227de",
   "metadata": {},
   "source": [
    "Below is the set of lists we developed for each of the vowel-based rhyming classes based on our sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad25756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AW = [\"bloodhound\", \"crowns\", \"blackout\", \"reroute\", \"loud\", \"hometown\", \"scowl\", \"countdown\", \"rouse\", \"mount\"]\n",
    "AY = [\"devise\", \"privatize\", \"bribe\", \"modernize\", \"coincide\", \"chimes\", \"deprived\", \"reunite\", \"apprise\", \"knifelike\"]\n",
    "EH = [\"doorsteps\", \"aspects\", \"flare\", \"sleepwear\", \"pens\", \"pastel\", \"bullpen\", \"pipette\"]\n",
    "ER = [\"rewired\", \"spindler\", \"harvesters\", \"thunders\", \"lowered\", \"gander\", \"prisoners\", \"trimmer\", \"scholar\", \"modern\"]\n",
    "EY = [\"prepaid\", \"gateways\", \"blockades\", \"replace\", \"cliched\", \"acclimate\", \"drain\", \"birthdays\", \"upscale\", \"sedate\"]\n",
    "IH = [\"hallways\", \"parades\", \"dislocate\", \"hurricane\", \"escape\", \"downplay\", \"shortchange\", \"lace\", \"days\"]\n",
    "IY = [\"coyote\", \"squeaky\", \"delete\", \"cheek\", \"cream\", \"blackberry\", \"publicly\", \"blatantly\"]\n",
    "OW = [\"yolks\", \"chrome\", \"intone\", \"pronto\", \"sorrow\", \"disowned\", \"potatoes\", \"mole\", \"notes\"]\n",
    "OY = [\"decoy\", \"convoy\", \"noise\", \"annoy\", \"purloin\", \"steroid\", \"datapoint\", \"boy\", \"tabloids\", \"soy\"]\n",
    "UH = [\"wolves\", \"underwood\", \"scrapbooks\", \"cooked\", \"schedules\", \"cookbooks\", \"endure\", \"understood\", \"rook\", \"woods\"]\n",
    "UW = [\"balloons\", \"typhoons\", \"duped\", \"croon\", \"loon\", \"resume\", \"ingenue\", \"remove\", \"lawsuit\", \"troops\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012b878",
   "metadata": {},
   "source": [
    "Based on the lists generated above, we created a dictionary of words in our vocabulary mapped to the rhyme class of their last syllable. As this is a rather large dictionary, we wrote it to to an external text file that we then import below to use moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bfce3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve rhyme dictionary from external file \n",
    "from rhyme_dict import rhyme_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2558a4",
   "metadata": {},
   "source": [
    "## Generate Iambic Pentameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56587e27",
   "metadata": {},
   "source": [
    "Here we use the stress classes defined above to generate lines of iambic pentameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac412fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_input(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return random.sample(set(x2.extract_paths(max_cycles=3).keys()),n)[0].replace('|', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0965b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(wordClasses : list, defs) -> (str, str):\n",
    "    # [wordClasses] a LIST of [word class, frequency] lists, with word classes defined in [defs]\n",
    "    # Frequencies should add up to 1\n",
    "    # \n",
    "    # Returns: (word class, sample)\n",
    "    r = random.random()\n",
    "    for wordClass in wordClasses:\n",
    "        r -= wordClass[1]\n",
    "        if r < 0:\n",
    "            return (wordClass[0], sample_input(hfst.regex(wordClass[0], definitions=defs), n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef2625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesToList(lst, wordClasses : dict):\n",
    "    result = []\n",
    "    sumFreq = 0\n",
    "    for wc in lst:\n",
    "        result.append([wc, wordClasses[wc]])\n",
    "        sumFreq += wordClasses[wc]\n",
    "    for i in range(len(result)):\n",
    "        result[i][1] /= sumFreq\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9546ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iambs(wordClasses : dict, defs):\n",
    "    # [wordClasses] a DICTIONARY mapping word classes (defined in [defs]) to frequencies\n",
    "    # Frequencies should add up to 1\n",
    "    # Each element: s1 (primary), s2 (secondary), s0 (unstressed)\n",
    "    syllables = []\n",
    "    words_out = []\n",
    "    index = 0\n",
    "    while index < 10:\n",
    "        if index == 0:\n",
    "            preLst = [\"s0\", \"s0s1\", \"s2s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\", \"s2s1s0\", \"s2s1s2\"]\n",
    "        elif index == 8:\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\"])\n",
    "        elif index == 9:\n",
    "            preLst = [\"s1\"]\n",
    "        else:\n",
    "            # Unstressed\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\", \"s2s1s0\", \"s2s1s2\"])\n",
    "            # Stressed\n",
    "            else:\n",
    "                preLst = [\"s1\", \"s1s0\", \"s1s2\", \"s1s0s1\", \"s1s0s2\", \"s1s2s1\", \"s1s2s2\"]\n",
    "                if syllables[index - 1] == \"s0\":\n",
    "                    preLst.extend([\"s2s0s2\", \"s2s0s1\", \"s2s2s1\"])\n",
    "                    \n",
    "        lst = classesToList(preLst, wordClasses)      \n",
    "        wordClass, word = sample(lst, defs)\n",
    "        wordSyl = wordClass.split(\"s\")\n",
    "        for syl in wordSyl:\n",
    "            if syl != \"\":   \n",
    "                syllables.append(\"s\" + syl) \n",
    "                index += 1\n",
    "        words_out.append(word)\n",
    "        \n",
    "    return words_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058c4e5",
   "metadata": {},
   "source": [
    "## Creating the Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a13f77",
   "metadata": {},
   "source": [
    "Below we create a base grammar with stress defined for each of the terminals, which we then use to generate our final grammar that contains all possible non-terminal constructions with their stress decomposition included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a398128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "class wordIterable:\n",
    "    def __init__(self, file, n):\n",
    "        p1 = load_parser(file, trace=0, cache=False)\n",
    "        g1 = p1.grammar()\n",
    "        self.parser = p1\n",
    "        self.grammar = g1\n",
    "        self.n = n\n",
    "        self.sents = []\n",
    "        gen1 = generate(g1, depth=2*n)\n",
    "        s_temp = list(gen1)\n",
    "        for s in s_temp:\n",
    "            if len(s) == self.n and self.check_grammatical(s) and s not in self.sents:\n",
    "                self.sents.append(s)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if len(self.sents) > 0:\n",
    "            i = randint(0, len(self.sents) - 1)\n",
    "            sent = self.sents[i]\n",
    "            del self.sents[i]\n",
    "            return sent\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def check_grammatical(self, s):\n",
    "        try:\n",
    "            t = next(self.parser.parse(s))\n",
    "            return True\n",
    "        except StopIteration:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fe355cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding lists as dictionary.keys() makes the program run faster\n",
    "wordClasses = {\"s0\": 1/16, \"s1\": 1/16, \"s0s1\": 1/16, \"s1s0\": 1/16, \"s2s1\": 1/16, \"s1s2\": 1/16, \"s0s1s0\": 1/16, \"s0s1s2\": 1/16, \"s0s2s0\": 1/16, \"s2s1s0\": 1/16, \"s2s1s2\": 1/16, \"s1s0s1\": 1/16, \"s1s0s2\": 1/16, \"s1s2s1\": 1/16, \"s2s0s2\": 1/16, \"s2s0s1\": 1/16}\n",
    "rhymePatterns = {\"AA\": 0, \"AE\": 0, \"AH\": 0, \"AO\": 0, \"AW\": 0, \"AY\": 0, \"EH\": 0, \"ER\": 0, \"EY\": 0, \"IH\": 0, \"IY\": 0, \"OW\": 0, \"OY\": 0, \"UH\": 0, \"UW\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1763b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail = 0\n",
    "with open('grammar.fcfg', 'r') as f:\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            tail += 1\n",
    "            lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bc1b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_left(left):\n",
    "    if '[' not in left and left == 'S' or left.split('[')[0] == 'S':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "399b9cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_stress(left, right, sp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# sp: stress patterns for symbols 1 and 2 [sp1, sp2]\n",
    "# return (left, right)\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_stress = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_stress = left[:ind] + '_' + ''.join(sp) + left[ind:]\n",
    "        else:\n",
    "            left_stress = left + '_' + ''.join(sp)\n",
    "        \n",
    "    right_stress = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_stress.append(right[i][:ind] + '_' + sp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_stress.append(right[i] + '_' + sp[i])\n",
    "            \n",
    "    return left_stress + ' -> ' + right_stress[0] + ' ' + (right_stress[1] if len(right) > 1 else '')\n",
    "    #left_stress, right_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cdf8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_rhyme(left, right, rp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# rp: rhyme patterns for symbols 1 and 2 [rp1, rp2]\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_rhyme = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_rhyme = left[:ind] + '_' + rp[-1] + left[ind:]\n",
    "        else:\n",
    "            left_rhyme = left + '_' + rp[-1]\n",
    "    \n",
    "    right_rhyme = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_rhyme.append(right[i][:ind] + '_' + rp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_rhyme.append(right[i] + '_' + rp[i])\n",
    "    \n",
    "    return left_rhyme + ' -> ' + right_rhyme[0] + ' ' + (right_rhyme[1] if len(right) > 1 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0a57a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_meter_add(sp1, sp2):\n",
    "    sp1end = 's' + sp1.split('s')[-1]\n",
    "    sp2begin = 's' + sp2.split('s')[1]\n",
    "    if sp1end == 's0':\n",
    "        if sp2begin == 's1':\n",
    "            return True\n",
    "        if sp2begin == 's0':\n",
    "            return False\n",
    "    if sp1end == 's1':\n",
    "        if sp2begin == 's0':\n",
    "            return True\n",
    "        if sp2begin == 's1':\n",
    "            return False\n",
    "    if sp1end == 's2':\n",
    "        sp1pu = 's' + sp1.split('s')[-2]\n",
    "        if sp1pu == 's0':\n",
    "            if sp2begin == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp1pu == 's1':\n",
    "            if sp2begin == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp1\")\n",
    "    if sp2begin == 's2':\n",
    "        sp2sec = 's' + sp2.split('s')[2]\n",
    "        if sp2sec == 's0':\n",
    "            if sp1end == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp2sec == 's1':\n",
    "            if sp1end == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b144a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iambic_pent(left, sp1, sp2):\n",
    "    if check_sentence_left(left):\n",
    "        if not check_meter_add(sp1, sp2):\n",
    "            return False\n",
    "        if len(sp1 + sp2) != 20:\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's1':\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's2' and len(sp1.split('s')[1:]) > 1 and 's' + sp1.split('s')[2] == 's0':\n",
    "            return False\n",
    "        return True\n",
    "    else:\n",
    "        return check_meter_add(sp1, sp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09517151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(wordClasses):\n",
    "    result = []\n",
    "    for sp in wordClasses:\n",
    "        result.append(sp)\n",
    "    \n",
    "    for sp1 in wordClasses:\n",
    "        for sp2 in wordClasses:\n",
    "            if check_meter_add(sp1, sp2):\n",
    "                result.append(sp1 + sp2)\n",
    "    \n",
    "    limitHit = False\n",
    "    for sp in result:\n",
    "        if len(sp) >= 10:\n",
    "            limitHit = True\n",
    "            break\n",
    "            \n",
    "    if not limitHit:\n",
    "        result = concat(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1d94311",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = wordClasses.keys()\n",
    "wc = concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25d594a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_grammar(wordClasses):\n",
    "    grammar_section = False\n",
    "    lexical_section = False\n",
    "    index = 0;\n",
    "\n",
    "    while grammar_section == False:\n",
    "        if lines[index] == '# Grammar Rules':\n",
    "            grammar_section = True\n",
    "        index += 1\n",
    "\n",
    "    new_grammar = set()\n",
    "    while lexical_section == False:\n",
    "        if lines[index] == '# Lexical Rules':\n",
    "            lexical_section = True\n",
    "        if ' -> ' in lines[index]:\n",
    "            left = lines[index].split(' -> ')[0]\n",
    "            right = lines[index].split(' -> ')[1].split(' ')\n",
    "            if len(right) == 1:\n",
    "                for sp in wordClasses:\n",
    "                    new_grammar.add(rule_add_stress(left, right, [sp]))\n",
    "            else:\n",
    "                for sp1 in wordClasses:\n",
    "                    for sp2 in wordClasses:\n",
    "                        if check_iambic_pent(left, sp1, sp2):\n",
    "                            new_grammar.add(rule_add_stress(left, right, [sp1, sp2]))\n",
    "                            #for rp1 in rhymePatterns:\n",
    "                                #for rp2 in rhymePatterns:\n",
    "                                    #sleft, sright = rule_add_stress(left, right, [sp1, sp2])\n",
    "                                    #rule_add_rhyme(sleft, sright, [rp1, rp2])\n",
    "                            \n",
    "        index += 1\n",
    "\n",
    "    lexical_rules = []\n",
    "    while index < tail:\n",
    "        lexical_rules.append(lines[index])\n",
    "        index += 1\n",
    "\n",
    "    \n",
    "    with open('grammarnew.fcfg', 'w') as f:\n",
    "        f.write(\"% start S\\n\")\n",
    "        for rule in new_grammar:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')\n",
    "        for rule in lexical_rules:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d66bcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_grammar(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13168abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = load_parser(\"grammarnew.fcfg\", trace=0, cache=False)\n",
    "gd = pd.grammar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2c4a1",
   "metadata": {},
   "source": [
    "## Sonnet Generation Method 1: Using the NLTK Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69523c8c",
   "metadata": {},
   "source": [
    "Our first attempt at generation used NLTK's built in generator. As expected, it takes a very long time to run the generations, so we have stored the results of running 100,000 sentence generations in a text file that can be used to create the sonnets to save time. However, we have left the code to generate around 100 or so sentences, which takes much shorter and demonstrates the functionality of our generator without demanding the full night that it took to run such a large set of generations.\n",
    "\n",
    "Two huge drawbacks of the NLTK generator are first, that it does not take tags into account when generating sentences, so all information we wanted the generator to take into account had to be factored into the labels of our grammar, making it much bigger than it would have been if tags were an option. Secondly, the generator has a specific order that it generates sentences in, which means sentences following a very specific construction (e.g., “The baboon eats white fruit,” “The baboon eats white grapes,” “The baboon eats white trees”) will take up the first thousand rows of generations. As we made our vocabulary bigger, the NLTK generator failed us more, because we did not have the processing power to run the millions of generations needed to get the diversity of sentences our grammar had the potential to produce. This was our first attempt at generating a sonnet, and though it had some drawbacks, it did manage to produce something useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcdcc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grammatical(p, s):\n",
    "    try:\n",
    "        t = next(p.parse(s))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "gen = generate(gd, depth=20)\n",
    "\n",
    "s = next(gen)\n",
    "generated_sents = []\n",
    "for i in range(100):\n",
    "    while not check_grammatical(pd, s) or s in generated_sents:\n",
    "        s = next(gen)\n",
    "    generated_sents.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5422d",
   "metadata": {},
   "source": [
    "Due to the time it takes to run generations, we have stashed 100,000 generated sentences in a text file and have the generator code here only produce 100 so functioanlity can be seen without stalling the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4edf54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stashing the results of our call to generate in a file ##\n",
    "\n",
    "# with open(r'sentences_new.txt', 'w') as fp:\n",
    "#     for s in generated_sents:\n",
    "#         # write each item on a new line\n",
    "#         fp.write(\"%s\\n\" % s)\n",
    "#     print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b6e52c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# For reading in the results of generate from a text file ##\n",
    "\n",
    "test_sents = []\n",
    "with open('sentences.txt') as file:\n",
    "    for line in file:\n",
    "        test_sents.append(line)\n",
    "    print('Done')\n",
    "\n",
    "new_lst = []\n",
    "for i in test_sents:\n",
    "    elements = re.findall(r'[a-z]+', i[0:-1])\n",
    "    res = [x for x in elements]\n",
    "    new_lst.append(res)\n",
    "\n",
    "generated_sents = new_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6527001",
   "metadata": {},
   "source": [
    "As mentioned, the generation function built into nltk has a very specific order to how it generates sentences, so we attempted to shuffle the indices of the generations to create as much variety as possible in the lines that go into our sonnets. This did not end up working that well, since the first 10,000 generations all contain the same first 4 words or so, but it helped a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e751631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle indices \n",
    "random.shuffle(generated_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efa41d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de4886b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dets = ['the', 'some', 'his', 'her', 'their', 'its', 'my', 'your', 'our']\n",
    "\n",
    "def check_line(line, poem_so_far):\n",
    "    \"\"\"\n",
    "    Verifies that a given word has only appeared in the existing body of \n",
    "    the sonnet no more than once before.\n",
    "    \"\"\"\n",
    "\n",
    "    for word in line:\n",
    "        count = 0\n",
    "        for line in poem_so_far:\n",
    "            if word in line:\n",
    "                count += 1\n",
    "            \n",
    "        if count > 1 and word not in dets:\n",
    "            return False\n",
    "            \n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90f3525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abab_stanza(generated_sents, rhyme_dict, poem_so_far=[]):\n",
    "    \"\"\"\n",
    "    Generates a quatrain with an a/b/a/b rhyme scheme based on the following parameters\n",
    "    \n",
    "    ARGUMENTS\n",
    "    =========\n",
    "    \n",
    "    generated_sents : list\n",
    "        generations produced from the grammar \n",
    "    \n",
    "    rhyme_dict : dict\n",
    "        dictionary of the rhyme associated with each word in the grammar \n",
    "    \n",
    "    poem_so_far : list of lists\n",
    "        the existing lines of the poem that have been accumulated so far\n",
    "    \"\"\"\n",
    "    \n",
    "    r1 = random.randint(0, len(generated_sents) - 1)\n",
    "    first_sent = generated_sents[r1]\n",
    "    sents_list = poem_so_far\n",
    "    \n",
    "    while not check_line(first_sent, sents_list):\n",
    "        r1 = random.randint(0, len(generated_sents) - 1)\n",
    "        first_sent = generated_sents[r1]\n",
    "    \n",
    "\n",
    "    sents_list.append(first_sent)\n",
    "    \n",
    "    a_word = first_sent[-1]\n",
    "    a = rhyme_dict[a_word]\n",
    "    \n",
    "    r2 = random.randint(0, len(generated_sents) - 1)\n",
    "    second_sent = generated_sents[r2]\n",
    "    \n",
    "    while not check_line(second_sent, sents_list) or second_sent[-1] == a_word:\n",
    "        r2 = random.randint(0, len(generated_sents) - 1)\n",
    "        second_sent = generated_sents[r2]\n",
    "    \n",
    "    sents_list.append(second_sent)\n",
    "    \n",
    "    b_word = second_sent[-1]\n",
    "    b = rhyme_dict[b_word]\n",
    "   \n",
    "    \n",
    "    # shuffle indicies \n",
    "    inds1 = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds1)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(inds1):\n",
    "        sent = generated_sents[inds1[i]]\n",
    "        if rhyme_dict[sent[-1]] == a and sent not in sents_list and sent[-1] != a_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break;\n",
    "        i += 1\n",
    "        if i == len(inds1):\n",
    "            raise Exception(\"Sorry! Could not find a rhyme.\")\n",
    "\n",
    "    inds2 = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds2)\n",
    "    \n",
    "    j = 0\n",
    "    while j < len(inds2):\n",
    "        sent = generated_sents[inds2[j]]                    \n",
    "        if rhyme_dict[sent[-1]] == b and sent not in sents_list and sent[-1] != b_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break; \n",
    "        j += 1\n",
    "        if j == len(inds2):\n",
    "            raise Exception(\"Sorry! Could not find a rhyme.\")\n",
    "    \n",
    "    return sents_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a4e562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gg_couplet(generated_sents, rhyme_dict, poem_so_far):\n",
    "    \"\"\"\n",
    "    Generates a rhyming couplet based on the following parameters\n",
    "    \n",
    "    ARGUMENTS\n",
    "    =========\n",
    "    \n",
    "    generated_sents : list\n",
    "        generations produced from the grammar \n",
    "    rhyme_dict : dict\n",
    "        dictionary of the rhyme associated with each word in the grammar \n",
    "    poem_so_far : list of lists\n",
    "        the existing lines of the poem that have been accumulated so far\n",
    "    \"\"\"\n",
    "    \n",
    "    r = random.randint(0, len(generated_sents) - 1)\n",
    "    sent = generated_sents[r]\n",
    "    sents_list = poem_so_far\n",
    "    \n",
    "    while not check_line(sent, sents_list):\n",
    "        r = random.randint(0, len(generated_sents) - 1)\n",
    "        sent = generated_sents[r]\n",
    "    \n",
    "    sents_list.append(sent)\n",
    "          \n",
    "    g_word = sent[-1]\n",
    "    g = rhyme_dict[g_word]\n",
    "    \n",
    "    inds = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(inds):\n",
    "        sent = generated_sents[inds[i]]          \n",
    "        if rhyme_dict[sent[-1]] == g and sent not in sents_list and sent[-1] != g_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break;\n",
    "        i +=1\n",
    "        if i == len(inds):\n",
    "            raise Exception(\"Sorry! Could not find a rhyme.\")\n",
    "        \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670cc4a",
   "metadata": {},
   "source": [
    "## Building the Sonnet Take 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5cc12",
   "metadata": {},
   "source": [
    "Now we get to put all of the pieces together to generate a sonnet! Due to the smaller size of our vocabulary and the constraints placed around stress and rhyme, it can take the generator a few tries to successfully compose a sonnet. As a result, it may take a moment for the poem to appear. We hope you enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60880a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonnet(generated_sents, rhyme_dict):\n",
    "    ind = True\n",
    "    while ind:\n",
    "        try:\n",
    "            first_stage = generate_abab_stanza(generated_sents, rhyme_dict, [])\n",
    "            second_stage = generate_abab_stanza(generated_sents, rhyme_dict, first_stage)\n",
    "            third_stage = generate_abab_stanza(generated_sents, rhyme_dict, second_stage)\n",
    "            final_sonnet = generate_gg_couplet(generated_sents, rhyme_dict, third_stage)\n",
    "            return final_sonnet\n",
    "        except:\n",
    "            ind = True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09f85fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sonnet = sonnet(generated_sents, rhyme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3da23271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================SONNET================\n",
      "\n",
      "malformed abhorrent lords replace the flare\n",
      "malformed sedate typhoons replace balloons\n",
      "sedate balloons replant the lowered pens\n",
      "abhorrent prepaid notes escape typhoons\n",
      "\n",
      "\n",
      "the knifelike bloodhound thunders lowered yolks\n",
      "the chrome parades apprise the hurricane\n",
      "the knifelike lawsuit basks rewired notes\n",
      "the straightened upscale wolves dislocate days\n",
      "\n",
      "\n",
      "the understood blockades remove the charm\n",
      "the chrome cliched parades escape the lords\n",
      "the upscale hurricane replants the jock\n",
      "the prepaid bloodhound basks the straightened thorns\n",
      "\n",
      "\n",
      "the modern wolves purloin the megaton\n",
      "the duped cliched blockades dislocate stuffs\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=================SONNET================\" + \"\\n\")\n",
    "count = 0\n",
    "for line in final_sonnet:\n",
    "    print(*line)\n",
    "    count += 1\n",
    "    if count == 4 or count == 8 or count == 12:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a500e3",
   "metadata": {},
   "source": [
    "## Final Method: Creating Our Own Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75a1fd",
   "metadata": {},
   "source": [
    "After realizing that the functionality of the NLTK generator was not enough for what we hoped to do with this project, we constructed a randomized generator that addresses the issue of ordered generation we were having previously. We also expanded our grammar to include more determiners, which rendered the NLTK generator functionally useless at creating diveristy in its sentences, but made ours more proficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "441bf594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random(g, rhyme_dict, start='S', rhyme_with=None, avoid_word=None, syllables=0, prev_node=None):\n",
    "    if start[0] == \"'\" and start[-1] == \"'\":\n",
    "        word = start[1:-1]\n",
    "        if rhyme_with != None and 10 - syllables == len(prev_node.split('_')[1].split('s')[1:]) and rhyme_dict[word] != rhyme_with:\n",
    "            return None\n",
    "        else:\n",
    "            return [word]\n",
    "    \n",
    "    randlst = []\n",
    "    for rule in g:\n",
    "        if rule.split(' -> ')[0] == start:\n",
    "            randlst.append(rule)\n",
    "            \n",
    "    #print(randlst)\n",
    "            \n",
    "    if len(randlst) == 0:\n",
    "        return None\n",
    "\n",
    "    rg = None\n",
    "    while rg == None:\n",
    "        sentence = []\n",
    "        ri = random.randint(0, len(randlst) - 1)\n",
    "        r = randlst[ri]\n",
    "        left = r.split(' -> ')[0]\n",
    "        right = r.split(' -> ')[1].split(' ')\n",
    "        #print(r)\n",
    "        eliminate_options = True\n",
    "        #print(left, right)\n",
    "        for i, item in enumerate(right):\n",
    "            #print(right)\n",
    "            \"\"\"\n",
    "            if len(right) == 1 and right[0][0] == \"'\" and right[0][-1] == \"'\":\n",
    "                newSyllablesLeft = 0\n",
    "            elif i == len(right) - 1:\n",
    "                newSyllablesLeft = len(right[0].split('_')[1].split('s')[1:])\n",
    "            else:\n",
    "                newSyllablesLeft = len(right[1].split('_')[1].split('s')[1:])\n",
    "            \"\"\"\n",
    "            if i == 0:\n",
    "                syllablesNext = syllables\n",
    "            else:\n",
    "                syllablesNext = syllables + len(right[0].split('_')[1].split('s')[1:])\n",
    "            #print(left, right)\n",
    "            #print(syllablesNext)\n",
    "            rg = generate_random(g, rhyme_dict, start=item, rhyme_with=rhyme_with, avoid_word=avoid_word, syllables=syllablesNext, prev_node=left)\n",
    "            #print(rg)\n",
    "            if rg == None:\n",
    "                break\n",
    "            if avoid_word != None and rg[0] == avoid_word: # in avoid_words:\n",
    "                rg = None\n",
    "                #eliminate_options = False\n",
    "                break\n",
    "            for word in rg:\n",
    "                sentence.append(word)\n",
    "                \n",
    "        if eliminate_options == True:\n",
    "            if len(randlst) == 1:\n",
    "                return None\n",
    "            else:\n",
    "                randlst = randlst[0:ri] + randlst[ri+1:len(randlst)]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "574864fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abab_stanza(grammar_list, rhyme_dict, poem_so_far=[]):\n",
    "    first_sent = generate_random(grammar_list, rhyme_dict)\n",
    "    sents_list = poem_so_far\n",
    "    sents_list.append(first_sent)\n",
    "    a_word = first_sent[-1]\n",
    "    a = rhyme_dict[a_word]\n",
    "    \n",
    "    second_sent = generate_random(grammar_list, rhyme_dict, avoid_word=a_word) #, avoid_words=[a_word]\n",
    "    sents_list.append(second_sent)\n",
    "    b_word = second_sent[-1]\n",
    "    b = rhyme_dict[b_word]\n",
    "    \n",
    "    third_sent = generate_random(grammar_list, rhyme_dict, rhyme_with=a, avoid_word=a_word) #, avoid_words=[a_word, b_word]\n",
    "    sents_list.append(third_sent)\n",
    "    c_word = third_sent[-1]\n",
    "    \n",
    "    fourth_sent = generate_random(grammar_list, rhyme_dict, rhyme_with=b, avoid_word=b_word) #, avoid_words=[a_word, b_word, c_word]\n",
    "    sents_list.append(fourth_sent)\n",
    "    \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7368c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gg_couplet(grammar_list, rhyme_dict, poem_so_far):\n",
    "    first_sent = generate_random(grammar_list, rhyme_dict)\n",
    "    sents_list = poem_so_far\n",
    "    sents_list.append(first_sent)\n",
    "    a_word = first_sent[-1]\n",
    "    a = rhyme_dict[a_word]\n",
    "    \n",
    "    second_sent = generate_random(grammar_list, rhyme_dict, rhyme_with=a, avoid_word=a_word) #, avoid_words=[a_word]\n",
    "    sents_list.append(second_sent)\n",
    "    \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c7df08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = []\n",
    "with open('grammarnew.fcfg', 'r') as f:\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            g.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98620c85",
   "metadata": {},
   "source": [
    "## Building the Sonnet Final Take!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b7cd1",
   "metadata": {},
   "source": [
    "Now we get to put all of the pieces together to generate a sonnet! We hope you enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29e93edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stage = generate_abab_stanza(g, rhyme_dict, [])\n",
    "second_stage = generate_abab_stanza(g, rhyme_dict, first_stage)\n",
    "third_stage = generate_abab_stanza(g, rhyme_dict, second_stage)\n",
    "final_sonnet = generate_gg_couplet(g, rhyme_dict, third_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f927f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================SONNET================\n",
      "\n",
      "unpacked potatoes croon his good blockades\n",
      "abhorrent snacks surround adroit balloons\n",
      "abhorrent gateways acclimate her rays\n",
      "cartoon pastel typhoons escape typhoons\n",
      "\n",
      "\n",
      "their modern schedules tape my late campaign\n",
      "malformed parades endure the bored lagoon\n",
      "her lowered mailbag thunders straightened rays\n",
      "the good sakura meets my squeaky troops\n",
      "\n",
      "\n",
      "her chrome parades surround his straightened face\n",
      "abhorrent frauds surround the underwood\n",
      "her straightened lowered charm replants its rays\n",
      "inept potatoes smother modern wolves\n",
      "\n",
      "\n",
      "their lowered scowl awakens shorn balloons\n",
      "her underwood acquires rotten troops\n"
     ]
    }
   ],
   "source": [
    "print(\"=================SONNET================\" + \"\\n\")\n",
    "count = 0\n",
    "for line in final_sonnet:\n",
    "    print(*line)\n",
    "    count += 1\n",
    "    if count == 4 or count == 8 or count == 12:\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3env",
   "language": "python",
   "name": "p3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
