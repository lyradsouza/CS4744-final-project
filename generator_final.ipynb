{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6ee6c7",
   "metadata": {},
   "source": [
    "## Project Members:\n",
    "    Crystal Zhu (cyz22)\n",
    "    Lyra D'Souza (lad279)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6396cf",
   "metadata": {},
   "source": [
    "## Brief Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1c647",
   "metadata": {},
   "source": [
    "Describe what we intended "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f7964",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3214c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import hfst_dev as hfst\n",
    "import graphviz\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import CFG\n",
    "from nltk import grammar, parse\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.parse.util import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c853a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream English \n",
    "\n",
    "istream = hfst.HfstInputStream('English')\n",
    "assert istream.is_good() == True\n",
    "English = istream.read()\n",
    "istream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457447eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up definitions from phoneclass.fst \n",
    "\n",
    "defs = {'English' : English}\n",
    "\n",
    "VowAA = hfst.regex('AA0 | AA1 | AA2', definitions=defs)\n",
    "defs['VowAA'] = VowAA\n",
    "VowAE = hfst.regex('AE0 | AE1 | AE2', definitions=defs)\n",
    "defs['VowAE'] = VowAE\n",
    "VowAH = hfst.regex('AH0 | AH1 | AH2', definitions=defs)\n",
    "defs['VowAH'] = VowAH\n",
    "VowAO = hfst.regex('AO0 | AO1 | AO2', definitions=defs)\n",
    "defs['VowAO'] = VowAO\n",
    "VowAW = hfst.regex('AW0 | AW1 | AW2', definitions=defs)\n",
    "defs['VowAW'] = VowAW\n",
    "VowAY = hfst.regex('AY0 | AY1 | AY2', definitions=defs)\n",
    "defs['VowAY'] = VowAY\n",
    "VowEH = hfst.regex('EH0 | EH1 | EH2', definitions=defs)\n",
    "defs['VowEH'] = VowEH\n",
    "VowER = hfst.regex('ER0 | ER1 | ER2', definitions=defs)\n",
    "defs['VowER'] = VowER\n",
    "VowEY = hfst.regex('EY0 | EY1 | EY2', definitions=defs)\n",
    "defs['VowEY'] = VowEY\n",
    "VowIH = hfst.regex('IH0 | IH1 | IH2', definitions=defs)\n",
    "defs['VowIH'] = VowIH\n",
    "VowIY = hfst.regex('IY0 | IY1 | IY2', definitions=defs)\n",
    "defs['VowIY'] = VowIY\n",
    "VowOW = hfst.regex('OW0 | OW1 | OW2', definitions=defs)\n",
    "defs['VowOW'] = VowOW\n",
    "VowOY = hfst.regex('OY0 | OY1 | OY2', definitions=defs)\n",
    "defs['VowOY'] = VowOY\n",
    "VowUH = hfst.regex('UH0 | UH1 | UH2', definitions=defs)\n",
    "defs['VowUH'] = VowUH\n",
    "VowUW = hfst.regex('UW0 | UW1 | UW2', definitions=defs)\n",
    "defs['VowUW'] = VowUW\n",
    "\n",
    "Vow0 = hfst.regex('AH0| IH0| ER0| IY0| OW0| AA0| EH0| UW0| AE0| AO0| AY0| EY0| AW0| UH0| OY0', definitions=defs)\n",
    "defs['Vow0'] = Vow0\n",
    "Vow1 = hfst.regex('EH1| AE1| AA1| IH1| IY1| EY1| OW1| AO1| AY1| AH1| UW1| ER1| AW1| UH1| OY1', definitions=defs)\n",
    "defs['Vow1'] = Vow1\n",
    "Vow2 = hfst.regex('EH2| EY2| AE2| AY2| AA2| IH2| OW2| IY2| AO2| UW2| AH2| AW2| ER2| UH2| OY2', definitions=defs)\n",
    "defs['Vow2'] = Vow2\n",
    "\n",
    "Vow = hfst.regex('Vow0 | Vow1 | Vow2', definitions=defs)\n",
    "defs['Vow'] = Vow\n",
    "\n",
    "Nas = hfst.regex('N | M | NG', definitions=defs)\n",
    "defs['Nas'] = Nas\n",
    "\n",
    "Phone = hfst.regex('AH0| N| S| L| T| R| K| D| IH0| M| Z| ER0| IY0| B| EH1| P| AE1| AA1| IH1| F| G| V| IY1| NG| HH| EY1| W| SH| OW1| OW0| AO1| AY1| AH1| UW1| JH| Y| CH| AA0| ER1| EH2| EY2| AE2| AY2| AA2| EH0| IH2| TH| AW1| OW2| UW0| IY2| AO2| AE0| UH1| AO0| AY0| UW2| AH2| EY0| OY1| AW2| DH| ZH| ER2| UH2| AW0| UH0| OY2| OY0', definitions = defs)\n",
    "defs['Phone'] = Phone\n",
    "\n",
    "Cons = hfst.regex('[Phone - Vow]', definitions = defs)\n",
    "defs['Cons'] = Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f8ce8",
   "metadata": {},
   "source": [
    "## Creating Classes of Words Based on Stress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a2a69",
   "metadata": {},
   "source": [
    "We use hfst to create classes of words that we can use to construct iambic pentameter based on the stress classes provided to us: primary stress (s1), secondary stress (s2), and unstressed (s0). We have only created machines for the stress cases that are relevnat to the unstressed-stressed pattern of iambic pentameter, so for example, something like s0s0 would not be helpful, therefore we have excluded it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678d9a7",
   "metadata": {},
   "source": [
    "### One Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0cacc",
   "metadata": {},
   "source": [
    "#### Stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5eb9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeeb7b8",
   "metadata": {},
   "source": [
    "#### Unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b3bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* ].l]].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bba20a",
   "metadata": {},
   "source": [
    "### Two Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fdd15",
   "metadata": {},
   "source": [
    "#### Main stress first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af72513",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df8093",
   "metadata": {},
   "source": [
    "#### Main stress second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c44b687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1\"] = n\n",
    "\n",
    "expr = '[English .o. [[Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2679a",
   "metadata": {},
   "source": [
    "### Three Syllable Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098528c",
   "metadata": {},
   "source": [
    "#### Stressed, unstressed, stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716ce8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s1\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s0s2\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s1\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow2 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s2\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow0 Cons* Vow1 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s0s1\"] = q\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow1 Cons* Vow2 Cons* Vow2 Cons*]].l].u'\n",
    "r = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s1s2s2\"] = r\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow2 Cons* Vow1 Cons*]].l].u'\n",
    "s = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s2s1\"] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b92153",
   "metadata": {},
   "source": [
    "#### Unstressed, stressed, unstressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1fa4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "n = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s0\"] = n\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s1s2\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow0 Cons* Vow2 Cons* Vow0 Cons*]].l].u'\n",
    "o = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s0s2s0\"] = o\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow0 Cons*]].l].u'\n",
    "p = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s0\"] = p\n",
    "\n",
    "expr = '[English .o. [[ Cons* Vow2 Cons* Vow1 Cons* Vow2 Cons*]].l].u'\n",
    "q = hfst.regex(expr, definitions=defs)\n",
    "defs[\"s2s1s2\"] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a7a65",
   "metadata": {},
   "source": [
    "In some cases when constructing our vocabulary and our grammar, it became necessary to verify the stress pattern of a word. Below is the code we used to view the lower side representation of a given word based on the streamed in English dictionary in hfst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8ca80c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SAE0KUH1RAA2']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_input(x,n=1,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "\n",
    "# Example word: sakura\n",
    "expr = '[{sakura} .o. English].l'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "sample_input(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d39b80",
   "metadata": {},
   "source": [
    "## Generating Rhyme Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcafdf",
   "metadata": {},
   "source": [
    "Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4fa78b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = '[English .o. [[ Phone* VowAA Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAA\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAE Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAE\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAO Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAO\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAW\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowAY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeAY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowEH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeEH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowER Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeER\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowEY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeEY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowIH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeIH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowIY Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeIY\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowOW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeOW\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowUH Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeUH\"] = m\n",
    "\n",
    "expr = '[English .o. [[ Phone* VowUW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "defs[\"rhymeUW\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61d422",
   "metadata": {},
   "source": [
    "Based on the rhyme classes we define above through the use of hfst, we use sampling to generate a vocabulary for our end rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c01cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_input(x,n=10,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))\n",
    "\n",
    "# Example vowel sound: VowUW\n",
    "expr = '[English .o. [[ Phone* VowUW Cons*]].l].u'\n",
    "m = hfst.regex(expr, definitions=defs)\n",
    "sample_input(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "AW = [\"bloodhound\", \"crowns\", \"blackout\", \"reroute\", \"loud\", \"hometown\", \"scowl\", \"countdown\", \"rouse\", \"mount\"]\n",
    "AY = [\"devise\", \"privatize\", \"bribe\", \"modernize\", \"coincide\", \"chimes\", \"deprived\", \"reunite\", \"apprise\", \"knifelike\"]\n",
    "EH = [\"doorsteps\", \"aspects\", \"flare\", \"sleepwear\", \"pens\", \"pastel\", \"bullpen\", \"pipette\"]\n",
    "ER = [\"rewired\", \"spindler\", \"harvesters\", \"thunders\", \"lowered\", \"gander\", \"prisoners\", \"trimmer\", \"scholar\", \"modern\"]\n",
    "EY = [\"prepaid\", \"gateways\", \"blockades\", \"replace\", \"cliched\", \"acclimate\", \"drain\", \"birthdays\", \"upscale\", \"sedate\"]\n",
    "IH = [\"hallways\", \"parades\", \"dislocate\", \"hurricane\", \"escape\", \"downplay\", \"shortchange\", \"lace\", \"days\"]\n",
    "IY = [\"coyote\", \"squeaky\", \"delete\", \"cheek\", \"cream\", \"blackberry\", \"publicly\", \"blatantly\"]\n",
    "OW = [\"yolks\", \"chrome\", \"intone\", \"pronto\", \"sorrow\", \"disowned\", \"potatoes\", \"mole\", \"notes\"]\n",
    "OY = [\"decoy\", \"convoy\", \"noise\", \"annoy\", \"purloin\", \"steroid\", \"datapoint\", \"boy\", \"tabloids\", \"soy\"]\n",
    "UH = [\"wolves\", \"underwood\", \"scrapbooks\", \"cooked\", \"schedules\", \"cookbooks\", \"endure\", \"understood\", \"rook\", \"woods\"]\n",
    "UW = [\"balloons\", \"typhoons\", \"duped\", \"croon\", \"loon\", \"resume\", \"ingenue\", \"remove\", \"lawsuit\", \"troops\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f228ebb",
   "metadata": {},
   "source": [
    "Based on the lists generated above, we created a dictionary of words in our vocabulary mapped to the rhyme class of their last syllable. As this is a rather large dictionary, we wrote it to to an external text file that we then import below to use moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve rhyme dictionary from external file \n",
    "from rhyme_dict import rhyme_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2558a4",
   "metadata": {},
   "source": [
    "## Generate Iambic Pentameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6210764",
   "metadata": {},
   "source": [
    "Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac412fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input and output\n",
    "\n",
    "def sample_input(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.input_project()\n",
    "        x2.minimize()\n",
    "        word = None\n",
    "        dictionary = ['upright', 'abundant', 'annoying', 'adversely', 'quickly', 'very', 'and', 'or', 'but', 'every', 'some', 'no', 'a', 'that', 'who', 'whom', 'risotto', 'aqueduct', 'aqueducts', 'wok', 'woks', 'guitar', 'guitars', 'trustee', 'trustees', 'accredits', 'accredit']\n",
    "        while word == None or word not in dictionary:\n",
    "            word = random.sample(set(x2.extract_paths(max_cycles=3).keys()),n)[0].replace('|', '')\n",
    "        return word\n",
    "def sample_output(x,n=8,cycles=3):\n",
    "        x2 = x.copy()\n",
    "        x2.output_project()\n",
    "        x2.minimize()\n",
    "        return(random.sample(set(x2.extract_paths(max_cycles=3).keys()),n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0965b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(wordClasses : list, defs) -> (str, str):\n",
    "    # [wordClasses] a LIST of [word class, frequency] lists, with word classes defined in [defs]\n",
    "    # Frequencies should add up to 1\n",
    "    # \n",
    "    # Returns: (word class, sample)\n",
    "    r = random.random()\n",
    "    for wordClass in wordClasses:\n",
    "        r -= wordClass[1]\n",
    "        if r < 0:\n",
    "            return (wordClass[0], sample_input(hfst.regex(wordClass[0], definitions=defs), n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef2625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesToList(lst, wordClasses : dict):\n",
    "    result = []\n",
    "    sumFreq = 0\n",
    "    for wc in lst:\n",
    "        result.append([wc, wordClasses[wc]])\n",
    "        sumFreq += wordClasses[wc]\n",
    "    for i in range(len(result)):\n",
    "        result[i][1] /= sumFreq\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9546ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iambs(wordClasses : dict, defs):\n",
    "    # [wordClasses] a DICTIONARY mapping word classes (defined in [defs]) to frequencies\n",
    "    # Frequencies should add up to 1\n",
    "    # Each element: s1 (primary), s2 (secondary), s0 (unstressed)\n",
    "    syllables = []\n",
    "    words_out = []\n",
    "    index = 0\n",
    "    while index < 10:\n",
    "        if index == 0:\n",
    "            preLst = [\"s0\", \"s0s1\", \"s2s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\", \"s2s1s0\", \"s2s1s2\"]\n",
    "        elif index == 8:\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\"])\n",
    "        elif index == 9:\n",
    "            preLst = [\"s1\"]\n",
    "        else:\n",
    "            # Unstressed\n",
    "            if index % 2 == 0:\n",
    "                preLst = [\"s0\", \"s0s1\", \"s0s1s0\", \"s0s1s2\", \"s0s2s0\"]\n",
    "                if syllables[index - 1] == \"s1\":\n",
    "                    preLst.extend([\"s2s1\", \"s2s1s0\", \"s2s1s2\"])\n",
    "            # Stressed\n",
    "            else:\n",
    "                preLst = [\"s1\", \"s1s0\", \"s1s2\", \"s1s0s1\", \"s1s0s2\", \"s1s2s1\", \"s1s2s2\"]\n",
    "                if syllables[index - 1] == \"s0\":\n",
    "                    preLst.extend([\"s2s0s2\", \"s2s0s1\", \"s2s2s1\"])\n",
    "                    \n",
    "        lst = classesToList(preLst, wordClasses)      \n",
    "        wordClass, word = sample(lst, defs)\n",
    "        wordSyl = wordClass.split(\"s\")\n",
    "        for syl in wordSyl:\n",
    "            if syl != \"\":   \n",
    "                syllables.append(\"s\" + syl) \n",
    "                index += 1\n",
    "        words_out.append(word)\n",
    "        \n",
    "    return words_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe8c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordClasses = {\"s0\": 1/16, \"s1\": 1/16, \"s0s1\": 1/16, \"s1s0\": 1/16, \"s2s1\": 1/16, \"s1s2\": 1/16, \"s0s1s0\": 1/16, \"s0s1s2\": 1/16, \"s0s2s0\": 1/16, \"s2s1s0\": 1/16, \"s2s1s2\": 1/16, \"s1s0s1\": 1/16, \"s1s0s2\": 1/16, \"s1s2s1\": 1/16, \"s2s0s2\": 1/16, \"s2s0s1\": 1/16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc3bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grammatical(s):\n",
    "    try:\n",
    "        t = next(p.parse(s))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a9194f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = load_parser(\"grammar.fcfg\", trace=0, cache=False)\n",
    "g = p.grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1044bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49508f50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/3705285304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_grammatical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_iambs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordClasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/2947860502.py\u001b[0m in \u001b[0;36mgenerate_iambs\u001b[0;34m(wordClasses, defs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassesToList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreLst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mwordClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mwordSyl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msyl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordSyl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/1584782538.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(wordClasses, defs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mwordClass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwordClass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhfst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordClass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefinitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/1737598299.py\u001b[0m in \u001b[0;36msample_input\u001b[0;34m(x, n, cycles)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'upright'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'abundant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annoying'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adversely'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quickly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'very'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'and'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'or'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'but'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'every'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'some'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'no'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'that'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'who'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'whom'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'risotto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aqueduct'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aqueducts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wok'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'woks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'guitar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'guitars'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trustee'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trustees'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accredits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accredit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_cycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4744/p3env/lib/python3.7/site-packages/libhfst_dev.py\u001b[0m in \u001b[0;36mextract_paths\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   4547\u001b[0m               \u001b[0mretval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_random_paths_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4549\u001b[0;31m               \u001b[0mretval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_paths_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cycles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4550\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4744/p3env/lib/python3.7/site-packages/libhfst_dev.py\u001b[0m in \u001b[0;36m_extract_paths_fd\u001b[0;34m(self, max_num, cycles, filter_fd)\u001b[0m\n\u001b[1;32m   4106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_paths_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_libhfst_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHfstTransducer__extract_paths_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_fd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_random_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sent = None\n",
    "while sent == None or not check_grammatical(sent):\n",
    "    sent = generate_iambs(wordClasses, defs)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'some actor whom a black cat that chases a shiny rat quickly chases chases small dogs'.split()\n",
    "t = next(p.parse(s))\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a398128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "class wordIterable:\n",
    "    def __init__(self, file, n):\n",
    "        p1 = load_parser(file, trace=0, cache=False)\n",
    "        g1 = p1.grammar()\n",
    "        self.parser = p1\n",
    "        self.grammar = g1\n",
    "        self.n = n\n",
    "        self.sents = []\n",
    "        gen1 = generate(g1, depth=2*n)\n",
    "        s_temp = list(gen1)\n",
    "        for s in s_temp:\n",
    "            if len(s) == self.n and self.check_grammatical(s) and s not in self.sents:\n",
    "                self.sents.append(s)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if len(self.sents) > 0:\n",
    "            i = randint(0, len(self.sents) - 1)\n",
    "            sent = self.sents[i]\n",
    "            del self.sents[i]\n",
    "            return sent\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def check_grammatical(self, s):\n",
    "        try:\n",
    "            t = next(self.parser.parse(s))\n",
    "            return True\n",
    "        except StopIteration:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a703e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'risotto', 'is', 'upright']\n",
      "['every', 'risotto', 'is', 'abundant']\n",
      "['every', 'risotto', 'is', 'annoying']\n",
      "['every', 'risotto', 'was', 'upright']\n",
      "['every', 'risotto', 'was', 'abundant']\n",
      "['every', 'risotto', 'was', 'annoying']\n",
      "['every', 'aqueduct', 'is', 'upright']\n",
      "['every', 'aqueduct', 'is', 'abundant']\n",
      "['every', 'aqueduct', 'is', 'annoying']\n",
      "['every', 'aqueduct', 'was', 'upright']\n",
      "['every', 'aqueduct', 'was', 'abundant']\n",
      "['every', 'aqueduct', 'was', 'annoying']\n",
      "['every', 'wok', 'is', 'upright']\n",
      "['every', 'wok', 'is', 'abundant']\n",
      "['every', 'wok', 'is', 'annoying']\n",
      "['every', 'wok', 'was', 'upright']\n",
      "['every', 'wok', 'was', 'abundant']\n",
      "['every', 'wok', 'was', 'annoying']\n",
      "['every', 'guitar', 'is', 'upright']\n",
      "['every', 'guitar', 'is', 'abundant']\n",
      "['every', 'guitar', 'is', 'annoying']\n",
      "['every', 'guitar', 'was', 'upright']\n",
      "['every', 'guitar', 'was', 'abundant']\n",
      "['every', 'guitar', 'was', 'annoying']\n",
      "['every', 'trustee', 'is', 'upright']\n",
      "['every', 'trustee', 'is', 'abundant']\n",
      "['every', 'trustee', 'is', 'annoying']\n",
      "['every', 'trustee', 'was', 'upright']\n",
      "['every', 'trustee', 'was', 'abundant']\n",
      "['every', 'trustee', 'was', 'annoying']\n",
      "['some', 'risotto', 'is', 'upright']\n",
      "['some', 'risotto', 'is', 'abundant']\n",
      "['some', 'risotto', 'is', 'annoying']\n",
      "['some', 'risotto', 'was', 'upright']\n",
      "['some', 'risotto', 'was', 'abundant']\n",
      "['some', 'risotto', 'was', 'annoying']\n",
      "['some', 'aqueduct', 'is', 'upright']\n",
      "['some', 'aqueduct', 'is', 'abundant']\n",
      "['some', 'aqueduct', 'is', 'annoying']\n",
      "['some', 'aqueduct', 'was', 'upright']\n",
      "['some', 'aqueduct', 'was', 'abundant']\n",
      "['some', 'aqueduct', 'was', 'annoying']\n",
      "['some', 'aqueducts', 'are', 'upright']\n",
      "['some', 'aqueducts', 'are', 'abundant']\n",
      "['some', 'aqueducts', 'are', 'annoying']\n",
      "['some', 'aqueducts', 'were', 'upright']\n",
      "['some', 'aqueducts', 'were', 'abundant']\n",
      "['some', 'aqueducts', 'were', 'annoying']\n",
      "['some', 'wok', 'is', 'upright']\n",
      "['some', 'wok', 'is', 'abundant']\n"
     ]
    }
   ],
   "source": [
    "gen = generate(g, depth=4)\n",
    "\n",
    "generated_sents = []\n",
    "for i in range(50):\n",
    "    s = next(gen)\n",
    "    while not check_grammatical(s) or s in generated_sents:\n",
    "        s = next(gen)\n",
    "    generated_sents.append(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fe355cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently encoding lists as dictionary.keys() makes everything run faster\n",
    "wordClasses = {\"s0\": 1/16, \"s1\": 1/16, \"s0s1\": 1/16, \"s1s0\": 1/16, \"s2s1\": 1/16, \"s1s2\": 1/16, \"s0s1s0\": 1/16, \"s0s1s2\": 1/16, \"s0s2s0\": 1/16, \"s2s1s0\": 1/16, \"s2s1s2\": 1/16, \"s1s0s1\": 1/16, \"s1s0s2\": 1/16, \"s1s2s1\": 1/16, \"s2s0s2\": 1/16, \"s2s0s1\": 1/16}\n",
    "rhymePatterns = {\"AA\": 0, \"AE\": 0, \"AH\": 0, \"AO\": 0, \"AW\": 0, \"AY\": 0, \"EH\": 0, \"ER\": 0, \"EY\": 0, \"IH\": 0, \"IY\": 0, \"OW\": 0, \"OY\": 0, \"UH\": 0, \"UW\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1763b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail = 0\n",
    "with open('dummy.fcfg', 'r') as f:\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            tail += 1\n",
    "            lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1bc1b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_left(left):\n",
    "    if '[' not in left and left == 'S' or left.split('[')[0] == 'S':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "399b9cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_stress(left, right, sp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# sp: stress patterns for symbols 1 and 2 [sp1, sp2]\n",
    "# return (left, right)\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_stress = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_stress = left[:ind] + '_' + ''.join(sp) + left[ind:]\n",
    "        else:\n",
    "            left_stress = left + '_' + ''.join(sp)\n",
    "        \n",
    "    right_stress = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_stress.append(right[i][:ind] + '_' + sp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_stress.append(right[i] + '_' + sp[i])\n",
    "            \n",
    "    return left_stress + ' -> ' + right_stress[0] + ' ' + (right_stress[1] if len(right) > 1 else '')\n",
    "    #left_stress, right_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1cdf8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_add_rhyme(left, right, rp):\n",
    "# left: one symbol\n",
    "# right: [symbol1, symbol2] or [symbol1]\n",
    "# rp: rhyme patterns for symbols 1 and 2 [rp1, rp2]\n",
    "\n",
    "    if check_sentence_left(left):\n",
    "        left_rhyme = left\n",
    "    else:\n",
    "        if '[' in left:\n",
    "            ind = left.find('[')\n",
    "            left_rhyme = left[:ind] + '_' + rp[-1] + left[ind:]\n",
    "        else:\n",
    "            left_rhyme = left + '_' + rp[-1]\n",
    "    \n",
    "    right_rhyme = []\n",
    "    for i in range(len(right)):\n",
    "        if '[' in right[i]:\n",
    "            ind = right[i].find('[')\n",
    "            right_rhyme.append(right[i][:ind] + '_' + rp[i] + right[i][ind:])\n",
    "        else:\n",
    "            right_rhyme.append(right[i] + '_' + rp[i])\n",
    "    \n",
    "    return left_rhyme + ' -> ' + right_rhyme[0] + ' ' + (right_rhyme[1] if len(right) > 1 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a0a57a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_meter_add(sp1, sp2):\n",
    "    sp1end = 's' + sp1.split('s')[-1]\n",
    "    sp2begin = 's' + sp2.split('s')[1]\n",
    "    if sp1end == 's0':\n",
    "        if sp2begin == 's1':\n",
    "            return True\n",
    "        if sp2begin == 's0':\n",
    "            return False\n",
    "    if sp1end == 's1':\n",
    "        if sp2begin == 's0':\n",
    "            return True\n",
    "        if sp2begin == 's1':\n",
    "            return False\n",
    "    if sp1end == 's2':\n",
    "        sp1pu = 's' + sp1.split('s')[-2]\n",
    "        if sp1pu == 's0':\n",
    "            if sp2begin == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp1pu == 's1':\n",
    "            if sp2begin == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp1\")\n",
    "    if sp2begin == 's2':\n",
    "        sp2sec = 's' + sp2.split('s')[2]\n",
    "        if sp2sec == 's0':\n",
    "            if sp1end == 's0':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif sp2sec == 's1':\n",
    "            if sp1end == 's1':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            raise Exception(\"Two s2's in a row in sp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3b144a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iambic_pent(left, sp1, sp2):\n",
    "    if check_sentence_left(left):\n",
    "        if not check_meter_add(sp1, sp2):\n",
    "            return False\n",
    "        if len(sp1 + sp2) != 20:\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's1':\n",
    "            return False\n",
    "        if 's' + sp1.split('s')[1] == 's2' and len(sp1.split('s')[1:]) > 1 and 's' + sp1.split('s')[2] == 's0':\n",
    "            return False\n",
    "        return True\n",
    "    else:\n",
    "        return check_meter_add(sp1, sp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09517151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(wordClasses):\n",
    "    result = []\n",
    "    for sp in wordClasses:\n",
    "        result.append(sp)\n",
    "    \n",
    "    for sp1 in wordClasses:\n",
    "        for sp2 in wordClasses:\n",
    "            if check_meter_add(sp1, sp2):\n",
    "                result.append(sp1 + sp2)\n",
    "    \n",
    "    limitHit = False\n",
    "    for sp in result:\n",
    "        if len(sp) >= 10:\n",
    "            limitHit = True\n",
    "            break\n",
    "            \n",
    "    if not limitHit:\n",
    "        result = concat(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c1d94311",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = wordClasses.keys()\n",
    "wc = concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "25d594a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_grammar(wordClasses):\n",
    "    grammar_section = False\n",
    "    lexical_section = False\n",
    "    index = 0;\n",
    "\n",
    "    while grammar_section == False:\n",
    "        if lines[index] == '# Grammar Rules':\n",
    "            grammar_section = True\n",
    "        index += 1\n",
    "\n",
    "    new_grammar = set()\n",
    "    while lexical_section == False:\n",
    "        if lines[index] == '# Lexical Rules':\n",
    "            lexical_section = True\n",
    "        if ' -> ' in lines[index]:\n",
    "            left = lines[index].split(' -> ')[0]\n",
    "            right = lines[index].split(' -> ')[1].split(' ')\n",
    "            if len(right) == 1:\n",
    "                for sp in wordClasses:\n",
    "                    new_grammar.add(rule_add_stress(left, right, [sp]))\n",
    "            else:\n",
    "                for sp1 in wordClasses:\n",
    "                    for sp2 in wordClasses:\n",
    "                        if check_iambic_pent(left, sp1, sp2):\n",
    "                            new_grammar.add(rule_add_stress(left, right, [sp1, sp2]))\n",
    "                            #for rp1 in rhymePatterns:\n",
    "                                #for rp2 in rhymePatterns:\n",
    "                                    #sleft, sright = rule_add_stress(left, right, [sp1, sp2])\n",
    "                                    #rule_add_rhyme(sleft, sright, [rp1, rp2])\n",
    "                            \n",
    "        index += 1\n",
    "\n",
    "    lexical_rules = []\n",
    "    while index < tail:\n",
    "        lexical_rules.append(lines[index])\n",
    "        index += 1\n",
    "\n",
    "    \n",
    "    with open('dummynew.fcfg', 'w') as f:\n",
    "        f.write(\"% start S\\n\")\n",
    "        for rule in new_grammar:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')\n",
    "        for rule in lexical_rules:\n",
    "            f.write(rule)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d66bcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_grammar(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b443e378",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_grammar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/3748289772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_grammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_grammar' is not defined"
     ]
    }
   ],
   "source": [
    "# len(new_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13168abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = load_parser(\"dummynew.fcfg\", trace=0, cache=False)\n",
    "gd = pd.grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a9c28e67",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/69xzh4jn4dxgqqk0g0d5tp9w0000gn/T/ipykernel_96592/556051789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'potatoes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'escape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ads'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#td = next(pd.parse(s))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#td = next(pd.parse(s))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#td = next(pd.parse(s))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = ['potatoes', 'escape', 'the', 'ads']\n",
    "td = next(pd.parse(s))\n",
    "#td = next(pd.parse(s))\n",
    "#td = next(pd.parse(s))\n",
    "#td = next(pd.parse(s))\n",
    "td.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fcdcc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grammatical(p, s):\n",
    "    try:\n",
    "        t = next(p.parse(s))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "gen = generate(gd, depth=20)\n",
    "\n",
    "s = next(gen)\n",
    "generated_sents = []\n",
    "for i in range(100000):\n",
    "    while not check_grammatical(pd, s) or s in generated_sents:\n",
    "        s = next(gen)\n",
    "    # print(s)\n",
    "    generated_sents.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb23f7",
   "metadata": {},
   "source": [
    "As expected, it takes a very long time to run the generations, so we have stored the results of our 100,000 sentence generation run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b936031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "## For stashing the results of generate in a file ##\n",
    "\n",
    "# with open(r'sentences.txt', 'w') as fp:\n",
    "#     for s in generated_sents:\n",
    "#         # write each item on a new line\n",
    "#         fp.write(\"%s\\n\" % s)\n",
    "#     print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250c736",
   "metadata": {},
   "source": [
    "As the generation function built into nltk has a very specific order to how it generates sentences, we shuffle the indices of teh generations to create as much variety as possible in the lines that go into our sonnets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7f5f9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle indices \n",
    "random.shuffle(generated_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6ea78",
   "metadata": {},
   "source": [
    "### Functions for Constructing Stanzas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf74b8",
   "metadata": {},
   "source": [
    "Below we tie together the many parts of our program into a handful of functions that generate the pieces that are combiend to create the structure of a sonnet. We have one function to create a quatrain-style stanza following the a/b/a/b rhyming pattern and anotehr function to produce a rhyming couplet, accompanied by assorted helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a12ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers \n",
    "\n",
    "def check_line(line, poem_so_far):\n",
    "    \"\"\"\n",
    "    Verifies that a given word has only appeared in the existing body of \n",
    "    the sonnet no more than once before.\n",
    "    \"\"\"\n",
    "    for word in line:\n",
    "        count = 0\n",
    "        for line in poem_so_far:\n",
    "            if word in line:\n",
    "                count += 1\n",
    "            if count > 1 and word != 'the':\n",
    "                return False\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "90f3525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abab_stanza(generated_sents, rhyme_dict, poem_so_far=[]):\n",
    "    \"\"\"\n",
    "    Generates a quatrain with an a/b/a/b rhyme scheme based on the following parameters\n",
    "    \n",
    "    ARGUMENTS\n",
    "    =========\n",
    "    \n",
    "    generated_sents : list\n",
    "        generations produced from the grammar \n",
    "    \n",
    "    rhyme_dict : dict\n",
    "        dictionary of the rhyme associated with each word in the grammar \n",
    "    \n",
    "    poem_so_far : list of lists\n",
    "        the existing lines of the poem that have been accumulated so far\n",
    "    \"\"\"\n",
    "    \n",
    "    r1 = random.randint(0, len(generated_sents) - 1)\n",
    "    first_sent = generated_sents[r1]\n",
    "    sents_list = poem_so_far\n",
    "    \n",
    "    while not check_line(first_sent, sents_list):\n",
    "        r1 = random.randint(0, len(generated_sents) - 1)\n",
    "        first_sent = generated_sents[r1]\n",
    "    \n",
    "\n",
    "    sents_list.append(first_sent)\n",
    "    \n",
    "    a_word = first_sent[-1]\n",
    "    a = rhyme_dict[a_word]\n",
    "    \n",
    "    r2 = random.randint(0, len(generated_sents) - 1)\n",
    "    second_sent = generated_sents[r2]\n",
    "    \n",
    "    while not check_line(second_sent, sents_list) or second_sent[-1] == a_word:\n",
    "        r2 = random.randint(0, len(generated_sents) - 1)\n",
    "        second_sent = generated_sents[r2]\n",
    "    \n",
    "    sents_list.append(second_sent)\n",
    "    \n",
    "    b_word = second_sent[-1]\n",
    "    b = rhyme_dict[b_word]\n",
    "   \n",
    "    \n",
    "    # shuffle indicies \n",
    "    inds1 = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds1)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(inds1):\n",
    "        sent = generated_sents[inds1[i]]\n",
    "        if rhyme_dict[sent[-1]] == a and sent not in sents_list and sent[-1] != a_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break;\n",
    "        i += 1\n",
    "        if i == len(inds1):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "\n",
    "    inds2 = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds2)\n",
    "    \n",
    "    j = 0\n",
    "    while j < len(inds2):\n",
    "        sent = generated_sents[inds2[j]]                    \n",
    "        if rhyme_dict[sent[-1]] == b and sent not in sents_list and sent[-1] != b_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break; \n",
    "        j += 1\n",
    "        if j == len(inds2):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "    \n",
    "    return sents_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gg_couplet(generated_sents, rhyme_dict, poem_so_far):\n",
    "    \"\"\"\n",
    "    Generates a rhyming couplet based on the following parameters\n",
    "    \n",
    "    ARGUMENTS\n",
    "    =========\n",
    "    \n",
    "    generated_sents : list\n",
    "        generations produced from the grammar \n",
    "    rhyme_dict : dict\n",
    "        dictionary of the rhyme associated with each word in the grammar \n",
    "    poem_so_far : list of lists\n",
    "        the existing lines of the poem that have been accumulated so far\n",
    "    \"\"\"\n",
    "    \n",
    "    r = random.randint(0, len(generated_sents) - 1)\n",
    "    sent = generated_sents[r]\n",
    "    sents_list = poem_so_far\n",
    "    \n",
    "    while not check_line(sent, sents_list):\n",
    "        r = random.randint(0, len(generated_sents) - 1)\n",
    "        sent = generated_sents[r]\n",
    "    \n",
    "    sents_list.append(sent)\n",
    "          \n",
    "    g_word = sent[-1]\n",
    "    g = rhyme_dict[g_word]\n",
    "    \n",
    "    inds = list(range(0, len(generated_sents)))\n",
    "    random.shuffle(inds)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(inds):\n",
    "        sent = generated_sents[inds[i]]          \n",
    "        if rhyme_dict[sent[-1]] == g and sent not in sents_list and sent[-1] != g_word and check_line(sent, sents_list):\n",
    "            sents_list.append(sent)\n",
    "            break;\n",
    "        i +=1\n",
    "        if i == len(inds):\n",
    "            print(\"Sorry! Could not find a rhyme.\")\n",
    "        \n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f6a1b",
   "metadata": {},
   "source": [
    "## Building the Sonnet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0e0b0",
   "metadata": {},
   "source": [
    "Now we get to put all of the pieces together to generate a sonnet! It is possible that running the below cell will result in the program saying it could not find a rhyme. This is due to the smaller size of our vocabulary and the constrainst placed around stress and rhyming. If that happens, simply run the the cell again, and a sonnet should be generated with new stress and rhyme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cd708459",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stage = generate_abab_stanza(generated_sents, rhyme_dict, [])\n",
    "second_stage = generate_abab_stanza(generated_sents, rhyme_dict, first_stage)\n",
    "third_stage = generate_abab_stanza(generated_sents, rhyme_dict, second_stage)\n",
    "final_sonnet = generate_gg_couplet(generated_sents, rhyme_dict, third_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cb39bc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================SONNET================\n",
      "\n",
      "the straightened knifelike troops escape the loon\n",
      "sedate sedate balloons replant the wasp\n",
      "sedate typhoons apprise the loud balloons\n",
      "malformed rewired notes remark the charm\n",
      "malformed abhorrent pens resume the mole\n",
      "the modern prepaid chimes escape the ads\n",
      "the squeaky sleepwear basks the straightened yolks\n",
      "the squeaky walnut basks the lowered chasse\n",
      "the knifelike cookbooks reroute upscale troops\n",
      "the prepaid schedules shortchange shorn blockades\n",
      "the upscale bullpen thunders loud typhoons\n",
      "the understood parades remark the drain\n",
      "the cooked parades endure the megaton\n",
      "the shorn deprived blockades dislocate stuffs\n"
     ]
    }
   ],
   "source": [
    "print(\"=================SONNET================\" + \"\\n\")\n",
    "for line in final_sonnet:\n",
    "    print(*line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3env",
   "language": "python",
   "name": "p3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
